
@article{chatzopoulos_mobile_2017,
	title = {Mobile Augmented Reality Survey: From Where We Are to Where We Go},
	volume = {5},
	issn = {2169-3536},
	url = {http://ieeexplore.ieee.org/document/7912316/},
	doi = {10.1109/ACCESS.2017.2698164},
	shorttitle = {Mobile Augmented Reality Survey},
	abstract = {The boom in the capabilities and features of mobile devices, like smartphones, tablets, and wearables, combined with the ubiquitous and affordable Internet access and the advances in the areas of cooperative networking, computer vision, and mobile cloud computing transformed mobile augmented reality ({MAR}) from science ﬁction to a reality. Although mobile devices are more constrained computationalwise from traditional computers, they have a multitude of sensors that can be used to the development of more sophisticated {MAR} applications and can be assisted from remote servers for the execution of their intensive parts. In this paper, after introducing the reader to the basics of {MAR}, we present a categorization of the application ﬁelds together with some representative examples. Next, we introduce the reader to the user interface and experience in {MAR} applications and continue with the core system components of the {MAR} systems. After that, we discuss advances in tracking and registration, since their functionality is crucial to any {MAR} application and the network connectivity of the devices that run {MAR} applications together with its importance to the performance of the application. We continue with the importance of data management in {MAR} systems and the systems performance and sustainability, and before we conclude this survey, we present existing challenging problems.},
	pages = {6917--6950},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Chatzopoulos, Dimitris and Bermejo, Carlos and Huang, Zhanpeng and Hui, Pan},
	urldate = {2019-08-22},
	date = {2017},
	langid = {english},
	file = {Chatzopoulos et al. - 2017 - Mobile Augmented Reality Survey From Where We Are.pdf:/home/ajk/Zotero/storage/7H78HKQF/Chatzopoulos et al. - 2017 - Mobile Augmented Reality Survey From Where We Are.pdf:application/pdf}
}

@inproceedings{huang_mobile_2012,
	location = {Taipei, Taiwan},
	title = {Mobile augmented reality based on cloud computing},
	isbn = {978-1-4673-2145-7 978-1-4673-2144-0 978-1-4673-2143-3},
	url = {http://ieeexplore.ieee.org/document/6325354/},
	doi = {10.1109/ICASID.2012.6325354},
	abstract = {In this paper, we implemented a mobile augmented reality system based on cloud computing. This system uses a mobile device with a camera to capture images of book spines and sends processed features to the cloud. In the cloud, the features are compared with the database and the information of the best matched book would be sent back to the mobile device. The information will then be rendered on the display via augmented reality. In order to reduce the transmission cost, the mobile device is used to perform most of the image processing tasks, such as the preprocessing, resizing, corner detection, and augmented reality rendering. On the other hand, the cloud is used to realize routine but large quantity feature comparisons. Using the cloud as the database also makes the future extension much more easily. For our prototype system, we use an Android smart phone as our mobile device, and Chunghwa Telecoms hicloud as the cloud.},
	eventtitle = {2012 International Conference on Anti-Counterfeiting, Security and Identification (2012 {ASID})},
	pages = {1--5},
	booktitle = {Anti-counterfeiting, Security, and Identification},
	publisher = {{IEEE}},
	author = {Huang, Bai-Ruei and Lin, Chang Hong and Lee, Chia-Han},
	urldate = {2019-08-22},
	date = {2012-08},
	langid = {english},
	file = {Huang et al. - 2012 - Mobile augmented reality based on cloud computing.pdf:/home/ajk/Zotero/storage/DPGV8NAL/Huang et al. - 2012 - Mobile augmented reality based on cloud computing.pdf:application/pdf}
}

@inproceedings{liu_dare:_2018,
	location = {Cambridge},
	title = {{DARE}: Dynamic Adaptive Mobile Augmented Reality with Edge Computing},
	isbn = {978-1-5386-6043-0},
	url = {https://ieeexplore.ieee.org/document/8526799/},
	doi = {10.1109/ICNP.2018.00011},
	shorttitle = {{DARE}},
	abstract = {Mobile augmented reality ({MAR}) is a killer application of mobile edge computing because of its high computation demand and stringent latency requirement. Since edge networks and computing resources are highly dynamic, handling such dynamics is essential for providing high-quality {MAR} services. In this paper, we design a new network protocol named {DARE} (dynamic adaptive {AR} over the edge) that enables mobile users to dynamically change their {AR} conﬁgurations according to wireless channel conditions and computation workloads in edge servers. The dynamic conﬁguration adaptations reduce the service latency of {MAR} users and maximize the quality of augmentation ({QoA}) under varying network conditions and computation workloads. Considering the video frame size and computation model, i.e., object detection algorithms, as two key parameters in adapting the {AR} conﬁguration, we develop analytical models to characterize the impact of these parameters on {QoA} and the service latency. Then, we design optimization mechanisms on both the edge server and {AR} devices to guide the {AR} conﬁguration adaptation and server computation resource allocation. The performance of the {DARE} protocol is validated through a small-scale testbed implementation.},
	eventtitle = {2018 {IEEE} 26th International Conference on Network Protocols ({ICNP})},
	pages = {1--11},
	booktitle = {2018 {IEEE} 26th International Conference on Network Protocols ({ICNP})},
	publisher = {{IEEE}},
	author = {Liu, Qiang and Han, Tao},
	urldate = {2019-08-22},
	date = {2018-09},
	langid = {english},
	file = {Liu and Han - 2018 - DARE Dynamic Adaptive Mobile Augmented Reality wi.pdf:/home/ajk/Zotero/storage/9FXHMG58/Liu and Han - 2018 - DARE Dynamic Adaptive Mobile Augmented Reality wi.pdf:application/pdf}
}

@article{akherfi_mobile_2018,
	title = {Mobile cloud computing for computation offloading: Issues and challenges},
	volume = {14},
	issn = {22108327},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210832716300400},
	doi = {10.1016/j.aci.2016.11.002},
	shorttitle = {Mobile cloud computing for computation offloading},
	abstract = {Despite the evolution and enhancements that mobile devices have experienced, they are still considered as limited computing devices. Today, users become more demanding and expect to execute computational intensive applications on their smartphone devices. Therefore, Mobile Cloud Computing ({MCC}) integrates mobile computing and Cloud Computing ({CC}) in order to extend capabilities of mobile devices using ofﬂoading techniques. Computation ofﬂoading tackles limitations of Smart Mobile Devices ({SMDs}) such as limited battery lifetime, limited processing capabilities, and limited storage capacity by ofﬂoading the execution and workload to other rich systems with better performance and resources. This paper presents the current ofﬂoading frameworks, computation ofﬂoading techniques, and analyzes them along with their main critical issues. In addition, it explores different important parameters based on which the frameworks are implemented such as ofﬂoading method and level of partitioning. Finally, it summarizes the issues in ofﬂoading frameworks in the {MCC} domain that requires further research.},
	pages = {1--16},
	number = {1},
	journaltitle = {Applied Computing and Informatics},
	shortjournal = {Applied Computing and Informatics},
	author = {Akherfi, Khadija and Gerndt, Micheal and Harroud, Hamid},
	urldate = {2019-08-22},
	date = {2018-01},
	langid = {english},
	file = {Akherfi et al. - 2018 - Mobile cloud computing for computation offloading.pdf:/home/ajk/Zotero/storage/RN9DKK9F/Akherfi et al. - 2018 - Mobile cloud computing for computation offloading.pdf:application/pdf}
}

@article{khan_survey_2014,
	title = {A Survey of Mobile Cloud Computing Application Models},
	volume = {16},
	issn = {1553-877X, 2373-745X},
	url = {https://ieeexplore.ieee.org/document/6553297/},
	doi = {10.1109/SURV.2013.062613.00160},
	abstract = {Smartphones are now capable of supporting a wide range of applications, many of which demand an ever increasing computational power. This poses a challenge because smartphones are resource-constrained devices with limited computation power, memory, storage, and energy. Fortunately, the cloud computing technology offers virtually unlimited dynamic resources for computation, storage, and service provision. Therefore, researchers envision extending cloud computing services to mobile devices to overcome the smartphones constraints. The challenge in doing so is that the traditional smartphone application models do not support the development of applications that can incorporate cloud computing features and requires specialized mobile cloud application models. This article presents mobile cloud architecture, ofﬂoading decision affecting entities, application models classiﬁcation, the latest mobile cloud application models, their critical analysis and future research directions.},
	pages = {393--413},
	number = {1},
	journaltitle = {{IEEE} Communications Surveys \& Tutorials},
	shortjournal = {{IEEE} Commun. Surv. Tutorials},
	author = {Khan, Atta ur Rehman and Othman, Mazliza and Madani, Sajjad Ahmad and Khan, Samee Ullah},
	urldate = {2019-08-22},
	date = {2014},
	langid = {english},
	file = {Khan et al. - 2014 - A Survey of Mobile Cloud Computing Application Mod.pdf:/home/ajk/Zotero/storage/FP24FFN3/Khan et al. - 2014 - A Survey of Mobile Cloud Computing Application Mod.pdf:application/pdf}
}

@article{everingham_pascal_2010,
	title = {The Pascal Visual Object Classes ({VOC}) Challenge},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The {PASCAL} Visual Object Classes ({VOC}) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	pages = {303--338},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	urldate = {2019-08-22},
	date = {2010-06},
	langid = {english},
	file = {Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:/home/ajk/Zotero/storage/G7XLVEKC/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf}
}

@article{zou_object_2019,
	title = {Object Detection in 20 Years: A Survey},
	url = {http://arxiv.org/abs/1905.05055},
	shorttitle = {Object Detection in 20 Years},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today’s object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century’s time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	journaltitle = {{arXiv}:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2019-08-22},
	date = {2019-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf:/home/ajk/Zotero/storage/GRTHFJJ8/Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf:application/pdf}
}

@inproceedings{redmon_you_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {779--788},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2019-08-22},
	date = {2016-06},
	langid = {english},
	file = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:/home/ajk/Zotero/storage/C5QM7YGV/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}

@article{liu_ssd:_2016,
	title = {{SSD}: Single Shot {MultiBox} Detector},
	volume = {9905},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	shorttitle = {{SSD}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named {SSD}, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. {SSD} is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes {SSD} easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the {PASCAL} {VOC}, {COCO}, and {ILSVRC} datasets conﬁrm that {SSD} has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, {SSD} achieves 74.3\% {mAP}1 on {VOC}2007 test at 59 {FPS} on a Nvidia Titan X and for 512 × 512 input, {SSD} achieves 76.9\% {mAP}, outperforming a comparable state-of-the-art Faster R-{CNN} model. Compared to other single stage methods, {SSD} has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	pages = {21--37},
	journaltitle = {{arXiv}:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	urldate = {2019-08-22},
	date = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:/home/ajk/Zotero/storage/C94N3WCT/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf}
}

@article{sandler_mobilenetv2:_2018,
	title = {{MobileNetV}2: Inverted Residuals and Linear Bottlenecks},
	url = {http://arxiv.org/abs/1801.04381},
	shorttitle = {{MobileNetV}2},
	abstract = {In this paper we describe a new mobile architecture, {MobileNetV}2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call {SSDLite}. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of {DeepLabv}3 which we call Mobile {DeepLabv}3.},
	journaltitle = {{arXiv}:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	urldate = {2019-08-22},
	date = {2018-01-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:/home/ajk/Zotero/storage/DCUGD8XB/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf}
}

@article{dean_large_nodate,
	title = {Large Scale Distributed Deep Networks},
	abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of {CPU} cores. We have developed a software framework called {DistBelief} that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour {SGD}, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-{BFGS}. Downpour {SGD} and Sandblaster L-{BFGS} both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on {ImageNet}, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
	pages = {9},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
	langid = {english},
	file = {Dean et al. - Large Scale Distributed Deep Networks.pdf:/home/ajk/Zotero/storage/2S2NKMFU/Dean et al. - Large Scale Distributed Deep Networks.pdf:application/pdf}
}

@inproceedings{teerapittayanon_distributed_2017,
	location = {Atlanta, {GA}, {USA}},
	title = {Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices},
	isbn = {978-1-5386-1792-2},
	url = {http://ieeexplore.ieee.org/document/7979979/},
	doi = {10.1109/ICDCS.2017.226},
	abstract = {We propose distributed deep neural networks ({DDNNs}) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network ({DNN}) in the cloud, a {DDNN} also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a {DDNN} can scale up in neural network size and scale out in geographical span. Due to its distributed nature, {DDNNs} enhance sensor fusion, system fault tolerance and data privacy for {DNN} applications. In implementing a {DDNN}, we map sections of a {DNN} onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a {DDNN} can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of ofﬂoading raw sensor data to be processed in the cloud, {DDNN} locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.},
	eventtitle = {2017 {IEEE} 37th International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {328--339},
	booktitle = {2017 {IEEE} 37th International Conference on Distributed Computing Systems ({ICDCS})},
	publisher = {{IEEE}},
	author = {Teerapittayanon, Surat and {McDanel}, Bradley and Kung, H.T.},
	urldate = {2019-08-22},
	date = {2017-06},
	langid = {english},
	file = {Teerapittayanon et al. - 2017 - Distributed Deep Neural Networks Over the Cloud, t.pdf:/home/ajk/Zotero/storage/A4QIEKK3/Teerapittayanon et al. - 2017 - Distributed Deep Neural Networks Over the Cloud, t.pdf:application/pdf}
}

@article{courbariaux_binaryconnect:_2015,
	title = {{BinaryConnect}: Training Deep Neural Networks with binary weights during propagations},
	url = {http://arxiv.org/abs/1511.00363},
	shorttitle = {{BinaryConnect}},
	abstract = {Deep Neural Networks ({DNN}) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, {GPUs} enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning ({DL}). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great beneﬁts to specialized {DL} hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce {BinaryConnect}, a method which consists in training a {DNN} with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that {BinaryConnect} acts as regularizer and we obtain near state-of-the-art results with {BinaryConnect} on the permutation-invariant {MNIST}, {CIFAR}-10 and {SVHN}.},
	journaltitle = {{arXiv}:1511.00363 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	urldate = {2019-08-22},
	date = {2015-11-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.00363},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Courbariaux et al. - 2015 - BinaryConnect Training Deep Neural Networks with .pdf:/home/ajk/Zotero/storage/FPAS4UN2/Courbariaux et al. - 2015 - BinaryConnect Training Deep Neural Networks with .pdf:application/pdf}
}

@article{zhou_edge_2019,
	title = {Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing},
	volume = {107},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/8736011/},
	doi = {10.1109/JPROC.2019.2918951},
	shorttitle = {Edge Intelligence},
	pages = {1738--1762},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
	urldate = {2019-08-22},
	date = {2019-08},
	langid = {english},
	file = {Zhou et al. - 2019 - Edge Intelligence Paving the Last Mile of Artific.pdf:/home/ajk/Zotero/storage/UF4HPULQ/Zhou et al. - 2019 - Edge Intelligence Paving the Last Mile of Artific.pdf:application/pdf}
}

@article{mach_mobile_2017,
	title = {Mobile Edge Computing: A Survey on Architecture and Computation Offloading},
	volume = {19},
	issn = {1553-877X},
	url = {http://ieeexplore.ieee.org/document/7879258/},
	doi = {10.1109/COMST.2017.2682318},
	shorttitle = {Mobile Edge Computing},
	abstract = {Technological evolution of mobile user equipment ({UEs}), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the {UEs} is constrained by limited battery capacity and energy consumption of the {UEs}. A suitable solution extending the battery life-time of the {UEs} is to ofﬂoad the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces signiﬁcant execution delay consisting of delivery of the ofﬂoaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the ofﬂoading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing ({MEC}), has been introduced. The {MEC} brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the {UE} while meeting strict delay requirements. The {MEC} computing resources can be exploited also by operators and third parties for speciﬁc purposes. In this paper, we ﬁrst describe major use cases and reference scenarios where the {MEC} is applicable. After that we survey existing concepts integrating {MEC} functionalities to the mobile networks and discuss current advancement in standardization of the {MEC}. The core of this survey is, then, focused on user-oriented use case in the {MEC}, i.e., computation ofﬂoading. In this regard, we divide the research on computation ofﬂoading to three key areas: 1) decision on computation ofﬂoading; 2) allocation of computing resource within the {MEC}; and 3) mobility management. Finally, we highlight lessons learned in area of the {MEC} and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the {MEC}.},
	pages = {1628--1656},
	number = {3},
	journaltitle = {{IEEE} Communications Surveys \& Tutorials},
	shortjournal = {{IEEE} Commun. Surv. Tutorials},
	author = {Mach, Pavel and Becvar, Zdenek},
	urldate = {2019-08-22},
	date = {2017},
	langid = {english},
	file = {Mach and Becvar - 2017 - Mobile Edge Computing A Survey on Architecture an.pdf:/home/ajk/Zotero/storage/DIDJK74Y/Mach and Becvar - 2017 - Mobile Edge Computing A Survey on Architecture an.pdf:application/pdf}
}

@inproceedings{liu_edge_2018,
	location = {Honolulu, {HI}},
	title = {An Edge Network Orchestrator for Mobile Augmented Reality},
	isbn = {978-1-5386-4128-6},
	url = {https://ieeexplore.ieee.org/document/8486241/},
	doi = {10.1109/INFOCOM.2018.8486241},
	abstract = {Mobile augmented reality ({MAR}) involves high complexity computation which cannot be performed efﬁciently on resource limited mobile devices. The performance of {MAR} would be signiﬁcantly improved by ofﬂoading the computation tasks to servers deployed with the close proximity to the users. In this paper, we design an edge network orchestrator to enable fast and accurate object analytics at the network edge for {MAR}. The measurement-based analytical models are built to characterize the tradeoff between the service latency and analytics accuracy in edge-based {MAR} systems. As a key component of the edge network orchestrator, a server assignment and frame resolution selection algorithm named {FACT} is proposed to mitigate the latency-accuracy tradeoff. Through network simulations, we evaluate the performance of the {FACT} algorithm and show the insights on optimizing the performance of edge-based {MAR} systems. We have implemented the edge network orchestrator and developed the corresponding communication protocol. Our experiments validate the performance of the proposed edge network orchestrator.},
	eventtitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	pages = {756--764},
	booktitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	publisher = {{IEEE}},
	author = {Liu, Qiang and Huang, Siqi and Opadere, Johnson and Han, Tao},
	urldate = {2019-08-22},
	date = {2018-04},
	langid = {english},
	file = {Liu et al. - 2018 - An Edge Network Orchestrator for Mobile Augmented .pdf:/home/ajk/Zotero/storage/Q8K9S35Q/Liu et al. - 2018 - An Edge Network Orchestrator for Mobile Augmented .pdf:application/pdf}
}

@article{huang_distributed_2018,
	title = {Distributed Deep Learning-based Offloading for Mobile Edge Computing Networks},
	issn = {1383-469X, 1572-8153},
	url = {http://link.springer.com/10.1007/s11036-018-1177-x},
	doi = {10.1007/s11036-018-1177-x},
	abstract = {This paper studies mobile edge computing ({MEC}) networks where multiple wireless devices ({WDs}) choose to offload their computation tasks to an edge server. To conserve energy and maintain quality of service for {WDs}, the optimization of joint offloading decision and bandwidth allocation is formulated as a mixed integer programming problem. However, the problem is computationally limited by the curse of dimensionality, which cannot be solved by general optimization tools in an effective and efficient way, especially for large-scale {WDs}. In this paper, we propose a distributed deep learning-based offloading ({DDLO}) algorithm for {MEC} networks, where multiple parallel {DNNs} are used to generate offloading decisions. We adopt a shared replay memory to store newly generated offloading decisions which are further to train and improve all {DNNs}. Extensive numerical results show that the proposed {DDLO} algorithm can generate near-optimal offloading decisions in less than one second.},
	journaltitle = {Mobile Networks and Applications},
	shortjournal = {Mobile Netw Appl},
	author = {Huang, Liang and Feng, Xu and Feng, Anqi and Huang, Yupin and Qian, Li Ping},
	urldate = {2019-08-22},
	date = {2018-11-29},
	langid = {english},
	file = {Huang et al. - 2018 - Distributed Deep Learning-based Offloading for Mob.pdf:/home/ajk/Zotero/storage/NHDB8GSQ/Huang et al. - 2018 - Distributed Deep Learning-based Offloading for Mob.pdf:application/pdf}
}

@article{shi_edge_2016,
	title = {Edge Computing: Vision and Challenges},
	volume = {3},
	issn = {2327-4662},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	shorttitle = {Edge Computing},
	abstract = {The proliferation of Internet of Things ({IoT}) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the deﬁnition of edge computing, followed by several case studies, ranging from cloud ofﬂoading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the ﬁeld of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	pages = {637--646},
	number = {5},
	journaltitle = {{IEEE} Internet of Things Journal},
	shortjournal = {{IEEE} Internet Things J.},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	urldate = {2019-08-22},
	date = {2016-10},
	langid = {english},
	file = {Shi et al. - 2016 - Edge Computing Vision and Challenges.pdf:/home/ajk/Zotero/storage/QW5JS9YX/Shi et al. - 2016 - Edge Computing Vision and Challenges.pdf:application/pdf}
}

@inproceedings{yu_computation_2017,
	location = {Montreal, {QC}},
	title = {Computation offloading for mobile edge computing: A deep learning approach},
	isbn = {978-1-5386-3529-2 978-1-5386-3531-5},
	url = {http://ieeexplore.ieee.org/document/8292514/},
	doi = {10.1109/PIMRC.2017.8292514},
	shorttitle = {Computation offloading for mobile edge computing},
	abstract = {Computation ofﬂoading has already shown itself to be successful for enabling resource-intensive applications on mobile devices. Moreover, in view of mobile edge computing ({MEC}) system, mobile devices can ofﬂoad compute-intensive tasks to a nearby cloudlet, so as to save the energy and enhance the processing speed. However, due to the varying network conditions and limited computation resources of cloudlets, the ofﬂoading actions taken by a mobile user may not achieve the lowest cost. In this paper, we develop a dynamic ofﬂoading framework for mobile users, considering the local overhead in the mobile terminal side, as well as the limited communication and computation resources in the network side. We formulate the ofﬂoading decision problem as a multi-label classiﬁcation problem and develop the Deep Supervised Learning ({DSL}) method to minimize the computation and ofﬂoading overhead. Simulation results show that our proposal can reduce system cost up to 49.24\%, 23.87\%, 15.69\%, and 11.18\% compared to the “no ofﬂoading” scheme, “random ofﬂoading” scheme, “total ofﬂoading” scheme and “multi-label linear classiﬁerbased ofﬂoading” scheme, respectively.},
	eventtitle = {2017 {IEEE} 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications ({PIMRC})},
	pages = {1--6},
	booktitle = {2017 {IEEE} 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications ({PIMRC})},
	publisher = {{IEEE}},
	author = {Yu, Shuai and Wang, Xin and Langar, Rami},
	urldate = {2019-08-22},
	date = {2017-10},
	langid = {english},
	file = {Yu et al. - 2017 - Computation offloading for mobile edge computing .pdf:/home/ajk/Zotero/storage/LHP3PAZM/Yu et al. - 2017 - Computation offloading for mobile edge computing .pdf:application/pdf}
}

@article{mao_survey_2017,
	title = {A Survey on Mobile Edge Computing: The Communication Perspective},
	volume = {19},
	issn = {1553-877X},
	url = {http://ieeexplore.ieee.org/document/8016573/},
	doi = {10.1109/COMST.2017.2745201},
	shorttitle = {A Survey on Mobile Edge Computing},
	abstract = {Driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing, from the centralized mobile cloud computing toward mobile edge computing ({MEC}). The main feature of {MEC} is to push mobile computing, network control and storage to the network edges (e.g., base stations and access points) so as to enable computation-intensive and latency-critical applications at the resource-limited mobile devices. {MEC} promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The promised gains of {MEC} have motivated extensive efforts in both academia and industry on developing the technology. A main thrust of {MEC} research is to seamlessly merge the two disciplines of wireless communications and mobile computing, resulting in a wide-range of new designs ranging from techniques for computation ofﬂoading to network architectures. This paper provides a comprehensive survey of the state-of-the-art {MEC} research with a focus on joint radio-and-computational resource management. We also discuss a set of issues, challenges, and future research directions for {MEC} research, including {MEC} system deployment, cache-enabled {MEC}, mobility management for {MEC}, green {MEC}, as well as privacy-aware {MEC}. Advancements in these directions will facilitate the transformation of {MEC} from theory to practice. Finally, we introduce recent standardization efforts on {MEC} as well as some typical {MEC} application scenarios.},
	pages = {2322--2358},
	number = {4},
	journaltitle = {{IEEE} Communications Surveys \& Tutorials},
	shortjournal = {{IEEE} Commun. Surv. Tutorials},
	author = {Mao, Yuyi and You, Changsheng and Zhang, Jun and Huang, Kaibin and Letaief, Khaled B.},
	urldate = {2019-08-22},
	date = {2017},
	langid = {english},
	file = {Mao et al. - 2017 - A Survey on Mobile Edge Computing The Communicati.pdf:/home/ajk/Zotero/storage/YWHDJ5C4/Mao et al. - 2017 - A Survey on Mobile Edge Computing The Communicati.pdf:application/pdf}
}

@article{skala_scalable_2015,
	title = {Scalable Distributed Computing Hierarchy: Cloud, Fog and Dew Computing},
	volume = {2},
	abstract = {The paper considers the conceptual approach for organization of the vertical hierarchical links between the scalable distributed computing paradigms: Cloud Computing, Fog Computing and Dew Computing. In this paper, the Dew Computing is described and recognized as a new structural layer in the existing distributed computing hierarchy. In the existing computing hierarchy, the Dew computing is positioned as the ground level for the Cloud and Fog computing paradigms. Vertical, complementary, hierarchical division from Cloud to Dew Computing satisﬁes the needs of high- and low-end computing demands in everyday life and work. These new computing paradigms lower the cost and improve the performance, particularly for concepts and applications such as the Internet of Things ({IoT}) and the Internet of Everything ({IoE}). In addition, the Dew computing paradigm will require new programming models that will efﬁciently reduce the complexity and improve the productivity and usability of scalable distributed computing, following the principles of High-Productivity computing.},
	pages = {9},
	number = {1},
	author = {Skala, Karolj and Davidovic, Davor and Afgan, Enis and Sovic, Ivan},
	date = {2015},
	langid = {english},
	file = {Skala et al. - 2015 - Scalable Distributed Computing Hierarchy Cloud, F.pdf:/home/ajk/Zotero/storage/7BCBE8R2/Skala et al. - 2015 - Scalable Distributed Computing Hierarchy Cloud, F.pdf:application/pdf}
}

@article{song_collaborative_nodate,
	title = {Collaborative Learning for Deep Neural Networks},
	abstract = {We introduce collaborative learning in which multiple classiﬁer heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classiﬁer heads on the same example provides supplementary information as well as regularization to each classiﬁer, thereby improving generalization. Second, intermediate-level representation ({ILR}) sharing with backpropagation rescaling aggregates the gradient ﬂows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on {CIFAR} and {ImageNet} datasets demonstrate that deep neural networks learned as a group in a collaborative way signiﬁcantly reduce the generalization error and increase the robustness to label noise.},
	pages = {10},
	author = {Song, Guocong and Chai, Wei},
	langid = {english},
	file = {Song and Chai - Collaborative Learning for Deep Neural Networks.pdf:/home/ajk/Zotero/storage/86GVGZSM/Song and Chai - Collaborative Learning for Deep Neural Networks.pdf:application/pdf}
}

@article{song_collaborative_2018,
	title = {Collaborative Learning for Deep Neural Networks},
	url = {http://arxiv.org/abs/1805.11761},
	abstract = {We introduce collaborative learning in which multiple classiﬁer heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classiﬁer heads on the same example provides supplementary information as well as regularization to each classiﬁer, thereby improving generalization. Second, intermediate-level representation ({ILR}) sharing with backpropagation rescaling aggregates the gradient ﬂows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on {CIFAR} and {ImageNet} datasets demonstrate that deep neural networks learned as a group in a collaborative way signiﬁcantly reduce the generalization error and increase the robustness to label noise.},
	journaltitle = {{arXiv}:1805.11761 [cs, stat]},
	author = {Song, Guocong and Chai, Wei},
	urldate = {2019-08-26},
	date = {2018-05-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.11761},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Song and Chai - 2018 - Collaborative Learning for Deep Neural Networks.pdf:/home/ajk/Zotero/storage/WUJEKLUH/Song and Chai - 2018 - Collaborative Learning for Deep Neural Networks.pdf:application/pdf}
}

@article{eshratifar_bottlenet:_2019,
	title = {{BottleNet}: A Deep Learning Architecture for Intelligent Mobile Cloud Computing Services},
	url = {http://arxiv.org/abs/1902.01000},
	shorttitle = {{BottleNet}},
	abstract = {Recent studies have shown the latency and energy consumption of deep neural networks can be significantly improved by splitting the network between the mobile device and cloud. This paper introduces a new deep learning architecture, called {BottleNet}, for reducing the feature size needed to be sent to the cloud. Furthermore, we propose a training method for compensating for the potential accuracy loss due to the lossy compression of features before transmitting them to the cloud. {BottleNet} achieves on average 30× improvement in end-to-end latency and 40× improvement in mobile energy consumption compared to the cloud-only approach with negligible accuracy loss.},
	journaltitle = {{arXiv}:1902.01000 [cs]},
	author = {Eshratifar, Amir Erfan and Esmaili, Amirhossein and Pedram, Massoud},
	urldate = {2019-08-26},
	date = {2019-02-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.01000},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Eshratifar et al. - 2019 - BottleNet A Deep Learning Architecture for Intell.pdf:/home/ajk/Zotero/storage/VA7BJQTM/Eshratifar et al. - 2019 - BottleNet A Deep Learning Architecture for Intell.pdf:application/pdf}
}

@inproceedings{teerapittayanon_branchynet:_2016,
	location = {Cancun},
	title = {{BranchyNet}: Fast inference via early exiting from deep neural networks},
	isbn = {978-1-5090-4847-2},
	url = {http://ieeexplore.ieee.org/document/7900006/},
	doi = {10.1109/ICPR.2016.7900006},
	shorttitle = {{BranchyNet}},
	abstract = {Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present {BranchyNet}, a novel deep network architecture that is augmented with additional side branch classiﬁers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high conﬁdence. {BranchyNet} exploits the observation that features learned at an early layer of a network may often be sufﬁcient for the classiﬁcation of many data points. For more difﬁcult samples, which are expected less frequently, {BranchyNet} will use further or all network layers to provide the best likelihood of correct prediction. We study the {BranchyNet} architecture using several well-known networks ({LeNet}, {AlexNet}, {ResNet}) and datasets ({MNIST}, {CIFAR}10) and show that it can both improve accuracy and signiﬁcantly reduce the inference time of the network.},
	eventtitle = {2016 23rd International Conference on Pattern Recognition ({ICPR})},
	pages = {2464--2469},
	booktitle = {2016 23rd International Conference on Pattern Recognition ({ICPR})},
	publisher = {{IEEE}},
	author = {Teerapittayanon, Surat and {McDanel}, Bradley and Kung, H.T.},
	urldate = {2019-08-26},
	date = {2016-12},
	langid = {english},
	file = {Teerapittayanon et al. - 2016 - BranchyNet Fast inference via early exiting from .pdf:/home/ajk/Zotero/storage/SDJSRV5Z/Teerapittayanon et al. - 2016 - BranchyNet Fast inference via early exiting from .pdf:application/pdf}
}

@article{stoica_berkeley_2017,
	title = {A Berkeley View of Systems Challenges for {AI}},
	url = {http://arxiv.org/abs/1712.05855},
	abstract = {With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, {AI} (Arti cial Intelligence) has moved from research labs to production. ese changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems so ware and architectures, and by the broad accessibility of these technologies.},
	journaltitle = {{arXiv}:1712.05855 [cs]},
	author = {Stoica, Ion and Song, Dawn and Popa, Raluca Ada and Patterson, David and Mahoney, Michael W. and Katz, Randy and Joseph, Anthony D. and Jordan, Michael and Hellerstein, Joseph M. and Gonzalez, Joseph E. and Goldberg, Ken and Ghodsi, Ali and Culler, David and Abbeel, Pieter},
	urldate = {2019-09-02},
	date = {2017-12-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.05855},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Stoica et al. - 2017 - A Berkeley View of Systems Challenges for AI.pdf:/home/ajk/Zotero/storage/XWTWHH3J/Stoica et al. - 2017 - A Berkeley View of Systems Challenges for AI.pdf:application/pdf}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than {VGG} nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classiﬁcation task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2019-09-04},
	date = {2015-12-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/ajk/Zotero/storage/ZLU3X59T/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://arxiv.org/abs/1411.1792},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the ﬁrst layer they learn features similar to Gabor ﬁlters and color blobs. Such ﬁrst-layer features appear not to be speciﬁc to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to speciﬁc by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus speciﬁcity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difﬁculties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on {ImageNet}, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A ﬁnal surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after ﬁne-tuning to the target dataset.},
	journaltitle = {{arXiv}:1411.1792 [cs]},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	urldate = {2019-09-16},
	date = {2014-11-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1411.1792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:/home/ajk/Zotero/storage/JIFRNQES/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:application/pdf}
}

@article{masters_revisiting_nodate,
	title = {Revisiting Small Batch Training for Deep Neural Networks},
	volume = {abs/1804.07612},
	url = {http://arxiv.org/abs/1804.07612},
	author = {Masters, Dominic and Luschi, Carlos},
	urldate = {2018-08-13},
	file = {1804.07612.pdf:/home/ajk/Zotero/storage/RMHUUJQ3/1804.07612.pdf:application/pdf}
}

@article{loshchilov_sgdr:_2016,
	title = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
	url = {http://arxiv.org/abs/1608.03983},
	shorttitle = {{SGDR}},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the {CIFAR}-10 and {CIFAR}-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of {EEG} recordings and on a downsampled version of the {ImageNet} dataset. Our source code is available at https://github.com/loshchil/{SGDR}},
	journaltitle = {{arXiv}:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2019-09-23},
	date = {2016-08-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.03983},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {Loshchilov og Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:/home/ajk/Zotero/storage/8ET8UWKA/Loshchilov og Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the inﬁnity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2019-09-23},
	date = {2014-12-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma og Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:/home/ajk/Zotero/storage/FN4U3UGE/Kingma og Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{cubuk_autoaugment:_2018,
	title = {{AutoAugment}: Learning Augmentation Policies from Data},
	url = {http://arxiv.org/abs/1805.09501},
	shorttitle = {{AutoAugment}},
	abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classiﬁers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called {AutoAugment} to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to ﬁnd the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on {CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet} (without additional data). On {ImageNet}, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On {CIFAR}-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-theart. Augmentation policies we ﬁnd are transferable between datasets. The policy learned on {ImageNet} transfers well to achieve signiﬁcant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-{IIT} Pets, {FGVC} Aircraft, and Stanford Cars.},
	journaltitle = {{arXiv}:1805.09501 [cs, stat]},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	urldate = {2019-09-23},
	date = {2018-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.09501},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Cubuk m.fl. - 2018 - AutoAugment Learning Augmentation Policies from D.pdf:/home/ajk/Zotero/storage/VQSTXHB2/Cubuk m.fl. - 2018 - AutoAugment Learning Augmentation Policies from D.pdf:application/pdf}
}

@article{perez_effectiveness_2017,
	title = {The Effectiveness of Data Augmentation in Image Classification using Deep Learning},
	url = {http://arxiv.org/abs/1712.04621},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classiﬁcation. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and ﬂipping input images. We artiﬁcially constrain our access to data to a small subset of the {ImageNet} dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with {GANs} to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classiﬁer, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	journaltitle = {{arXiv}:1712.04621 [cs]},
	author = {Perez, Luis and Wang, Jason},
	urldate = {2019-09-23},
	date = {2017-12-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.04621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Perez og Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:/home/ajk/Zotero/storage/2H2SMSN4/Perez og Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:application/pdf}
}

@article{shorten_survey_2019,
	title = {A survey on Image Data Augmentation for Deep Learning},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on {GANs} are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	pages = {60},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	urldate = {2019-09-23},
	date = {2019-12},
	langid = {english},
	file = {Shorten og Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:/home/ajk/Zotero/storage/753MNRMT/Shorten og Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf}
}

@inproceedings{lecun_lecun-98.pdf_1998,
	title = {lecun-98.pdf},
	volume = {86},
	url = {http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf},
	doi = {10.1109/5.726791},
	series = {11},
	eventtitle = {{IEEE}},
	pages = {2278--2324},
	booktitle = {Gradient-Based Learning Applied to Document Recognition},
	publisher = {{IEEE}},
	author = {{LeCun}, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	urldate = {2019-09-24},
	date = {1998-11},
	file = {lecun-98.pdf:/home/ajk/Zotero/storage/AWUVWAFU/lecun-98.pdf:application/pdf}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient {GPU} implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2019-09-24},
	date = {2017-05-24},
	langid = {english},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/ajk/Zotero/storage/2I6MP5RR/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@article{lecun_mnist_2010,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	author = {{LeCun}, Yann and Cortes, Corinna},
	urldate = {2016-01-14},
	date = {2010},
	keywords = {{MSc} \_checked character\_recognition mnist network neural}
}

@article{krizhevsky_cifar-10_nodate,
	title = {{CIFAR}-10 (Canadian Institute for Advanced Research)},
	url = {http://www.cs.toronto.edu/ kriz/cifar.html},
	abstract = {The {CIFAR}-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.},
	author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
	keywords = {Dataset}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision ({IJCV})},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015}
}

@inproceedings{kang_neurosurgeon:_2017,
	location = {Xi'an, China},
	title = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge},
	isbn = {978-1-4503-4465-4},
	url = {http://dl.acm.org/citation.cfm?doid=3037697.3037698},
	doi = {10.1145/3037697.3037698},
	shorttitle = {Neurosurgeon},
	abstract = {The computation for today’s intelligent personal assistants such as Apple Siri, Google Now, and Microsoft Cortana, is performed in the cloud. This cloud-only approach requires signiﬁcant amounts of data to be sent to the cloud over the wireless network and puts signiﬁcant computational pressure on the datacenter. However, as the computational resources in mobile devices become more powerful and energy efﬁcient, questions arise as to whether this cloud-only processing is desirable moving forward, and what are the implications of pushing some or all of this compute to the mobile devices on the edge.},
	eventtitle = {the Twenty-Second International Conference},
	pages = {615--629},
	booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems - {ASPLOS} '17},
	publisher = {{ACM} Press},
	author = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
	urldate = {2019-09-24},
	date = {2017},
	langid = {english},
	file = {Kang et al. - 2017 - Neurosurgeon Collaborative Intelligence Between t.pdf:/home/ajk/Zotero/storage/872F63RZ/Kang et al. - 2017 - Neurosurgeon Collaborative Intelligence Between t.pdf:application/pdf}
}

@article{bianco_benchmark_2018,
	title = {Benchmark Analysis of Representative Deep Neural Network Architectures},
	volume = {6},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/1810.00736},
	doi = {10.1109/ACCESS.2018.2877890},
	abstract = {This work presents an in-depth analysis of the majority of the deep neural networks ({DNNs}) proposed in the state of the art for image recognition. For each {DNN} multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of {DNNs} on two different computer architectures, a workstation equipped with a {NVIDIA} Titan X Pascal and an embedded system based on a {NVIDIA} Jetson {TX}1 board. This experimentation allows a direct comparison between {DNNs} running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the {DNN} architecture(s) that better ﬁt the resource constraints of practical deployments and applications. To complete this work, all the {DNNs}, as well as the software used for the analysis, are available online.},
	pages = {64270--64277},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
	urldate = {2019-09-24},
	date = {2018},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.00736},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bianco et al. - 2018 - Benchmark Analysis of Representative Deep Neural N.pdf:/home/ajk/Zotero/storage/2AULW7SE/Bianco et al. - 2018 - Benchmark Analysis of Representative Deep Neural N.pdf:application/pdf}
}

@article{vinyals_matching_2016,
	title = {Matching Networks for One Shot Learning},
	url = {http://arxiv.org/abs/1606.04080},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, {ImageNet}) and language tasks. Our algorithm improves one-shot accuracy on {ImageNet} from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	journaltitle = {{arXiv}:1606.04080 [cs, stat]},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	urldate = {2019-09-25},
	date = {2016-06-13},
	eprinttype = {arxiv},
	eprint = {1606.04080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.04080 PDF:/home/ajk/Zotero/storage/W2BTNZ3Z/Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ajk/Zotero/storage/AT467L6J/1606.html:text/html}
}

@article{choi_near-lossless_2018,
	title = {Near-Lossless Deep Feature Compression for Collaborative Intelligence},
	url = {http://arxiv.org/abs/1804.09963},
	abstract = {Collaborative intelligence is a new paradigm for efficient deployment of deep neural networks across the mobile-cloud infrastructure. By dividing the network between the mobile and the cloud, it is possible to distribute the computational workload such that the overall energy and/or latency of the system is minimized. However, this necessitates sending deep feature data from the mobile to the cloud in order to perform inference. In this work, we examine the differences between the deep feature data and natural image data, and propose a simple and effective near-lossless deep feature compressor. The proposed method achieves up to 5\% bit rate reduction compared to {HEVC}-Intra and even more against other popular image codecs. Finally, we suggest an approach for reconstructing the input image from compressed deep features in the cloud, that could serve to supplement the inference performed by the deep model.},
	journaltitle = {{arXiv}:1804.09963 [cs, eess]},
	author = {Choi, Hyomin and Bajic, Ivan V.},
	urldate = {2019-09-25},
	date = {2018-04-26},
	eprinttype = {arxiv},
	eprint = {1804.09963},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv\:1804.09963 PDF:/home/ajk/Zotero/storage/AW9VZH5X/Choi and Bajic - 2018 - Near-Lossless Deep Feature Compression for Collabo.pdf:application/pdf;arXiv.org Snapshot:/home/ajk/Zotero/storage/E3VXC9TP/1804.html:text/html}
}

@article{choi_deep_2018,
	title = {Deep feature compression for collaborative object detection},
	url = {http://arxiv.org/abs/1802.03931},
	abstract = {Recent studies have shown that the efficiency of deep neural networks in mobile applications can be significantly improved by distributing the computational workload between the mobile device and the cloud. This paradigm, termed collaborative intelligence, involves communicating feature data between the mobile and the cloud. The efficiency of such approach can be further improved by lossy compression of feature data, which has not been examined to date. In this work we focus on collaborative object detection and study the impact of both near-lossless and lossy compression of feature data on its accuracy. We also propose a strategy for improving the accuracy under lossy feature compression. Experiments indicate that using this strategy, the communication overhead can be reduced by up to 70\% without sacrificing accuracy.},
	journaltitle = {{arXiv}:1802.03931 [cs]},
	author = {Choi, Hyomin and Bajic, Ivan V.},
	urldate = {2019-09-25},
	date = {2018-02-12},
	eprinttype = {arxiv},
	eprint = {1802.03931},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1802.03931 PDF:/home/ajk/Zotero/storage/JYLYTVEE/Choi and Bajic - 2018 - Deep feature compression for collaborative object .pdf:application/pdf;arXiv.org Snapshot:/home/ajk/Zotero/storage/NC2AK7KM/1802.html:text/html}
}

@article{ko_edge-host_2018,
	title = {Edge-Host Partitioning of Deep Neural Networks with Feature Space Encoding for Resource-Constrained Internet-of-Things Platforms},
	url = {http://arxiv.org/abs/1802.03835},
	abstract = {This paper introduces partitioning an inference task of a deep neural network between an edge and a host platform in the {IoT} environment. We present a {DNN} as an encoding pipeline, and propose to transmit the output feature space of an intermediate layer to the host. The lossless or lossy encoding of the feature space is proposed to enhance the maximum input rate supported by the edge platform and/or reduce the energy of the edge platform. Simulation results show that partitioning a {DNN} at the end of convolutional (feature extraction) layers coupled with feature space encoding enables significant improvement in the energy-efficiency and throughput over the baseline configurations that perform the entire inference at the edge or at the host.},
	journaltitle = {{arXiv}:1802.03835 [cs]},
	author = {Ko, Jong Hwan and Na, Taesik and Amir, Mohammad Faisal and Mukhopadhyay, Saibal},
	urldate = {2019-09-25},
	date = {2018-02-11},
	eprinttype = {arxiv},
	eprint = {1802.03835},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1802.03835 PDF:/home/ajk/Zotero/storage/UIDPVEST/Ko et al. - 2018 - Edge-Host Partitioning of Deep Neural Networks wit.pdf:application/pdf;arXiv.org Snapshot:/home/ajk/Zotero/storage/KXCFSFWL/1802.html:text/html}
}

@inproceedings{park_big/little_2015,
	title = {Big/little deep neural network for ultra low power inference},
	doi = {10.1109/CODESISSS.2015.7331375},
	abstract = {Deep neural networks ({DNNs}) have recently proved their effectiveness in complex data analyses such as object/speech recognition. As their applications are being expanded to mobile devices, their energy efficiencies are becoming critical. In this paper, we propose a novel concept called big/{LITTLE} {DNN} ({BL}-{DNN}) which significantly reduces energy consumption required for {DNN} execution at a negligible loss of inference accuracy. The {BL}-{DNN} consists of a little {DNN} (consuming low energy) and a full-fledged big {DNN}. In order to reduce energy consumption, the {BL}-{DNN} aims at avoiding the execution of the big {DNN} whenever possible. The key idea for this goal is to execute the little {DNN} first for inference (without big {DNN} execution) and simply use its result as the final inference result as long as the result is estimated to be accurate. On the other hand, if the result from the little {DNN} is not considered to be accurate, the big {DNN} is executed to give the final inference result. This approach reduces the total energy consumption by obtaining the inference result only with the little, energy-efficient {DNN} in most cases, while maintaining the similar level of inference accuracy through selectively utilizing the big {DNN} execution. We present design-time and runtime methods to control the execution of big {DNN} under a trade-off between energy consumption and inference accuracy. Experiments with state-of-the-art {DNNs} for {ImageNet} and {MNIST} show that our proposed {BL}-{DNN} can offer up to 53.7\% ({ImageNet}) and 94.1\% ({MNIST}) reductions in energy consumption at a loss of 0.90\% ({ImageNet}) and 0.12\% ({MNIST}) in inference accuracy, respectively.},
	eventtitle = {2015 International Conference on Hardware/Software Codesign and System Synthesis ({CODES}+{ISSS})},
	pages = {124--132},
	booktitle = {2015 International Conference on Hardware/Software Codesign and System Synthesis ({CODES}+{ISSS})},
	author = {Park, E. and Kim, D. and Kim, S. and Kim, Y. and Kim, G. and Yoon, S. and Yoo, S.},
	date = {2015-10},
	keywords = {Accuracy, big/little deep neural network, Biological neural networks, {BL}-{DNN}, complex data analyses, data analysis, Deep neural network, energy consumption, Energy consumption, Energy efficiency, Hardware, {ImageNet}, inference mechanisms, low power, Memory management, {MNIST}, neural nets, Neurons, ultra low power inference},
	file = {IEEE Xplore Abstract Record:/home/ajk/Zotero/storage/YFDRYI6K/7331375.html:text/html;IEEE Xplore Full Text PDF:/home/ajk/Zotero/storage/U24CJXJC/Park et al. - 2015 - Biglittle deep neural network for ultra low power.pdf:application/pdf}
}

@article{tann_flexible_2018,
	title = {Flexible Deep Neural Network Processing},
	url = {http://arxiv.org/abs/1801.07353},
	abstract = {The recent success of Deep Neural Networks ({DNNs}) has drastically improved the state of the art for many application domains. While achieving high accuracy performance, deploying state-of-the-art {DNNs} is a challenge since they typically require billions of expensive arithmetic computations. In addition, {DNNs} are typically deployed in ensemble to boost accuracy performance, which further exacerbates the system requirements. This computational overhead is an issue for many platforms, e.g. data centers and embedded systems, with tight latency and energy budgets. In this article, we introduce flexible {DNNs} ensemble processing technique, which achieves large reduction in average inference latency while incurring small to negligible accuracy drop. Our technique is flexible in that it allows for dynamic adaptation between quality of results ({QoR}) and execution runtime. We demonstrate the effectiveness of the technique on {AlexNet} and {ResNet}-50 using the {ImageNet} dataset. This technique can also easily handle other types of networks.},
	journaltitle = {{arXiv}:1801.07353 [cs, stat]},
	author = {Tann, Hokchhay and Hashemi, Soheil and Reda, Sherief},
	urldate = {2019-09-25},
	date = {2018-01-22},
	eprinttype = {arxiv},
	eprint = {1801.07353},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1801.07353 PDF:/home/ajk/Zotero/storage/WFCZDTM7/Tann et al. - 2018 - Flexible Deep Neural Network Processing.pdf:application/pdf;arXiv.org Snapshot:/home/ajk/Zotero/storage/GM4MT6KL/1801.html:text/html}
}

@article{wu_blockdrop:_2017,
	title = {{BlockDrop}: Dynamic Inference Paths in Residual Networks},
	url = {http://arxiv.org/abs/1711.08393},
	shorttitle = {{BlockDrop}},
	abstract = {Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce {BlockDrop}, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks ({ResNets}) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained {ResNet}, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on {CIFAR} and {ImageNet}. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a {ResNet}-101 model, our method achieves a speedup of 20{\textbackslash}\% on average, going as high as 36{\textbackslash}\% for some images, while maintaining the same 76.4{\textbackslash}\% top-1 accuracy on {ImageNet}.},
	journaltitle = {{arXiv}:1711.08393 [cs]},
	author = {Wu, Zuxuan and Nagarajan, Tushar and Kumar, Abhishek and Rennie, Steven and Davis, Larry S. and Grauman, Kristen and Feris, Rogerio},
	urldate = {2019-09-25},
	date = {2017-11-22},
	eprinttype = {arxiv},
	eprint = {1711.08393},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1711.08393 PDF:/home/ajk/Zotero/storage/CWAR22UK/Wu et al. - 2017 - BlockDrop Dynamic Inference Paths in Residual Net.pdf:application/pdf;arXiv.org Snapshot:/home/ajk/Zotero/storage/QHQJIMGR/1711.html:text/html}
}

@book{goodfellow_deep_2016,
	title = {Deep Learning},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016}
}