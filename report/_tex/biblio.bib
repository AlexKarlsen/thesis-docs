
@inproceedings{deng_imagenet:_2009,
	title = {{ImageNet}: A Large-Scale Hierarchical Image Database},
	booktitle = {{CVPR}09},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
	date = {2009}
}

@article{masters_revisiting_nodate,
	title = {Revisiting Small Batch Training for Deep Neural Networks},
	volume = {abs/1804.07612},
	url = {http://arxiv.org/abs/1804.07612},
	author = {Masters, Dominic and Luschi, Carlos},
	urldate = {2018-08-13},
	file = {1804.07612.pdf:C\:\\Users\\ajk\\Zotero\\storage\\RMHUUJQ3\\1804.07612.pdf:application/pdf}
}

@article{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://arxiv.org/abs/1411.1792},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the ﬁrst layer they learn features similar to Gabor ﬁlters and color blobs. Such ﬁrst-layer features appear not to be speciﬁc to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to speciﬁc by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus speciﬁcity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difﬁculties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on {ImageNet}, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A ﬁnal surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after ﬁne-tuning to the target dataset.},
	journaltitle = {{arXiv}:1411.1792 [cs]},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	urldate = {2019-09-16},
	date = {2014-11-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1411.1792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:C\:\\Users\\ajk\\Zotero\\storage\\JIFRNQES\\Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:application/pdf}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than {VGG} nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classiﬁcation task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2019-09-04},
	date = {2015-12-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:C\:\\Users\\ajk\\Zotero\\storage\\ZLU3X59T\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{stoica_berkeley_2017,
	title = {A Berkeley View of Systems Challenges for {AI}},
	url = {http://arxiv.org/abs/1712.05855},
	abstract = {With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, {AI} (Arti cial Intelligence) has moved from research labs to production. ese changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems so ware and architectures, and by the broad accessibility of these technologies.},
	journaltitle = {{arXiv}:1712.05855 [cs]},
	author = {Stoica, Ion and Song, Dawn and Popa, Raluca Ada and Patterson, David and Mahoney, Michael W. and Katz, Randy and Joseph, Anthony D. and Jordan, Michael and Hellerstein, Joseph M. and Gonzalez, Joseph E. and Goldberg, Ken and Ghodsi, Ali and Culler, David and Abbeel, Pieter},
	urldate = {2019-09-02},
	date = {2017-12-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.05855},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Stoica et al. - 2017 - A Berkeley View of Systems Challenges for AI.pdf:C\:\\Users\\ajk\\Zotero\\storage\\XWTWHH3J\\Stoica et al. - 2017 - A Berkeley View of Systems Challenges for AI.pdf:application/pdf}
}

@inproceedings{teerapittayanon_distributed_2017,
	location = {Atlanta, {GA}, {USA}},
	title = {Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices},
	isbn = {978-1-5386-1792-2},
	url = {http://ieeexplore.ieee.org/document/7979979/},
	doi = {10.1109/ICDCS.2017.226},
	abstract = {We propose distributed deep neural networks ({DDNNs}) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network ({DNN}) in the cloud, a {DDNN} also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a {DDNN} can scale up in neural network size and scale out in geographical span. Due to its distributed nature, {DDNNs} enhance sensor fusion, system fault tolerance and data privacy for {DNN} applications. In implementing a {DDNN}, we map sections of a {DNN} onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a {DDNN} can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of ofﬂoading raw sensor data to be processed in the cloud, {DDNN} locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.},
	eventtitle = {2017 {IEEE} 37th International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {328--339},
	booktitle = {2017 {IEEE} 37th International Conference on Distributed Computing Systems ({ICDCS})},
	publisher = {{IEEE}},
	author = {Teerapittayanon, Surat and {McDanel}, Bradley and Kung, H.T.},
	urldate = {2019-08-26},
	date = {2017-06},
	langid = {english},
	file = {Teerapittayanon et al. - 2017 - Distributed Deep Neural Networks Over the Cloud, t.pdf:C\:\\Users\\ajk\\Zotero\\storage\\4HWB62A9\\Teerapittayanon et al. - 2017 - Distributed Deep Neural Networks Over the Cloud, t.pdf:application/pdf}
}

@inproceedings{teerapittayanon_branchynet:_2016,
	location = {Cancun},
	title = {{BranchyNet}: Fast inference via early exiting from deep neural networks},
	isbn = {978-1-5090-4847-2},
	url = {http://ieeexplore.ieee.org/document/7900006/},
	doi = {10.1109/ICPR.2016.7900006},
	shorttitle = {{BranchyNet}},
	abstract = {Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present {BranchyNet}, a novel deep network architecture that is augmented with additional side branch classiﬁers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high conﬁdence. {BranchyNet} exploits the observation that features learned at an early layer of a network may often be sufﬁcient for the classiﬁcation of many data points. For more difﬁcult samples, which are expected less frequently, {BranchyNet} will use further or all network layers to provide the best likelihood of correct prediction. We study the {BranchyNet} architecture using several well-known networks ({LeNet}, {AlexNet}, {ResNet}) and datasets ({MNIST}, {CIFAR}10) and show that it can both improve accuracy and signiﬁcantly reduce the inference time of the network.},
	eventtitle = {2016 23rd International Conference on Pattern Recognition ({ICPR})},
	pages = {2464--2469},
	booktitle = {2016 23rd International Conference on Pattern Recognition ({ICPR})},
	publisher = {{IEEE}},
	author = {Teerapittayanon, Surat and {McDanel}, Bradley and Kung, H.T.},
	urldate = {2019-08-26},
	date = {2016-12},
	langid = {english},
	file = {Teerapittayanon et al. - 2016 - BranchyNet Fast inference via early exiting from .pdf:C\:\\Users\\ajk\\Zotero\\storage\\SDJSRV5Z\\Teerapittayanon et al. - 2016 - BranchyNet Fast inference via early exiting from .pdf:application/pdf}
}

@article{eshratifar_bottlenet:_2019,
	title = {{BottleNet}: A Deep Learning Architecture for Intelligent Mobile Cloud Computing Services},
	url = {http://arxiv.org/abs/1902.01000},
	shorttitle = {{BottleNet}},
	abstract = {Recent studies have shown the latency and energy consumption of deep neural networks can be significantly improved by splitting the network between the mobile device and cloud. This paper introduces a new deep learning architecture, called {BottleNet}, for reducing the feature size needed to be sent to the cloud. Furthermore, we propose a training method for compensating for the potential accuracy loss due to the lossy compression of features before transmitting them to the cloud. {BottleNet} achieves on average 30× improvement in end-to-end latency and 40× improvement in mobile energy consumption compared to the cloud-only approach with negligible accuracy loss.},
	journaltitle = {{arXiv}:1902.01000 [cs]},
	author = {Eshratifar, Amir Erfan and Esmaili, Amirhossein and Pedram, Massoud},
	urldate = {2019-08-26},
	date = {2019-02-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.01000},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Eshratifar et al. - 2019 - BottleNet A Deep Learning Architecture for Intell.pdf:C\:\\Users\\ajk\\Zotero\\storage\\VA7BJQTM\\Eshratifar et al. - 2019 - BottleNet A Deep Learning Architecture for Intell.pdf:application/pdf}
}

@article{song_collaborative_2018,
	title = {Collaborative Learning for Deep Neural Networks},
	url = {http://arxiv.org/abs/1805.11761},
	abstract = {We introduce collaborative learning in which multiple classiﬁer heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classiﬁer heads on the same example provides supplementary information as well as regularization to each classiﬁer, thereby improving generalization. Second, intermediate-level representation ({ILR}) sharing with backpropagation rescaling aggregates the gradient ﬂows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on {CIFAR} and {ImageNet} datasets demonstrate that deep neural networks learned as a group in a collaborative way signiﬁcantly reduce the generalization error and increase the robustness to label noise.},
	journaltitle = {{arXiv}:1805.11761 [cs, stat]},
	author = {Song, Guocong and Chai, Wei},
	urldate = {2019-08-26},
	date = {2018-05-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.11761},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Song and Chai - 2018 - Collaborative Learning for Deep Neural Networks.pdf:C\:\\Users\\ajk\\Zotero\\storage\\WUJEKLUH\\Song and Chai - 2018 - Collaborative Learning for Deep Neural Networks.pdf:application/pdf}
}

@article{chatzopoulos_mobile_2017,
	title = {Mobile Augmented Reality Survey: From Where We Are to Where We Go},
	volume = {5},
	issn = {2169-3536},
	url = {http://ieeexplore.ieee.org/document/7912316/},
	doi = {10.1109/ACCESS.2017.2698164},
	shorttitle = {Mobile Augmented Reality Survey},
	abstract = {The boom in the capabilities and features of mobile devices, like smartphones, tablets, and wearables, combined with the ubiquitous and affordable Internet access and the advances in the areas of cooperative networking, computer vision, and mobile cloud computing transformed mobile augmented reality ({MAR}) from science ﬁction to a reality. Although mobile devices are more constrained computationalwise from traditional computers, they have a multitude of sensors that can be used to the development of more sophisticated {MAR} applications and can be assisted from remote servers for the execution of their intensive parts. In this paper, after introducing the reader to the basics of {MAR}, we present a categorization of the application ﬁelds together with some representative examples. Next, we introduce the reader to the user interface and experience in {MAR} applications and continue with the core system components of the {MAR} systems. After that, we discuss advances in tracking and registration, since their functionality is crucial to any {MAR} application and the network connectivity of the devices that run {MAR} applications together with its importance to the performance of the application. We continue with the importance of data management in {MAR} systems and the systems performance and sustainability, and before we conclude this survey, we present existing challenging problems.},
	pages = {6917--6950},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Chatzopoulos, Dimitris and Bermejo, Carlos and Huang, Zhanpeng and Hui, Pan},
	urldate = {2019-08-22},
	date = {2017},
	langid = {english},
	file = {Chatzopoulos et al. - 2017 - Mobile Augmented Reality Survey From Where We Are.pdf:C\:\\Users\\ajk\\Zotero\\storage\\7H78HKQF\\Chatzopoulos et al. - 2017 - Mobile Augmented Reality Survey From Where We Are.pdf:application/pdf}
}

@inproceedings{huang_mobile_2012,
	location = {Taipei, Taiwan},
	title = {Mobile augmented reality based on cloud computing},
	isbn = {978-1-4673-2145-7 978-1-4673-2144-0 978-1-4673-2143-3},
	url = {http://ieeexplore.ieee.org/document/6325354/},
	doi = {10.1109/ICASID.2012.6325354},
	abstract = {In this paper, we implemented a mobile augmented reality system based on cloud computing. This system uses a mobile device with a camera to capture images of book spines and sends processed features to the cloud. In the cloud, the features are compared with the database and the information of the best matched book would be sent back to the mobile device. The information will then be rendered on the display via augmented reality. In order to reduce the transmission cost, the mobile device is used to perform most of the image processing tasks, such as the preprocessing, resizing, corner detection, and augmented reality rendering. On the other hand, the cloud is used to realize routine but large quantity feature comparisons. Using the cloud as the database also makes the future extension much more easily. For our prototype system, we use an Android smart phone as our mobile device, and Chunghwa Telecoms hicloud as the cloud.},
	eventtitle = {2012 International Conference on Anti-Counterfeiting, Security and Identification (2012 {ASID})},
	pages = {1--5},
	booktitle = {Anti-counterfeiting, Security, and Identification},
	publisher = {{IEEE}},
	author = {Huang, Bai-Ruei and Lin, Chang Hong and Lee, Chia-Han},
	urldate = {2019-08-22},
	date = {2012-08},
	langid = {english},
	file = {Huang et al. - 2012 - Mobile augmented reality based on cloud computing.pdf:C\:\\Users\\ajk\\Zotero\\storage\\DPGV8NAL\\Huang et al. - 2012 - Mobile augmented reality based on cloud computing.pdf:application/pdf}
}

@inproceedings{liu_dare:_2018,
	location = {Cambridge},
	title = {{DARE}: Dynamic Adaptive Mobile Augmented Reality with Edge Computing},
	isbn = {978-1-5386-6043-0},
	url = {https://ieeexplore.ieee.org/document/8526799/},
	doi = {10.1109/ICNP.2018.00011},
	shorttitle = {{DARE}},
	abstract = {Mobile augmented reality ({MAR}) is a killer application of mobile edge computing because of its high computation demand and stringent latency requirement. Since edge networks and computing resources are highly dynamic, handling such dynamics is essential for providing high-quality {MAR} services. In this paper, we design a new network protocol named {DARE} (dynamic adaptive {AR} over the edge) that enables mobile users to dynamically change their {AR} conﬁgurations according to wireless channel conditions and computation workloads in edge servers. The dynamic conﬁguration adaptations reduce the service latency of {MAR} users and maximize the quality of augmentation ({QoA}) under varying network conditions and computation workloads. Considering the video frame size and computation model, i.e., object detection algorithms, as two key parameters in adapting the {AR} conﬁguration, we develop analytical models to characterize the impact of these parameters on {QoA} and the service latency. Then, we design optimization mechanisms on both the edge server and {AR} devices to guide the {AR} conﬁguration adaptation and server computation resource allocation. The performance of the {DARE} protocol is validated through a small-scale testbed implementation.},
	eventtitle = {2018 {IEEE} 26th International Conference on Network Protocols ({ICNP})},
	pages = {1--11},
	booktitle = {2018 {IEEE} 26th International Conference on Network Protocols ({ICNP})},
	publisher = {{IEEE}},
	author = {Liu, Qiang and Han, Tao},
	urldate = {2019-08-22},
	date = {2018-09},
	langid = {english},
	file = {Liu and Han - 2018 - DARE Dynamic Adaptive Mobile Augmented Reality wi.pdf:C\:\\Users\\ajk\\Zotero\\storage\\9FXHMG58\\Liu and Han - 2018 - DARE Dynamic Adaptive Mobile Augmented Reality wi.pdf:application/pdf}
}

@article{akherfi_mobile_2018,
	title = {Mobile cloud computing for computation offloading: Issues and challenges},
	volume = {14},
	issn = {22108327},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210832716300400},
	doi = {10.1016/j.aci.2016.11.002},
	shorttitle = {Mobile cloud computing for computation offloading},
	abstract = {Despite the evolution and enhancements that mobile devices have experienced, they are still considered as limited computing devices. Today, users become more demanding and expect to execute computational intensive applications on their smartphone devices. Therefore, Mobile Cloud Computing ({MCC}) integrates mobile computing and Cloud Computing ({CC}) in order to extend capabilities of mobile devices using ofﬂoading techniques. Computation ofﬂoading tackles limitations of Smart Mobile Devices ({SMDs}) such as limited battery lifetime, limited processing capabilities, and limited storage capacity by ofﬂoading the execution and workload to other rich systems with better performance and resources. This paper presents the current ofﬂoading frameworks, computation ofﬂoading techniques, and analyzes them along with their main critical issues. In addition, it explores different important parameters based on which the frameworks are implemented such as ofﬂoading method and level of partitioning. Finally, it summarizes the issues in ofﬂoading frameworks in the {MCC} domain that requires further research.},
	pages = {1--16},
	number = {1},
	journaltitle = {Applied Computing and Informatics},
	shortjournal = {Applied Computing and Informatics},
	author = {Akherfi, Khadija and Gerndt, Micheal and Harroud, Hamid},
	urldate = {2019-08-22},
	date = {2018-01},
	langid = {english},
	file = {Akherfi et al. - 2018 - Mobile cloud computing for computation offloading.pdf:C\:\\Users\\ajk\\Zotero\\storage\\RN9DKK9F\\Akherfi et al. - 2018 - Mobile cloud computing for computation offloading.pdf:application/pdf}
}

@article{khan_survey_2014,
	title = {A Survey of Mobile Cloud Computing Application Models},
	volume = {16},
	issn = {1553-877X, 2373-745X},
	url = {https://ieeexplore.ieee.org/document/6553297/},
	doi = {10.1109/SURV.2013.062613.00160},
	abstract = {Smartphones are now capable of supporting a wide range of applications, many of which demand an ever increasing computational power. This poses a challenge because smartphones are resource-constrained devices with limited computation power, memory, storage, and energy. Fortunately, the cloud computing technology offers virtually unlimited dynamic resources for computation, storage, and service provision. Therefore, researchers envision extending cloud computing services to mobile devices to overcome the smartphones constraints. The challenge in doing so is that the traditional smartphone application models do not support the development of applications that can incorporate cloud computing features and requires specialized mobile cloud application models. This article presents mobile cloud architecture, ofﬂoading decision affecting entities, application models classiﬁcation, the latest mobile cloud application models, their critical analysis and future research directions.},
	pages = {393--413},
	number = {1},
	journaltitle = {{IEEE} Communications Surveys \& Tutorials},
	shortjournal = {{IEEE} Commun. Surv. Tutorials},
	author = {Khan, Atta ur Rehman and Othman, Mazliza and Madani, Sajjad Ahmad and Khan, Samee Ullah},
	urldate = {2019-08-22},
	date = {2014},
	langid = {english},
	file = {Khan et al. - 2014 - A Survey of Mobile Cloud Computing Application Mod.pdf:C\:\\Users\\ajk\\Zotero\\storage\\FP24FFN3\\Khan et al. - 2014 - A Survey of Mobile Cloud Computing Application Mod.pdf:application/pdf}
}

@article{everingham_pascal_2010,
	title = {The Pascal Visual Object Classes ({VOC}) Challenge},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The {PASCAL} Visual Object Classes ({VOC}) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	pages = {303--338},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	urldate = {2019-08-22},
	date = {2010-06},
	langid = {english},
	file = {Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:C\:\\Users\\ajk\\Zotero\\storage\\G7XLVEKC\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf}
}

@article{zou_object_2019,
	title = {Object Detection in 20 Years: A Survey},
	url = {http://arxiv.org/abs/1905.05055},
	shorttitle = {Object Detection in 20 Years},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today’s object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century’s time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	journaltitle = {{arXiv}:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2019-08-22},
	date = {2019-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf:C\:\\Users\\ajk\\Zotero\\storage\\GRTHFJJ8\\Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf:application/pdf}
}

@inproceedings{redmon_you_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {779--788},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2019-08-22},
	date = {2016-06},
	langid = {english},
	file = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:C\:\\Users\\ajk\\Zotero\\storage\\C5QM7YGV\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}

@article{liu_ssd:_2016,
	title = {{SSD}: Single Shot {MultiBox} Detector},
	volume = {9905},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	shorttitle = {{SSD}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named {SSD}, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. {SSD} is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes {SSD} easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the {PASCAL} {VOC}, {COCO}, and {ILSVRC} datasets conﬁrm that {SSD} has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, {SSD} achieves 74.3\% {mAP}1 on {VOC}2007 test at 59 {FPS} on a Nvidia Titan X and for 512 × 512 input, {SSD} achieves 76.9\% {mAP}, outperforming a comparable state-of-the-art Faster R-{CNN} model. Compared to other single stage methods, {SSD} has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	pages = {21--37},
	journaltitle = {{arXiv}:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	urldate = {2019-08-22},
	date = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:C\:\\Users\\ajk\\Zotero\\storage\\C94N3WCT\\Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf}
}

@article{sandler_mobilenetv2:_2018,
	title = {{MobileNetV}2: Inverted Residuals and Linear Bottlenecks},
	url = {http://arxiv.org/abs/1801.04381},
	shorttitle = {{MobileNetV}2},
	abstract = {In this paper we describe a new mobile architecture, {MobileNetV}2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call {SSDLite}. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of {DeepLabv}3 which we call Mobile {DeepLabv}3.},
	journaltitle = {{arXiv}:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	urldate = {2019-08-22},
	date = {2018-01-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:C\:\\Users\\ajk\\Zotero\\storage\\DCUGD8XB\\Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf}
}

@inproceedings{teerapittayanon_distributed_2017-1,
	location = {Atlanta, {GA}, {USA}},
	title = {Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices},
	isbn = {978-1-5386-1792-2},
	url = {http://ieeexplore.ieee.org/document/7979979/},
	doi = {10.1109/ICDCS.2017.226},
	abstract = {We propose distributed deep neural networks ({DDNNs}) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network ({DNN}) in the cloud, a {DDNN} also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a {DDNN} can scale up in neural network size and scale out in geographical span. Due to its distributed nature, {DDNNs} enhance sensor fusion, system fault tolerance and data privacy for {DNN} applications. In implementing a {DDNN}, we map sections of a {DNN} onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a {DDNN} can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of ofﬂoading raw sensor data to be processed in the cloud, {DDNN} locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.},
	eventtitle = {2017 {IEEE} 37th International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {328--339},
	booktitle = {2017 {IEEE} 37th International Conference on Distributed Computing Systems ({ICDCS})},
	publisher = {{IEEE}},
	author = {Teerapittayanon, Surat and {McDanel}, Bradley and Kung, H.T.},
	urldate = {2019-08-22},
	date = {2017-06},
	langid = {english},
	file = {Teerapittayanon et al. - 2017 - Distributed Deep Neural Networks Over the Cloud, t.pdf:C\:\\Users\\ajk\\Zotero\\storage\\A4QIEKK3\\Teerapittayanon et al. - 2017 - Distributed Deep Neural Networks Over the Cloud, t.pdf:application/pdf}
}

@article{courbariaux_binaryconnect:_2015,
	title = {{BinaryConnect}: Training Deep Neural Networks with binary weights during propagations},
	url = {http://arxiv.org/abs/1511.00363},
	shorttitle = {{BinaryConnect}},
	abstract = {Deep Neural Networks ({DNN}) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, {GPUs} enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning ({DL}). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great beneﬁts to specialized {DL} hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce {BinaryConnect}, a method which consists in training a {DNN} with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that {BinaryConnect} acts as regularizer and we obtain near state-of-the-art results with {BinaryConnect} on the permutation-invariant {MNIST}, {CIFAR}-10 and {SVHN}.},
	journaltitle = {{arXiv}:1511.00363 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	urldate = {2019-08-22},
	date = {2015-11-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.00363},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {Courbariaux et al. - 2015 - BinaryConnect Training Deep Neural Networks with .pdf:C\:\\Users\\ajk\\Zotero\\storage\\FPAS4UN2\\Courbariaux et al. - 2015 - BinaryConnect Training Deep Neural Networks with .pdf:application/pdf}
}

@article{zhou_edge_2019,
	title = {Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing},
	volume = {107},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/8736011/},
	doi = {10.1109/JPROC.2019.2918951},
	shorttitle = {Edge Intelligence},
	pages = {1738--1762},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
	urldate = {2019-08-22},
	date = {2019-08},
	langid = {english},
	file = {Zhou et al. - 2019 - Edge Intelligence Paving the Last Mile of Artific.pdf:C\:\\Users\\ajk\\Zotero\\storage\\UF4HPULQ\\Zhou et al. - 2019 - Edge Intelligence Paving the Last Mile of Artific.pdf:application/pdf}
}

@article{mach_mobile_2017,
	title = {Mobile Edge Computing: A Survey on Architecture and Computation Offloading},
	volume = {19},
	issn = {1553-877X},
	url = {http://ieeexplore.ieee.org/document/7879258/},
	doi = {10.1109/COMST.2017.2682318},
	shorttitle = {Mobile Edge Computing},
	abstract = {Technological evolution of mobile user equipment ({UEs}), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the {UEs} is constrained by limited battery capacity and energy consumption of the {UEs}. A suitable solution extending the battery life-time of the {UEs} is to ofﬂoad the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces signiﬁcant execution delay consisting of delivery of the ofﬂoaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the ofﬂoading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing ({MEC}), has been introduced. The {MEC} brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the {UE} while meeting strict delay requirements. The {MEC} computing resources can be exploited also by operators and third parties for speciﬁc purposes. In this paper, we ﬁrst describe major use cases and reference scenarios where the {MEC} is applicable. After that we survey existing concepts integrating {MEC} functionalities to the mobile networks and discuss current advancement in standardization of the {MEC}. The core of this survey is, then, focused on user-oriented use case in the {MEC}, i.e., computation ofﬂoading. In this regard, we divide the research on computation ofﬂoading to three key areas: 1) decision on computation ofﬂoading; 2) allocation of computing resource within the {MEC}; and 3) mobility management. Finally, we highlight lessons learned in area of the {MEC} and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the {MEC}.},
	pages = {1628--1656},
	number = {3},
	journaltitle = {{IEEE} Communications Surveys \& Tutorials},
	shortjournal = {{IEEE} Commun. Surv. Tutorials},
	author = {Mach, Pavel and Becvar, Zdenek},
	urldate = {2019-08-22},
	date = {2017},
	langid = {english},
	file = {Mach and Becvar - 2017 - Mobile Edge Computing A Survey on Architecture an.pdf:C\:\\Users\\ajk\\Zotero\\storage\\DIDJK74Y\\Mach and Becvar - 2017 - Mobile Edge Computing A Survey on Architecture an.pdf:application/pdf}
}

@inproceedings{liu_edge_2018,
	location = {Honolulu, {HI}},
	title = {An Edge Network Orchestrator for Mobile Augmented Reality},
	isbn = {978-1-5386-4128-6},
	url = {https://ieeexplore.ieee.org/document/8486241/},
	doi = {10.1109/INFOCOM.2018.8486241},
	abstract = {Mobile augmented reality ({MAR}) involves high complexity computation which cannot be performed efﬁciently on resource limited mobile devices. The performance of {MAR} would be signiﬁcantly improved by ofﬂoading the computation tasks to servers deployed with the close proximity to the users. In this paper, we design an edge network orchestrator to enable fast and accurate object analytics at the network edge for {MAR}. The measurement-based analytical models are built to characterize the tradeoff between the service latency and analytics accuracy in edge-based {MAR} systems. As a key component of the edge network orchestrator, a server assignment and frame resolution selection algorithm named {FACT} is proposed to mitigate the latency-accuracy tradeoff. Through network simulations, we evaluate the performance of the {FACT} algorithm and show the insights on optimizing the performance of edge-based {MAR} systems. We have implemented the edge network orchestrator and developed the corresponding communication protocol. Our experiments validate the performance of the proposed edge network orchestrator.},
	eventtitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	pages = {756--764},
	booktitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	publisher = {{IEEE}},
	author = {Liu, Qiang and Huang, Siqi and Opadere, Johnson and Han, Tao},
	urldate = {2019-08-22},
	date = {2018-04},
	langid = {english},
	file = {Liu et al. - 2018 - An Edge Network Orchestrator for Mobile Augmented .pdf:C\:\\Users\\ajk\\Zotero\\storage\\Q8K9S35Q\\Liu et al. - 2018 - An Edge Network Orchestrator for Mobile Augmented .pdf:application/pdf}
}

@article{huang_distributed_2018,
	title = {Distributed Deep Learning-based Offloading for Mobile Edge Computing Networks},
	issn = {1383-469X, 1572-8153},
	url = {http://link.springer.com/10.1007/s11036-018-1177-x},
	doi = {10.1007/s11036-018-1177-x},
	abstract = {This paper studies mobile edge computing ({MEC}) networks where multiple wireless devices ({WDs}) choose to offload their computation tasks to an edge server. To conserve energy and maintain quality of service for {WDs}, the optimization of joint offloading decision and bandwidth allocation is formulated as a mixed integer programming problem. However, the problem is computationally limited by the curse of dimensionality, which cannot be solved by general optimization tools in an effective and efficient way, especially for large-scale {WDs}. In this paper, we propose a distributed deep learning-based offloading ({DDLO}) algorithm for {MEC} networks, where multiple parallel {DNNs} are used to generate offloading decisions. We adopt a shared replay memory to store newly generated offloading decisions which are further to train and improve all {DNNs}. Extensive numerical results show that the proposed {DDLO} algorithm can generate near-optimal offloading decisions in less than one second.},
	journaltitle = {Mobile Networks and Applications},
	shortjournal = {Mobile Netw Appl},
	author = {Huang, Liang and Feng, Xu and Feng, Anqi and Huang, Yupin and Qian, Li Ping},
	urldate = {2019-08-22},
	date = {2018-11-29},
	langid = {english},
	file = {Huang et al. - 2018 - Distributed Deep Learning-based Offloading for Mob.pdf:C\:\\Users\\ajk\\Zotero\\storage\\NHDB8GSQ\\Huang et al. - 2018 - Distributed Deep Learning-based Offloading for Mob.pdf:application/pdf}
}

@article{shi_edge_2016,
	title = {Edge Computing: Vision and Challenges},
	volume = {3},
	issn = {2327-4662},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	shorttitle = {Edge Computing},
	abstract = {The proliferation of Internet of Things ({IoT}) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the deﬁnition of edge computing, followed by several case studies, ranging from cloud ofﬂoading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the ﬁeld of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	pages = {637--646},
	number = {5},
	journaltitle = {{IEEE} Internet of Things Journal},
	shortjournal = {{IEEE} Internet Things J.},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	urldate = {2019-08-22},
	date = {2016-10},
	langid = {english},
	file = {Shi et al. - 2016 - Edge Computing Vision and Challenges.pdf:C\:\\Users\\ajk\\Zotero\\storage\\QW5JS9YX\\Shi et al. - 2016 - Edge Computing Vision and Challenges.pdf:application/pdf}
}

@inproceedings{yu_computation_2017,
	location = {Montreal, {QC}},
	title = {Computation offloading for mobile edge computing: A deep learning approach},
	isbn = {978-1-5386-3529-2 978-1-5386-3531-5},
	url = {http://ieeexplore.ieee.org/document/8292514/},
	doi = {10.1109/PIMRC.2017.8292514},
	shorttitle = {Computation offloading for mobile edge computing},
	abstract = {Computation ofﬂoading has already shown itself to be successful for enabling resource-intensive applications on mobile devices. Moreover, in view of mobile edge computing ({MEC}) system, mobile devices can ofﬂoad compute-intensive tasks to a nearby cloudlet, so as to save the energy and enhance the processing speed. However, due to the varying network conditions and limited computation resources of cloudlets, the ofﬂoading actions taken by a mobile user may not achieve the lowest cost. In this paper, we develop a dynamic ofﬂoading framework for mobile users, considering the local overhead in the mobile terminal side, as well as the limited communication and computation resources in the network side. We formulate the ofﬂoading decision problem as a multi-label classiﬁcation problem and develop the Deep Supervised Learning ({DSL}) method to minimize the computation and ofﬂoading overhead. Simulation results show that our proposal can reduce system cost up to 49.24\%, 23.87\%, 15.69\%, and 11.18\% compared to the “no ofﬂoading” scheme, “random ofﬂoading” scheme, “total ofﬂoading” scheme and “multi-label linear classiﬁerbased ofﬂoading” scheme, respectively.},
	eventtitle = {2017 {IEEE} 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications ({PIMRC})},
	pages = {1--6},
	booktitle = {2017 {IEEE} 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications ({PIMRC})},
	publisher = {{IEEE}},
	author = {Yu, Shuai and Wang, Xin and Langar, Rami},
	urldate = {2019-08-22},
	date = {2017-10},
	langid = {english},
	file = {Yu et al. - 2017 - Computation offloading for mobile edge computing .pdf:C\:\\Users\\ajk\\Zotero\\storage\\LHP3PAZM\\Yu et al. - 2017 - Computation offloading for mobile edge computing .pdf:application/pdf}
}

@article{mao_survey_2017,
	title = {A Survey on Mobile Edge Computing: The Communication Perspective},
	volume = {19},
	issn = {1553-877X},
	url = {http://ieeexplore.ieee.org/document/8016573/},
	doi = {10.1109/COMST.2017.2745201},
	shorttitle = {A Survey on Mobile Edge Computing},
	abstract = {Driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing, from the centralized mobile cloud computing toward mobile edge computing ({MEC}). The main feature of {MEC} is to push mobile computing, network control and storage to the network edges (e.g., base stations and access points) so as to enable computation-intensive and latency-critical applications at the resource-limited mobile devices. {MEC} promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The promised gains of {MEC} have motivated extensive efforts in both academia and industry on developing the technology. A main thrust of {MEC} research is to seamlessly merge the two disciplines of wireless communications and mobile computing, resulting in a wide-range of new designs ranging from techniques for computation ofﬂoading to network architectures. This paper provides a comprehensive survey of the state-of-the-art {MEC} research with a focus on joint radio-and-computational resource management. We also discuss a set of issues, challenges, and future research directions for {MEC} research, including {MEC} system deployment, cache-enabled {MEC}, mobility management for {MEC}, green {MEC}, as well as privacy-aware {MEC}. Advancements in these directions will facilitate the transformation of {MEC} from theory to practice. Finally, we introduce recent standardization efforts on {MEC} as well as some typical {MEC} application scenarios.},
	pages = {2322--2358},
	number = {4},
	journaltitle = {{IEEE} Communications Surveys \& Tutorials},
	shortjournal = {{IEEE} Commun. Surv. Tutorials},
	author = {Mao, Yuyi and You, Changsheng and Zhang, Jun and Huang, Kaibin and Letaief, Khaled B.},
	urldate = {2019-08-22},
	date = {2017},
	langid = {english},
	file = {Mao et al. - 2017 - A Survey on Mobile Edge Computing The Communicati.pdf:C\:\\Users\\ajk\\Zotero\\storage\\YWHDJ5C4\\Mao et al. - 2017 - A Survey on Mobile Edge Computing The Communicati.pdf:application/pdf}
}

@article{skala_scalable_2015,
	title = {Scalable Distributed Computing Hierarchy: Cloud, Fog and Dew Computing},
	volume = {2},
	abstract = {The paper considers the conceptual approach for organization of the vertical hierarchical links between the scalable distributed computing paradigms: Cloud Computing, Fog Computing and Dew Computing. In this paper, the Dew Computing is described and recognized as a new structural layer in the existing distributed computing hierarchy. In the existing computing hierarchy, the Dew computing is positioned as the ground level for the Cloud and Fog computing paradigms. Vertical, complementary, hierarchical division from Cloud to Dew Computing satisﬁes the needs of high- and low-end computing demands in everyday life and work. These new computing paradigms lower the cost and improve the performance, particularly for concepts and applications such as the Internet of Things ({IoT}) and the Internet of Everything ({IoE}). In addition, the Dew computing paradigm will require new programming models that will efﬁciently reduce the complexity and improve the productivity and usability of scalable distributed computing, following the principles of High-Productivity computing.},
	pages = {9},
	number = {1},
	author = {Skala, Karolj and Davidovic, Davor and Afgan, Enis and Sovic, Ivan},
	date = {2015},
	langid = {english},
	file = {Skala et al. - 2015 - Scalable Distributed Computing Hierarchy Cloud, F.pdf:C\:\\Users\\ajk\\Zotero\\storage\\7BCBE8R2\\Skala et al. - 2015 - Scalable Distributed Computing Hierarchy Cloud, F.pdf:application/pdf}
}

@article{shorten_survey_2019,
	title = {A survey on Image Data Augmentation for Deep Learning},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on {GANs} are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	pages = {60},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	urldate = {2019-09-23},
	date = {2019-12},
	langid = {english},
	file = {Shorten og Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:C\:\\Users\\ajk\\Zotero\\storage\\753MNRMT\\Shorten og Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf}
}

@article{wang_effectiveness_nodate,
	title = {The Effectiveness of Data Augmentation in Image Classiﬁcation using Deep Learning},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classiﬁcation. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and ﬂipping input images. We artiﬁcially constrain our access to data to a small subset of the {ImageNet} dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with {GANs} to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classiﬁer, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	pages = {8},
	author = {Wang, Jason and Mall, Serra and Perez, Luis},
	langid = {english},
	file = {Wang m.fl. - The Effectiveness of Data Augmentation in Image Cl.pdf:C\:\\Users\\ajk\\Zotero\\storage\\NGV9C6W7\\Wang m.fl. - The Effectiveness of Data Augmentation in Image Cl.pdf:application/pdf}
}

@article{cubuk_autoaugment:_2018,
	title = {{AutoAugment}: Learning Augmentation Policies from Data},
	url = {http://arxiv.org/abs/1805.09501},
	shorttitle = {{AutoAugment}},
	abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classiﬁers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called {AutoAugment} to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to ﬁnd the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on {CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet} (without additional data). On {ImageNet}, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On {CIFAR}-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-theart. Augmentation policies we ﬁnd are transferable between datasets. The policy learned on {ImageNet} transfers well to achieve signiﬁcant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-{IIT} Pets, {FGVC} Aircraft, and Stanford Cars.},
	journaltitle = {{arXiv}:1805.09501 [cs, stat]},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	urldate = {2019-09-23},
	date = {2018-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.09501},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Cubuk m.fl. - 2018 - AutoAugment Learning Augmentation Policies from D.pdf:C\:\\Users\\ajk\\Zotero\\storage\\VQSTXHB2\\Cubuk m.fl. - 2018 - AutoAugment Learning Augmentation Policies from D.pdf:application/pdf}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the inﬁnity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2019-09-23},
	date = {2014-12-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma og Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:C\:\\Users\\ajk\\Zotero\\storage\\FN4U3UGE\\Kingma og Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{loshchilov_sgdr:_2016,
	title = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
	url = {http://arxiv.org/abs/1608.03983},
	shorttitle = {{SGDR}},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the {CIFAR}-10 and {CIFAR}-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of {EEG} recordings and on a downsampled version of the {ImageNet} dataset. Our source code is available at https://github.com/loshchil/{SGDR}},
	journaltitle = {{arXiv}:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2019-09-23},
	date = {2016-08-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.03983},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {Loshchilov og Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:C\:\\Users\\ajk\\Zotero\\storage\\8ET8UWKA\\Loshchilov og Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf}
}

@article{bianco_benchmark_2018,
	title = {Benchmark Analysis of Representative Deep Neural Network Architectures},
	volume = {6},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/1810.00736},
	doi = {10.1109/ACCESS.2018.2877890},
	abstract = {This work presents an in-depth analysis of the majority of the deep neural networks ({DNNs}) proposed in the state of the art for image recognition. For each {DNN} multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of {DNNs} on two different computer architectures, a workstation equipped with a {NVIDIA} Titan X Pascal and an embedded system based on a {NVIDIA} Jetson {TX}1 board. This experimentation allows a direct comparison between {DNNs} running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the {DNN} architecture(s) that better ﬁt the resource constraints of practical deployments and applications. To complete this work, all the {DNNs}, as well as the software used for the analysis, are available online.},
	pages = {64270--64277},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
	urldate = {2019-09-24},
	date = {2018},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.00736},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bianco et al. - 2018 - Benchmark Analysis of Representative Deep Neural N.pdf:C\:\\Users\\ajk\\Zotero\\storage\\2AULW7SE\\Bianco et al. - 2018 - Benchmark Analysis of Representative Deep Neural N.pdf:application/pdf}
}

@inproceedings{kang_neurosurgeon:_2017,
	location = {Xi'an, China},
	title = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge},
	isbn = {978-1-4503-4465-4},
	url = {http://dl.acm.org/citation.cfm?doid=3037697.3037698},
	doi = {10.1145/3037697.3037698},
	shorttitle = {Neurosurgeon},
	abstract = {The computation for today’s intelligent personal assistants such as Apple Siri, Google Now, and Microsoft Cortana, is performed in the cloud. This cloud-only approach requires signiﬁcant amounts of data to be sent to the cloud over the wireless network and puts signiﬁcant computational pressure on the datacenter. However, as the computational resources in mobile devices become more powerful and energy efﬁcient, questions arise as to whether this cloud-only processing is desirable moving forward, and what are the implications of pushing some or all of this compute to the mobile devices on the edge.},
	eventtitle = {the Twenty-Second International Conference},
	pages = {615--629},
	booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems - {ASPLOS} '17},
	publisher = {{ACM} Press},
	author = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
	urldate = {2019-09-24},
	date = {2017},
	langid = {english},
	file = {Kang et al. - 2017 - Neurosurgeon Collaborative Intelligence Between t.pdf:C\:\\Users\\ajk\\Zotero\\storage\\872F63RZ\\Kang et al. - 2017 - Neurosurgeon Collaborative Intelligence Between t.pdf:application/pdf}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision ({IJCV})},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015}
}

@article{krizhevsky_cifar-10_nodate,
	title = {{CIFAR}-10 (Canadian Institute for Advanced Research)},
	url = {http://www.cs.toronto.edu/ kriz/cifar.html},
	abstract = {The {CIFAR}-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.},
	author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
	keywords = {Dataset}
}

@article{lecun_mnist_2010,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	author = {{LeCun}, Yann and Cortes, Corinna},
	urldate = {2016-01-14},
	date = {2010},
	keywords = {{MSc} \_checked character\_recognition mnist network neural}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient {GPU} implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2019-09-24},
	date = {2017-05-24},
	langid = {english},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:C\:\\Users\\ajk\\Zotero\\storage\\2I6MP5RR\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@inproceedings{lecun_lecun-98.pdf_1998,
	title = {lecun-98.pdf},
	volume = {86},
	url = {http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf},
	doi = {10.1109/5.726791},
	series = {11},
	eventtitle = {{IEEE}},
	pages = {2278--2324},
	booktitle = {Gradient-Based Learning Applied to Document Recognition},
	publisher = {{IEEE}},
	author = {{LeCun}, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	urldate = {2019-09-24},
	date = {1998-11},
	file = {lecun-98.pdf:C\:\\Users\\ajk\\Zotero\\storage\\AWUVWAFU\\lecun-98.pdf:application/pdf}
}

@book{goodfellow_deep_2016,
	title = {Deep Learning},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
	file = {Goodfellow m.fl. - 2016 - Deep Learning.pdf:C\:\\Users\\ajk\\Zotero\\storage\\R2LXCUCM\\Goodfellow m.fl. - 2016 - Deep Learning.pdf:application/pdf}
}

@article{wu_blockdrop:_2017,
	title = {{BlockDrop}: Dynamic Inference Paths in Residual Networks},
	url = {http://arxiv.org/abs/1711.08393},
	shorttitle = {{BlockDrop}},
	abstract = {Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce {BlockDrop}, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks ({ResNets}) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained {ResNet}, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on {CIFAR} and {ImageNet}. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a {ResNet}-101 model, our method achieves a speedup of 20{\textbackslash}\% on average, going as high as 36{\textbackslash}\% for some images, while maintaining the same 76.4{\textbackslash}\% top-1 accuracy on {ImageNet}.},
	journaltitle = {{arXiv}:1711.08393 [cs]},
	author = {Wu, Zuxuan and Nagarajan, Tushar and Kumar, Abhishek and Rennie, Steven and Davis, Larry S. and Grauman, Kristen and Feris, Rogerio},
	urldate = {2019-09-25},
	date = {2017-11-22},
	eprinttype = {arxiv},
	eprint = {1711.08393},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1711.08393 PDF:C\:\\Users\\ajk\\Zotero\\storage\\CWAR22UK\\Wu et al. - 2017 - BlockDrop Dynamic Inference Paths in Residual Net.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\QHQJIMGR\\1711.html:text/html}
}

@article{tann_flexible_2018,
	title = {Flexible Deep Neural Network Processing},
	url = {http://arxiv.org/abs/1801.07353},
	abstract = {The recent success of Deep Neural Networks ({DNNs}) has drastically improved the state of the art for many application domains. While achieving high accuracy performance, deploying state-of-the-art {DNNs} is a challenge since they typically require billions of expensive arithmetic computations. In addition, {DNNs} are typically deployed in ensemble to boost accuracy performance, which further exacerbates the system requirements. This computational overhead is an issue for many platforms, e.g. data centers and embedded systems, with tight latency and energy budgets. In this article, we introduce flexible {DNNs} ensemble processing technique, which achieves large reduction in average inference latency while incurring small to negligible accuracy drop. Our technique is flexible in that it allows for dynamic adaptation between quality of results ({QoR}) and execution runtime. We demonstrate the effectiveness of the technique on {AlexNet} and {ResNet}-50 using the {ImageNet} dataset. This technique can also easily handle other types of networks.},
	journaltitle = {{arXiv}:1801.07353 [cs, stat]},
	author = {Tann, Hokchhay and Hashemi, Soheil and Reda, Sherief},
	urldate = {2019-09-25},
	date = {2018-01-22},
	eprinttype = {arxiv},
	eprint = {1801.07353},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1801.07353 PDF:C\:\\Users\\ajk\\Zotero\\storage\\WFCZDTM7\\Tann et al. - 2018 - Flexible Deep Neural Network Processing.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\GM4MT6KL\\1801.html:text/html}
}

@inproceedings{park_big/little_2015,
	title = {Big/little deep neural network for ultra low power inference},
	doi = {10.1109/CODESISSS.2015.7331375},
	abstract = {Deep neural networks ({DNNs}) have recently proved their effectiveness in complex data analyses such as object/speech recognition. As their applications are being expanded to mobile devices, their energy efficiencies are becoming critical. In this paper, we propose a novel concept called big/{LITTLE} {DNN} ({BL}-{DNN}) which significantly reduces energy consumption required for {DNN} execution at a negligible loss of inference accuracy. The {BL}-{DNN} consists of a little {DNN} (consuming low energy) and a full-fledged big {DNN}. In order to reduce energy consumption, the {BL}-{DNN} aims at avoiding the execution of the big {DNN} whenever possible. The key idea for this goal is to execute the little {DNN} first for inference (without big {DNN} execution) and simply use its result as the final inference result as long as the result is estimated to be accurate. On the other hand, if the result from the little {DNN} is not considered to be accurate, the big {DNN} is executed to give the final inference result. This approach reduces the total energy consumption by obtaining the inference result only with the little, energy-efficient {DNN} in most cases, while maintaining the similar level of inference accuracy through selectively utilizing the big {DNN} execution. We present design-time and runtime methods to control the execution of big {DNN} under a trade-off between energy consumption and inference accuracy. Experiments with state-of-the-art {DNNs} for {ImageNet} and {MNIST} show that our proposed {BL}-{DNN} can offer up to 53.7\% ({ImageNet}) and 94.1\% ({MNIST}) reductions in energy consumption at a loss of 0.90\% ({ImageNet}) and 0.12\% ({MNIST}) in inference accuracy, respectively.},
	eventtitle = {2015 International Conference on Hardware/Software Codesign and System Synthesis ({CODES}+{ISSS})},
	pages = {124--132},
	booktitle = {2015 International Conference on Hardware/Software Codesign and System Synthesis ({CODES}+{ISSS})},
	author = {Park, E. and Kim, D. and Kim, S. and Kim, Y. and Kim, G. and Yoon, S. and Yoo, S.},
	date = {2015-10},
	keywords = {Accuracy, big/little deep neural network, Biological neural networks, {BL}-{DNN}, complex data analyses, data analysis, Deep neural network, Energy consumption, energy consumption, Energy efficiency, Hardware, {ImageNet}, inference mechanisms, low power, Memory management, {MNIST}, neural nets, Neurons, ultra low power inference},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ajk\\Zotero\\storage\\YFDRYI6K\\7331375.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\U24CJXJC\\Park et al. - 2015 - Biglittle deep neural network for ultra low power.pdf:application/pdf}
}

@article{ko_edge-host_2018,
	title = {Edge-Host Partitioning of Deep Neural Networks with Feature Space Encoding for Resource-Constrained Internet-of-Things Platforms},
	url = {http://arxiv.org/abs/1802.03835},
	abstract = {This paper introduces partitioning an inference task of a deep neural network between an edge and a host platform in the {IoT} environment. We present a {DNN} as an encoding pipeline, and propose to transmit the output feature space of an intermediate layer to the host. The lossless or lossy encoding of the feature space is proposed to enhance the maximum input rate supported by the edge platform and/or reduce the energy of the edge platform. Simulation results show that partitioning a {DNN} at the end of convolutional (feature extraction) layers coupled with feature space encoding enables significant improvement in the energy-efficiency and throughput over the baseline configurations that perform the entire inference at the edge or at the host.},
	journaltitle = {{arXiv}:1802.03835 [cs]},
	author = {Ko, Jong Hwan and Na, Taesik and Amir, Mohammad Faisal and Mukhopadhyay, Saibal},
	urldate = {2019-09-25},
	date = {2018-02-11},
	eprinttype = {arxiv},
	eprint = {1802.03835},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1802.03835 PDF:C\:\\Users\\ajk\\Zotero\\storage\\UIDPVEST\\Ko et al. - 2018 - Edge-Host Partitioning of Deep Neural Networks wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\KXCFSFWL\\1802.html:text/html}
}

@article{choi_deep_2018,
	title = {Deep feature compression for collaborative object detection},
	url = {http://arxiv.org/abs/1802.03931},
	abstract = {Recent studies have shown that the efficiency of deep neural networks in mobile applications can be significantly improved by distributing the computational workload between the mobile device and the cloud. This paradigm, termed collaborative intelligence, involves communicating feature data between the mobile and the cloud. The efficiency of such approach can be further improved by lossy compression of feature data, which has not been examined to date. In this work we focus on collaborative object detection and study the impact of both near-lossless and lossy compression of feature data on its accuracy. We also propose a strategy for improving the accuracy under lossy feature compression. Experiments indicate that using this strategy, the communication overhead can be reduced by up to 70\% without sacrificing accuracy.},
	journaltitle = {{arXiv}:1802.03931 [cs]},
	author = {Choi, Hyomin and Bajic, Ivan V.},
	urldate = {2019-09-25},
	date = {2018-02-12},
	eprinttype = {arxiv},
	eprint = {1802.03931},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1802.03931 PDF:C\:\\Users\\ajk\\Zotero\\storage\\JYLYTVEE\\Choi and Bajic - 2018 - Deep feature compression for collaborative object .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\NC2AK7KM\\1802.html:text/html}
}

@article{choi_near-lossless_2018,
	title = {Near-Lossless Deep Feature Compression for Collaborative Intelligence},
	url = {http://arxiv.org/abs/1804.09963},
	abstract = {Collaborative intelligence is a new paradigm for efficient deployment of deep neural networks across the mobile-cloud infrastructure. By dividing the network between the mobile and the cloud, it is possible to distribute the computational workload such that the overall energy and/or latency of the system is minimized. However, this necessitates sending deep feature data from the mobile to the cloud in order to perform inference. In this work, we examine the differences between the deep feature data and natural image data, and propose a simple and effective near-lossless deep feature compressor. The proposed method achieves up to 5\% bit rate reduction compared to {HEVC}-Intra and even more against other popular image codecs. Finally, we suggest an approach for reconstructing the input image from compressed deep features in the cloud, that could serve to supplement the inference performed by the deep model.},
	journaltitle = {{arXiv}:1804.09963 [cs, eess]},
	author = {Choi, Hyomin and Bajic, Ivan V.},
	urldate = {2019-09-25},
	date = {2018-04-26},
	eprinttype = {arxiv},
	eprint = {1804.09963},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv\:1804.09963 PDF:C\:\\Users\\ajk\\Zotero\\storage\\AW9VZH5X\\Choi and Bajic - 2018 - Near-Lossless Deep Feature Compression for Collabo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\E3VXC9TP\\1804.html:text/html}
}

@article{vinyals_matching_2016,
	title = {Matching Networks for One Shot Learning},
	url = {http://arxiv.org/abs/1606.04080},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, {ImageNet}) and language tasks. Our algorithm improves one-shot accuracy on {ImageNet} from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	journaltitle = {{arXiv}:1606.04080 [cs, stat]},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	urldate = {2019-09-25},
	date = {2016-06-13},
	eprinttype = {arxiv},
	eprint = {1606.04080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.04080 PDF:C\:\\Users\\ajk\\Zotero\\storage\\W2BTNZ3Z\\Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\AT467L6J\\1606.html:text/html}
}

@article{huang_multi-scale_2017,
	title = {Multi-Scale Dense Networks for Resource Efficient Image Classification},
	url = {http://arxiv.org/abs/1703.09844},
	abstract = {In this paper we investigate image classiﬁcation with computational resource limits at test time. Two such settings are: 1. anytime classiﬁcation, where the network’s prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classiﬁcation, where a ﬁxed amount of computation is available to classify a set of examples that can be spent unevenly across “easier” and “harder” inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classiﬁers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classiﬁers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classiﬁcation early on, we use a two-dimensional multi-scale network architecture that maintains coarse and ﬁne level features all-throughout the network. Experiments on three image-classiﬁcation tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings.},
	journaltitle = {{arXiv}:1703.09844 [cs]},
	author = {Huang, Gao and Chen, Danlu and Li, Tianhong and Wu, Felix and van der Maaten, Laurens and Weinberger, Kilian Q.},
	urldate = {2019-09-27},
	date = {2017-03-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.09844},
	keywords = {Computer Science - Machine Learning},
	file = {Huang m.fl. - 2017 - Multi-Scale Dense Networks for Resource Efficient .pdf:C\:\\Users\\ajk\\Zotero\\storage\\ZJGTCEQ3\\Huang m.fl. - 2017 - Multi-Scale Dense Networks for Resource Efficient .pdf:application/pdf}
}

@article{wang_skipnet:_2017,
	title = {{SkipNet}: Learning Dynamic Routing in Convolutional Networks},
	url = {http://arxiv.org/abs/1711.09485},
	shorttitle = {{SkipNet}},
	abstract = {While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufﬁcient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce {SkipNet}, a modiﬁed residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show {SkipNet} reduces computation by 30 90\% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.},
	journaltitle = {{arXiv}:1711.09485 [cs]},
	author = {Wang, Xin and Yu, Fisher and Dou, Zi-Yi and Darrell, Trevor and Gonzalez, Joseph E.},
	urldate = {2019-09-27},
	date = {2017-11-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.09485},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang m.fl. - 2017 - SkipNet Learning Dynamic Routing in Convolutional.pdf:C\:\\Users\\ajk\\Zotero\\storage\\A5JHVZT9\\Wang m.fl. - 2017 - SkipNet Learning Dynamic Routing in Convolutional.pdf:application/pdf}
}

@article{xu_hybrid_2018,
	title = {Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge Devices},
	url = {http://arxiv.org/abs/1811.00482},
	shorttitle = {Hybrid Pruning},
	abstract = {We introduce hybrid pruning which combines both coarse-grained channel and fine-grained weight pruning to reduce model size, computation and power demands with no to little loss in accuracy for enabling modern networks deployment on resource-constrained devices, such as always-on security cameras and drones. Additionally, to effectively perform channel pruning, we propose a fast sensitivity test that helps us quickly identify the sensitivity of within and across layers of a network to the output accuracy for target multiplier accumulators ({MACs}) or accuracy tolerance. Our experiment shows significantly better results on {ResNet}50 on {ImageNet} compared to existing work, even with an additional constraint of channels be hardware-friendly number.},
	journaltitle = {{arXiv}:1811.00482 [cs]},
	author = {Xu, Xiaofan and Park, Mi Sun and Brick, Cormac},
	urldate = {2019-09-27},
	date = {2018-11-01},
	eprinttype = {arxiv},
	eprint = {1811.00482},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv\:1811.00482 PDF:C\:\\Users\\ajk\\Zotero\\storage\\QWMP5L9E\\Xu m.fl. - 2018 - Hybrid Pruning Thinner Sparse Networks for Fast I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\T8M3R955\\1811.html:text/html}
}

@article{huang_densely_2016,
	title = {Densely Connected Convolutional Networks},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network ({DenseNet}), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. {DenseNets} have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks ({CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet}). {DenseNets} obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/{DenseNet} .},
	journaltitle = {{arXiv}:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	urldate = {2019-09-28},
	date = {2016-08-24},
	eprinttype = {arxiv},
	eprint = {1608.06993},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1608.06993 PDF:C\:\\Users\\ajk\\Zotero\\storage\\V6HFAUQH\\Huang m.fl. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\EVWAX6CM\\1608.html:text/html}
}

@article{larsson_fractalnet:_2016,
	title = {{FractalNet}: Ultra-Deep Neural Networks without Residuals},
	url = {http://arxiv.org/abs/1605.07648},
	shorttitle = {{FractalNet}},
	abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both {CIFAR} and {ImageNet} classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
	journaltitle = {{arXiv}:1605.07648 [cs]},
	author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
	urldate = {2019-09-28},
	date = {2016-05-24},
	eprinttype = {arxiv},
	eprint = {1605.07648},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1605.07648 PDF:C\:\\Users\\ajk\\Zotero\\storage\\BL2736BE\\Larsson m.fl. - 2016 - FractalNet Ultra-Deep Neural Networks without Res.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\2LDGXD5L\\1605.html:text/html}
}

@report{karlsen_prototyping_nodate,
	title = {Prototyping of Image Classification Offloading in Mobile Edge Computing},
	url = {https://drive.google.com/drive/u/0/folders/1TJt1Q1XlgdwXcSthaLerykOFuDceVOBp},
	institution = {Aarhus University},
	author = {Karlsen, Alex}
}

@article{zhang_shufflenet:_2017,
	title = {{ShuffleNet}: An Extremely Efficient Convolutional Neural Network for Mobile Devices},
	url = {http://arxiv.org/abs/1707.01083},
	shorttitle = {{ShuffleNet}},
	abstract = {We introduce an extremely computation-efficient {CNN} architecture named {ShuffleNet}, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 {MFLOPs}). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on {ImageNet} classification and {MS} {COCO} object detection demonstrate the superior performance of {ShuffleNet} over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent {MobileNet} on {ImageNet} classification task, under the computation budget of 40 {MFLOPs}. On an {ARM}-based mobile device, {ShuffleNet} achieves {\textasciitilde}13x actual speedup over {AlexNet} while maintaining comparable accuracy.},
	journaltitle = {{arXiv}:1707.01083 [cs]},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	urldate = {2019-10-02},
	date = {2017-07-04},
	eprinttype = {arxiv},
	eprint = {1707.01083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1707.01083 PDF:C\:\\Users\\ajk\\Zotero\\storage\\4SFD7BUX\\Zhang m.fl. - 2017 - ShuffleNet An Extremely Efficient Convolutional N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\E48AXYCV\\1707.html:text/html}
}

@article{ma_shufflenet_2018,
	title = {{ShuffleNet} V2: Practical Guidelines for Efficient {CNN} Architecture Design},
	url = {http://arxiv.org/abs/1807.11164},
	shorttitle = {{ShuffleNet} V2},
	abstract = {Currently, the neural network architecture design is mostly guided by the {\textbackslash}emph\{indirect\} metric of computation complexity, i.e., {FLOPs}. However, the {\textbackslash}emph\{direct\} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering {FLOPs}. Based on a series of controlled experiments, this work derives several practical {\textbackslash}emph\{guidelines\} for efficient network design. Accordingly, a new architecture is presented, called {\textbackslash}emph\{{ShuffleNet} V2\}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.},
	journaltitle = {{arXiv}:1807.11164 [cs]},
	author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
	urldate = {2019-10-02},
	date = {2018-07-30},
	eprinttype = {arxiv},
	eprint = {1807.11164},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1807.11164 PDF:C\:\\Users\\ajk\\Zotero\\storage\\DGM3ZYKN\\Ma m.fl. - 2018 - ShuffleNet V2 Practical Guidelines for Efficient .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\LRCASAT8\\1807.html:text/html}
}

@article{howard_mobilenets:_2017,
	title = {{MobileNets}: Efficient Convolutional Neural Networks for Mobile Vision Applications},
	url = {http://arxiv.org/abs/1704.04861},
	shorttitle = {{MobileNets}},
	abstract = {We present a class of efficient models called {MobileNets} for mobile and embedded vision applications. {MobileNets} are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on {ImageNet} classification. We then demonstrate the effectiveness of {MobileNets} across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	journaltitle = {{arXiv}:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	urldate = {2019-10-02},
	date = {2017-04-16},
	eprinttype = {arxiv},
	eprint = {1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1704.04861 PDF:C\:\\Users\\ajk\\Zotero\\storage\\WZGA9LTG\\Howard m.fl. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\BHYQMBM7\\1704.html:text/html}
}

@article{courbariaux_binarized_2016,
	title = {Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
	url = {http://arxiv.org/abs/1602.02830},
	shorttitle = {Binarized Neural Networks},
	abstract = {We introduce a method to train Binarized Neural Networks ({BNNs}) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, {BNNs} drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of {BNNs} we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, {BNNs} achieved nearly state-of-the-art results over the {MNIST}, {CIFAR}-10 and {SVHN} datasets. Last but not least, we wrote a binary matrix multiplication {GPU} kernel with which it is possible to run our {MNIST} {BNN} 7 times faster than with an unoptimized {GPU} kernel, without suffering any loss in classification accuracy. The code for training and running our {BNNs} is available on-line.},
	journaltitle = {{arXiv}:1602.02830 [cs]},
	author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	urldate = {2019-10-02},
	date = {2016-02-08},
	eprinttype = {arxiv},
	eprint = {1602.02830},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1602.02830 PDF:C\:\\Users\\ajk\\Zotero\\storage\\LZ2NKUKY\\Courbariaux m.fl. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\42X7E9CH\\1602.html:text/html}
}

@article{li_edge_2018,
	title = {Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy},
	url = {http://arxiv.org/abs/1806.07840},
	shorttitle = {Edge Intelligence},
	abstract = {As the backbone technology of machine learning, deep neural networks ({DNNs}) have have quickly ascended to the spotlight. Running {DNNs} on resource-constrained mobile devices is, however, by no means trivial, since it incurs high performance and energy overhead. While offloading {DNNs} to the cloud for execution suffers unpredictable performance, due to the uncontrolled long wide-area network latency. To address these challenges, in this paper, we propose Edgent, a collaborative and on-demand {DNN} co-inference framework with device-edge synergy. Edgent pursues two design knobs: (1) {DNN} partitioning that adaptively partitions {DNN} computation between device and edge, in order to leverage hybrid computation resources in proximity for real-time {DNN} inference. (2) {DNN} right-sizing that accelerates {DNN} inference through early-exit at a proper intermediate {DNN} layer to further reduce the computation latency. The prototype implementation and extensive evaluations based on Raspberry Pi demonstrate Edgent's effectiveness in enabling on-demand low-latency edge intelligence.},
	journaltitle = {{arXiv}:1806.07840 [cs]},
	author = {Li, En and Zhou, Zhi and Chen, Xu},
	urldate = {2019-10-02},
	date = {2018-06-20},
	eprinttype = {arxiv},
	eprint = {1806.07840},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Multimedia, Computer Science - Networking and Internet Architecture},
	file = {arXiv\:1806.07840 PDF:C\:\\Users\\ajk\\Zotero\\storage\\6CVAXNZU\\Li m.fl. - 2018 - Edge Intelligence On-Demand Deep Learning Model C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\TW7J8W35\\1806.html:text/html}
}

@article{hadidi_collaborative_2019,
	title = {Collaborative Execution of Deep Neural Networks on Internet of Things Devices},
	url = {http://arxiv.org/abs/1901.02537},
	abstract = {With recent advancements in deep neural networks ({DNNs}), we are able to solve traditionally challenging problems. Since {DNNs} are compute intensive, consumers, to deploy a service, need to rely on expensive and scarce compute resources in the cloud. This approach, in addition to its dependability on high-quality network infrastructure and data centers, raises new privacy concerns. These challenges may limit {DNN}-based applications, so many researchers have tried optimize {DNNs} for local and in-edge execution. However, inadequate power and computing resources of edge devices along with small number of requests limits current optimizations applicability, such as batch processing. In this paper, we propose an approach that utilizes aggregated existing computing power of Internet of Things ({IoT}) devices surrounding an environment by creating a collaborative network. In this approach, {IoT} devices cooperate to conduct single-batch inferencing in real time. While exploiting several new model-parallelism methods and their distribution characteristics, our approach enhances the collaborative network by creating a balanced and distributed processing pipeline. We have illustrated our work using many Raspberry Pis with studying {DNN} models such as {AlexNet}, {VGG}16, Xception, and C3D.},
	journaltitle = {{arXiv}:1901.02537 [cs]},
	author = {Hadidi, Ramyad and Cao, Jiashen and Ryoo, Micheal S. and Kim, Hyesoon},
	urldate = {2019-10-02},
	date = {2019-01-08},
	eprinttype = {arxiv},
	eprint = {1901.02537},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1901.02537 PDF:C\:\\Users\\ajk\\Zotero\\storage\\7NQNKAJC\\Hadidi m.fl. - 2019 - Collaborative Execution of Deep Neural Networks on.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\YXVPCDCR\\1901.html:text/html}
}

@article{bolukbasi_adaptive_2017,
	title = {Adaptive Neural Networks for Efficient Inference},
	url = {http://arxiv.org/abs/1702.07811},
	abstract = {We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the {ImageNet} image recognition challenge with minimal ({\textless}1\%) loss of top5 accuracy.},
	journaltitle = {{arXiv}:1702.07811 [cs, stat]},
	author = {Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
	urldate = {2019-10-02},
	date = {2017-02-24},
	eprinttype = {arxiv},
	eprint = {1702.07811},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv\:1702.07811 PDF:C\:\\Users\\ajk\\Zotero\\storage\\CCCGIC8U\\Bolukbasi m.fl. - 2017 - Adaptive Neural Networks for Efficient Inference.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\PHIG44ZC\\1702.html:text/html}
}

@article{leroux_cascading_2017,
	title = {The cascading neural network: building the Internet of Smart Things},
	volume = {52},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-017-1029-1},
	doi = {10.1007/s10115-017-1029-1},
	shorttitle = {The cascading neural network},
	abstract = {Most of the research on deep neural networks so far has been focused on obtaining higher accuracy levels by building increasingly large and deep architectures. Training and evaluating these models is only feasible when large amounts of resources such as processing power and memory are available. Typical applications that could beneﬁt from these models are, however, executed on resource-constrained devices. Mobile devices such as smartphones already use deep learning techniques, but they often have to perform all processing on a remote cloud. We propose a new architecture called a cascading network that is capable of distributing a deep neural network between a local device and the cloud while keeping the required communication network trafﬁc to a minimum. The network begins processing on the constrained device, and only relies on the remote part when the local part does not provide an accurate enough result. The cascading network allows for an early-stopping mechanism during the recall phase of the network. We evaluated our approach in an Internet of Things context where a deep neural network adds intelligence to a large amount of heterogeneous connected devices. This technique enables a whole variety of autonomous systems where sensors, actuators and computing nodes can work together. We show that the cascading architecture allows for a substantial improvement in evaluation speed on constrained devices while the loss in accuracy is kept to a minimum.},
	pages = {791--814},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Leroux, Sam and Bohez, Steven and De Coninck, Elias and Verbelen, Tim and Vankeirsbilck, Bert and Simoens, Pieter and Dhoedt, Bart},
	urldate = {2019-10-02},
	date = {2017-09},
	langid = {english},
	file = {Leroux m.fl. - 2017 - The cascading neural network building the Interne.pdf:C\:\\Users\\ajk\\Zotero\\storage\\I42WQH7U\\Leroux m.fl. - 2017 - The cascading neural network building the Interne.pdf:application/pdf}
}

@inproceedings{leroux_resource-constrained_2015,
	location = {Killarney, Ireland},
	title = {Resource-constrained classification using a cascade of neural network layers},
	isbn = {978-1-4799-1960-4},
	url = {http://ieeexplore.ieee.org/document/7280601/},
	doi = {10.1109/IJCNN.2015.7280601},
	abstract = {Deep neural networks are the state of the art technique for a wide variety of classiﬁcation problems. Although deeper networks are able to make more accurate classiﬁcations, the value brought by an additional hidden layer diminishes rapidly. Even shallow networks are able to achieve relatively good results on various classiﬁcation problems. Only for a small subset of the samples do the deeper layers make a signiﬁcant difference. We describe an architecture in which only the samples that can not be classiﬁed with a sufﬁcient conﬁdence by a shallow network have to be processed by the deeper layers. Instead of training a network with one output layer at the end of the network, we train several output layers, one for each hidden layer. When an output layer is sufﬁciently conﬁdent in this result, we stop propagating at this layer and the deeper layers need not be evaluated. The choice of a threshold conﬁdence value allows us to trade-off accuracy and speed.},
	eventtitle = {2015 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--7},
	booktitle = {2015 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Leroux, Sam and Bohez, Steven and Verbelen, Tim and Vankeirsbilck, Bert and Simoens, Pieter and Dhoedt, Bart},
	urldate = {2019-10-13},
	date = {2015-07},
	langid = {english},
	file = {Leroux m.fl. - 2015 - Resource-constrained classification using a cascad.pdf:C\:\\Users\\ajk\\Zotero\\storage\\H8YTMEU3\\Leroux m.fl. - 2015 - Resource-constrained classification using a cascad.pdf:application/pdf}
}

@article{hinton_distilling_2015,
	title = {Distilling the Knowledge in a Neural Network},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	journaltitle = {{arXiv}:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	urldate = {2019-10-13},
	date = {2015-03-09},
	eprinttype = {arxiv},
	eprint = {1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1503.02531 PDF:C\:\\Users\\ajk\\Zotero\\storage\\XG9FSUTL\\Hinton m.fl. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\KB2Y76IR\\1503.html:text/html}
}

@incollection{ba_deep_2014,
	title = {Do Deep Nets Really Need to be Deep?},
	url = {http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf},
	pages = {2654--2662},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Ba, Jimmy and Caruana, Rich},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2019-10-13},
	date = {2014},
	file = {NIPS Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\JQXFBFRU\\Ba og Caruana - 2014 - Do Deep Nets Really Need to be Deep.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\E54E639K\\5484-do-deep-nets-really-need-to-be-deep.html:text/html}
}

@article{cheng_survey_2017,
	title = {A Survey of Model Compression and Acceleration for Deep Neural Networks},
	url = {http://arxiv.org/abs/1710.09282},
	abstract = {Deep convolutional neural networks ({CNNs}) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past few years, tremendous progress has been made in this area. In this paper, we survey the recent advanced techniques for compacting and accelerating {CNNs} model developed. These techniques are roughly categorized into four schemes: parameter pruning and sharing, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and sharing will be described at the beginning, after that the other techniques will be introduced. For each scheme, we provide insightful analysis regarding the performance, related applications, advantages, and drawbacks etc. Then we will go through a few very recent additional successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrix, the main datasets used for evaluating the model performance and recent benchmarking efforts. Finally, we conclude this paper, discuss remaining challenges and possible directions on this topic.},
	journaltitle = {{arXiv}:1710.09282 [cs]},
	author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
	urldate = {2019-10-13},
	date = {2017-10-23},
	eprinttype = {arxiv},
	eprint = {1710.09282},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1710.09282 PDF:C\:\\Users\\ajk\\Zotero\\storage\\5LDSLLJA\\Cheng m.fl. - 2017 - A Survey of Model Compression and Acceleration for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\WT4TKGF5\\1710.html:text/html}
}

@article{romero_fitnets:_2014,
	title = {{FitNets}: Hints for Thin Deep Nets},
	url = {http://arxiv.org/abs/1412.6550},
	shorttitle = {{FitNets}},
	abstract = {While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on {CIFAR}-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.},
	journaltitle = {{arXiv}:1412.6550 [cs]},
	author = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
	urldate = {2019-10-18},
	date = {2014-12-19},
	eprinttype = {arxiv},
	eprint = {1412.6550},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1412.6550 PDF:C\:\\Users\\ajk\\Zotero\\storage\\X6KVQLQ6\\Romero et al. - 2014 - FitNets Hints for Thin Deep Nets.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\CT6VDCAR\\1412.html:text/html}
}

@article{romero_fitnets:_2014-1,
	title = {{FitNets}: Hints for Thin Deep Nets},
	url = {http://arxiv.org/abs/1412.6550},
	shorttitle = {{FitNets}},
	abstract = {While depth tends to improve network performances, it also makes gradient-based training more difﬁcult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and ﬁnal performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher’s intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on {CIFAR}-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.},
	journaltitle = {{arXiv}:1412.6550 [cs]},
	author = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
	urldate = {2019-10-18},
	date = {2014-12-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6550},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Romero et al. - 2014 - FitNets Hints for Thin Deep Nets.pdf:C\:\\Users\\ajk\\Zotero\\storage\\HRIPY8SJ\\Romero et al. - 2014 - FitNets Hints for Thin Deep Nets.pdf:application/pdf}
}

@article{hinton_distilling_2015-1,
	title = {Distilling the Knowledge in a Neural Network},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	journaltitle = {{arXiv}:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	urldate = {2019-10-18},
	date = {2015-03-09},
	eprinttype = {arxiv},
	eprint = {1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1503.02531 PDF:C\:\\Users\\ajk\\Zotero\\storage\\FQ9PPGW3\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\FFGJR2QF\\1503.html:text/html}
}

@article{zensius_jetson_nodate,
	title = {Jetson {TX}1 and {TX}2 Developer Kits},
	pages = {31},
	author = {Zensius, Michael},
	langid = {english},
	file = {Zensius - Jetson TX1 and TX2 Developer Kits.pdf:C\:\\Users\\ajk\\Zotero\\storage\\N9HR84MQ\\Zensius - Jetson TX1 and TX2 Developer Kits.pdf:application/pdf}
}

@inproceedings{zhang_communication_2017,
	location = {Shanghai},
	title = {The communication system between web application host computers and embedded systems based on Node.{JS}},
	isbn = {978-1-5386-1937-7},
	url = {http://ieeexplore.ieee.org/document/8302325/},
	doi = {10.1109/CISP-BMEI.2017.8302325},
	abstract = {In order to realize the communication between the Web applications and the embedded systems, and to improve the efficiency of {PC} program and embedded system communication, this paper puts forward a solution of building the hyper-text transfer protocol ({HTTP}) webserver combined with transmission control protocol or user datagram protocol ({TCP}/{UDP}) socket program using Node.{JS} platform. This paper also introduces the method of building the webserver on the Express frame work cooperated with the {MongoDB} database. The communication progress and protocol between host computers and the embedded system are illustrated as well.},
	eventtitle = {2017 10th International Congress on Image and Signal Processing, {BioMedical} Engineering and Informatics ({CISP}-{BMEI})},
	pages = {1--5},
	booktitle = {2017 10th International Congress on Image and Signal Processing, {BioMedical} Engineering and Informatics ({CISP}-{BMEI})},
	publisher = {{IEEE}},
	author = {Zhang, Ding and Lin, Shunhao and Fu, Yuqing and Huang, Shimei},
	urldate = {2019-10-21},
	date = {2017-10},
	langid = {english},
	file = {Zhang m.fl. - 2017 - The communication system between web application h.pdf:C\:\\Users\\ajk\\Zotero\\storage\\46BIK6CD\\Zhang m.fl. - 2017 - The communication system between web application h.pdf:application/pdf}
}

@inproceedings{sharma_are_2018,
	location = {San Francisco, {CA}},
	title = {Are Existing Knowledge Transfer Techniques Effective for Deep Learning with Edge Devices?},
	isbn = {978-1-5386-7238-9},
	url = {https://ieeexplore.ieee.org/document/8473375/},
	doi = {10.1109/EDGE.2018.00013},
	abstract = {With the emergence of edge computing paradigm, many applications such as image recognition and augmented reality require to perform machine learning ({ML}) and artiﬁcial intelligence ({AI}) tasks on edge devices. Most {AI} and {ML} models are large and computational-heavy, whereas edge devices are usually equipped with limited computational and storage resources. Such models can be compressed and reduced for deployment on edge devices, but they may lose their capability and not perform well. Recent works used knowledge transfer techniques to transfer information from a large network (termed teacher) to a small one (termed student) in order to improve the performance of the latter. This approach seems to be promising for learning on edge devices, but a thorough investigation on its effectiveness is lacking. This paper provides an extensive study on the performance (in both accuracy and convergence speed) of knowledge transfer, considering different student architectures and different techniques for transferring knowledge from teacher to student. The results show that the performance of {KT} does vary by architectures and transfer techniques. A good performance improvement is obtained by transferring knowledge from both the intermediate layers and last layer of the teacher to a shallower student. But other architectures and transfer techniques do not fare so well and some of them even lead to negative performance impact.},
	eventtitle = {2018 {IEEE} International Conference on Edge Computing ({EDGE})},
	pages = {42--49},
	booktitle = {2018 {IEEE} International Conference on Edge Computing ({EDGE})},
	publisher = {{IEEE}},
	author = {Sharma, Ragini and Biookaghazadeh, Saman and Li, Baoxin and Zhao, Ming},
	urldate = {2019-10-21},
	date = {2018-07},
	langid = {english},
	file = {Sharma m.fl. - 2018 - Are Existing Knowledge Transfer Techniques Effecti.pdf:C\:\\Users\\ajk\\Zotero\\storage\\WS92VXKM\\Sharma m.fl. - 2018 - Are Existing Knowledge Transfer Techniques Effecti.pdf:application/pdf}
}

@article{huang_distributed_2018-1,
	title = {Distributed Deep Learning-based Offloading for Mobile Edge Computing Networks},
	issn = {1383-469X, 1572-8153},
	url = {http://link.springer.com/10.1007/s11036-018-1177-x},
	doi = {10.1007/s11036-018-1177-x},
	abstract = {This paper studies mobile edge computing ({MEC}) networks where multiple wireless devices ({WDs}) choose to offload their computation tasks to an edge server. To conserve energy and maintain quality of service for {WDs}, the optimization of joint offloading decision and bandwidth allocation is formulated as a mixed integer programming problem. However, the problem is computationally limited by the curse of dimensionality, which cannot be solved by general optimization tools in an effective and efficient way, especially for large-scale {WDs}. In this paper, we propose a distributed deep learning-based offloading ({DDLO}) algorithm for {MEC} networks, where multiple parallel {DNNs} are used to generate offloading decisions. We adopt a shared replay memory to store newly generated offloading decisions which are further to train and improve all {DNNs}. Extensive numerical results show that the proposed {DDLO} algorithm can generate near-optimal offloading decisions in less than one second.},
	journaltitle = {Mobile Networks and Applications},
	shortjournal = {Mobile Netw Appl},
	author = {Huang, Liang and Feng, Xu and Feng, Anqi and Huang, Yupin and Qian, Li Ping},
	urldate = {2019-10-21},
	date = {2018-11-29},
	langid = {english},
	file = {Huang m.fl. - 2018 - Distributed Deep Learning-based Offloading for Mob.pdf:C\:\\Users\\ajk\\Zotero\\storage\\EJ8IMRX4\\Huang m.fl. - 2018 - Distributed Deep Learning-based Offloading for Mob.pdf:application/pdf}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 Tutorial: Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1701.00160},
	shorttitle = {{NIPS} 2016 Tutorial},
	abstract = {This report summarizes the tutorial presented by the author at {NIPS} 2016 on generative adversarial networks ({GANs}). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how {GANs} compare to other generative models, (3) the details of how {GANs} work, (4) research frontiers in {GANs}, and (5) state-of-the-art image models that combine {GANs} with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	journaltitle = {{arXiv}:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	urldate = {2019-10-21},
	date = {2016-12-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.00160},
	keywords = {Computer Science - Machine Learning},
	file = {Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:C\:\\Users\\ajk\\Zotero\\storage\\ZJNAJVML\\Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf}
}

@article{iandola_squeezenet:_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.5MB model size},
	url = {http://arxiv.org/abs/1602.07360},
	shorttitle = {{SqueezeNet}},
	abstract = {Recent research on deep convolutional neural networks ({CNNs}) has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple {CNN} architectures that achieve that accuracy level. With equivalent accuracy, smaller {CNN} architectures offer at least three advantages: (1) Smaller {CNNs} require less communication across servers during distributed training. (2) Smaller {CNNs} require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller {CNNs} are more feasible to deploy on {FPGAs} and other hardware with limited memory. To provide all of these advantages, we propose a small {CNN} architecture called {SqueezeNet}. {SqueezeNet} achieves {AlexNet}-level accuracy on {ImageNet} with 50x fewer parameters. Additionally, with model compression techniques, we are able to compress {SqueezeNet} to less than 0.5MB (510× smaller than {AlexNet}).},
	journaltitle = {{arXiv}:1602.07360 [cs]},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	urldate = {2019-10-23},
	date = {2016-11-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.07360},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:C\:\\Users\\ajk\\Zotero\\storage\\9YRQ8NVL\\Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf}
}

@article{perez_effectiveness_2017,
	title = {The Effectiveness of Data Augmentation in Image Classification using Deep Learning},
	url = {http://arxiv.org/abs/1712.04621},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classiﬁcation. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and ﬂipping input images. We artiﬁcially constrain our access to data to a small subset of the {ImageNet} dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with {GANs} to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classiﬁer, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	journaltitle = {{arXiv}:1712.04621 [cs]},
	author = {Perez, Luis and Wang, Jason},
	urldate = {2019-10-23},
	date = {2017-12-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.04621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Perez and Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:C\:\\Users\\ajk\\Zotero\\storage\\W2C5JZA6\\Perez and Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:application/pdf}
}

@online{li_cs231n:_2018,
	title = {{CS}231n: Convolutional Neural Networks Spring 2018},
	url = {http://cs231n.stanford.edu/},
	shorttitle = {{CS}231n},
	abstract = {Stanford course on Convolutional Neural Networks for Visual Recognition \# Course Description Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset ({ImageNet}). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the {ImageNet} Challenge. https://i.imgur.com/ps0x3Wo.png},
	titleaddon = {Convolutional Neural Networks for Visual Recognition},
	type = {University Course},
	author = {Li, Fei-Fei and Karpathy, Andrej and Johnson, Justin},
	urldate = {2019-10-23},
	date = {2018}
}

@article{li_learning_2018,
	title = {Learning {IoT} in Edge: Deep Learning for the Internet of Things with Edge Computing},
	volume = {32},
	issn = {0890-8044, 1558-156X},
	url = {https://ieeexplore.ieee.org/document/8270639/},
	doi = {10.1109/MNET.2018.1700202},
	shorttitle = {Learning {IoT} in Edge},
	abstract = {Deep learning is a promising approach for extracting accurate information from raw sensor data from {IoT} devices deployed in complex environments. Because of its multilayer structure, deep learning is also appropriate for the edge computing environment. Therefore, in this article, we first introduce deep learning for {IoTs} into the edge computing environment. Since existing edge nodes have limited processing capability, we also design a novel offloading strategy to optimize the performance of {IoT} deep learning applications with edge computing. In the performance evaluation, we test the performance of executing multiple deep learning tasks in an edge computing environment with our strategy. The evaluation results show that our method outperforms other optimization solutions on deep learning for {IoT}.},
	pages = {96--101},
	number = {1},
	journaltitle = {{IEEE} Network},
	shortjournal = {{IEEE} Network},
	author = {Li, He and Ota, Kaoru and Dong, Mianxiong},
	urldate = {2019-10-21},
	date = {2018-01},
	langid = {english},
	file = {Li et al. - 2018 - Learning IoT in Edge Deep Learning for the Intern.pdf:C\:\\Users\\ajk\\Zotero\\storage\\MU3DZAJI\\Li et al. - 2018 - Learning IoT in Edge Deep Learning for the Intern.pdf:application/pdf}
}

@article{kornblith_better_2019,
	title = {Do Better {ImageNet} Models Transfer Better?},
	url = {http://arxiv.org/abs/1805.08974},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on {ImageNet} necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between {ImageNet} accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on {ImageNet}; many common forms of regularization slightly improve {ImageNet} accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on {ImageNet} provides minimal beneﬁts, indicating the learned features from {ImageNet} do not transfer well to ﬁne-grained tasks. Together, our results show that {ImageNet} architectures generalize well across datasets, but {ImageNet} features are less general than previously suggested.},
	journaltitle = {{arXiv}:1805.08974 [cs, stat]},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	urldate = {2019-10-25},
	date = {2019-06-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.08974},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:C\:\\Users\\ajk\\Zotero\\storage\\68KJ7V5E\\Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf}
}

@incollection{dean_large_2012,
	title = {Large Scale Distributed Deep Networks},
	url = {http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf},
	pages = {1223--1231},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\{{\textbackslash}textbackslash\}textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V. and Ng, Andrew Y.},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	urldate = {2019-10-25},
	date = {2012},
	file = {NIPS Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\H7L2Y4ZD\\Dean et al. - 2012 - Large Scale Distributed Deep Networks.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\Q5DTE2UF\\4687-large-scale-distributed-deep-networks.html:text/html}
}

@article{bengio_practical_2012,
	title = {Practical recommendations for gradient-based training of deep architectures},
	url = {http://arxiv.org/abs/1206.5533},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	journaltitle = {{arXiv}:1206.5533 [cs]},
	author = {Bengio, Yoshua},
	urldate = {2019-10-25},
	date = {2012-09-16},
	eprinttype = {arxiv},
	eprint = {1206.5533},
	note = {version: 2},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\RZ8ZDN38\\Bengio - 2012 - Practical recommendations for gradient-based train.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\AQIWU38I\\1206.html:text/html}
}

@book{van_rossum_python_1995,
	title = {Python tutorial},
	publisher = {Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands},
	author = {Van Rossum, Guido and Drake Jr, Fred L},
	date = {1995}
}

@book{oliphant_guide_2006,
	title = {A guide to {NumPy}},
	volume = {1},
	publisher = {Trelgol Publishing {USA}},
	author = {Oliphant, Travis E},
	date = {2006}
}

@article{virtanen_scipy_2019,
	title = {{SciPy} 1.0–Fundamental Algorithms for Scientific Computing in Python},
	pages = {arXiv:1907.10121},
	journaltitle = {{arXiv} e-prints},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Jarrod Millman, K. and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, {CJ} and Polat, İlhan and Feng, Yu and Moore, Eric W. and Vand {erPlas}, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, {SciPy} 1. 0},
	date = {2019-07}
}

@inproceedings{marcel_torchvision_2010,
	location = {New York, {NY}, {USA}},
	title = {Torchvision the Machine-vision Package of Torch},
	isbn = {978-1-60558-933-6},
	url = {http://doi.acm.org/10.1145/1873951.1874254},
	doi = {10.1145/1873951.1874254},
	series = {{MM} '10},
	pages = {1485--1488},
	booktitle = {Proceedings of the 18th {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Marcel, Sébastien and Rodriguez, Yann},
	date = {2010},
	note = {event-place: Firenze, Italy},
	keywords = {face detection and recognition, machine learning, open source, pattern recognition, vision}
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic Differentiation in {PyTorch}},
	booktitle = {{NIPS} Autodiff Workshop},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and {DeVito}, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	date = {2017}
}

@software{jung_imgaug:_nodate,
	title = {imgaug: Image augmentation library for deep neural networks},
	rights = {{MIT}},
	url = {https://github.com/aleju/imgaug},
	shorttitle = {imgaug},
	version = {0.3.0},
	author = {Jung, Alexander},
	urldate = {2019-11-07},
	keywords = {augmentation,, {CNN},, computer vision,, deep learning,, image,, machine learning,, neural network,, overfitting},
	file = {Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\JY4FJBK4\\imgaug.html:text/html}
}

@software{zhu_thop_nodate,
	title = {thop 0.0.31-1910280903       : A tool to count the {FLOPs} of {PyTorch} model.},
	rights = {{MIT}},
	url = {https://github.com/Lyken17/pytorch-OpCounter/},
	shorttitle = {thop 0.0.31-1910280903},
	author = {Zhu, Ligeng},
	urldate = {2019-11-07},
	file = {Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\2V2QDARJ\\thop.html:text/html}
}

@inproceedings{cambazoglu_early_2010,
	location = {New York, New York, {USA}},
	title = {Early exit optimizations for additive machine learned ranking systems},
	isbn = {978-1-60558-889-6},
	url = {http://portal.acm.org/citation.cfm?doid=1718487.1718538},
	doi = {10.1145/1718487.1718538},
	abstract = {Some commercial web search engines rely on sophisticated machine learning systems for ranking web documents. Due to very large collection sizes and tight constraints on query response times, online eﬃciency of these learning systems forms a bottleneck. An important problem in such systems is to speedup the ranking process without sacriﬁcing much from the quality of results. In this paper, we propose optimization strategies that allow short-circuiting score computations in additive learning systems. The strategies are evaluated over a state-of-the-art machine learning system and a large, real-life query log, obtained from Yahoo!. By the proposed strategies, we are able to speedup the score computations by more than four times with almost no loss in result quality.},
	eventtitle = {the third {ACM} international conference},
	pages = {411},
	booktitle = {Proceedings of the third {ACM} international conference on Web search and data mining - {WSDM} '10},
	publisher = {{ACM} Press},
	author = {Cambazoglu, B. Barla and Zaragoza, Hugo and Chapelle, Olivier and Chen, Jiang and Liao, Ciya and Zheng, Zhaohui and Degenhardt, Jon},
	urldate = {2019-11-09},
	date = {2010},
	langid = {english},
	file = {Cambazoglu m.fl. - 2010 - Early exit optimizations for additive machine lear.pdf:C\:\\Users\\ajk\\Zotero\\storage\\LRC6VKLC\\Cambazoglu m.fl. - 2010 - Early exit optimizations for additive machine lear.pdf:application/pdf}
}

@inproceedings{viola_rapid_2001,
	location = {Kauai, {HI}, {USA}},
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	isbn = {978-0-7695-1272-3},
	url = {http://ieeexplore.ieee.org/document/990517/},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The ﬁrst is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on {AdaBoost}, which selects a small number of critical visual features from a larger set and yields extremely efﬁcient classiﬁers[6]. The third contribution is a method for combining increasingly more complex classiﬁers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object speciﬁc focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	eventtitle = {2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	pages = {I--511--I--518},
	booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	publisher = {{IEEE} Comput. Soc},
	author = {Viola, P. and Jones, M.},
	urldate = {2019-11-09},
	date = {2001},
	langid = {english},
	file = {Viola og Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:C\:\\Users\\ajk\\Zotero\\storage\\8WREN45A\\Viola og Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:application/pdf}
}

@inproceedings{szegedy_going_2015,
	location = {Boston, {MA}, {USA}},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC}14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC}14 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1--9},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2019-11-28},
	date = {2015-06},
	langid = {english},
	file = {Szegedy m.fl. - 2015 - Going deeper with convolutions.pdf:C\:\\Users\\ajk\\Zotero\\storage\\YFCNP4EN\\Szegedy m.fl. - 2015 - Going deeper with convolutions.pdf:application/pdf}
}

@inproceedings{lo_dynamic_2017,
	title = {A Dynamic Deep Neural Network Design for Efficient Workload Allocation in Edge Computing},
	doi = {10.1109/ICCD.2017.49},
	abstract = {Unreliable communication channels and limited computing resources at the edge end are two primary constraints of battery-powered movable devices, such as autonomous robots and unmanned aerial vehicles ({UAVs}). The impact is especially severe for those performing deep neural network ({DNN}) computations. With increasing demand for accuracy, the trend in modern {DNN} designs is the use of cascaded modularized layers. Implementing a deep network at the edge increases computational workloads and resource occupancy, leading to an increase in battery drain. Using a shallow network and offloading workloads to backbone servers, however, incur significant latency overheads caused by unstable communication channels. Hence, dynamic {DNN} design techniques for efficient workload allocation are urgently required to manage the amount of workload transmissions while achieving the required accuracy. In this paper, we explore the use of authentic operation ({AO}) unit and dynamic network structure to enhance {DNNs}. The {AO} unit defines a set of stochastic threshold values for different {DNN} output classes and determines at runtime if an input has to be transferred to backbone servers for further analysis. The dynamic network structure adjusts its depth according to channel availability. Experiments have been comprehensively performed on several well-known {DNN} models and datasets. Our results show that, on an average, the proposed techniques are able to reduce the amount of transmissions by up to 17\% compared to previous methods under the same accuracy requirement.},
	eventtitle = {2017 {IEEE} International Conference on Computer Design ({ICCD})},
	pages = {273--280},
	booktitle = {2017 {IEEE} International Conference on Computer Design ({ICCD})},
	author = {Lo, Chi and Su, Yu-Yi and Lee, Chun-Yi and Chang, Shih-Chieh},
	date = {2017-11},
	note = {{ISSN}: 1063-6404},
	keywords = {Deep neural network, neural nets, accuracy requirement, authentic operation, authentic operation unit, autonomous aerial vehicles, autonomous robots, backbone servers, battery drain, battery-powered movable devices, cascaded modularized layers, Communication channels, computational workloads, datasets, deep neural network computations, different {DNN} output classes, dynamic deep neural network design, dynamic {DNN} design techniques, dynamic network structure, edge computing, edge end, efficient workload allocation, Image edge detection, limited computing resources, modern {DNN} designs, multiprocessing systems, Neural networks, offloading workloads, Performance evaluation, primary constraints, remotely operated vehicles, required accuracy, Resource management, resource occupancy, Servers, shallow network, unmanned aerial vehicles, unreliable communication channels, unstable communication channels, Vehicle dynamics, workload allocation, workload transmissions},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ajk\\Zotero\\storage\\3FMI4NIY\\8119222.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\WUGZA9UP\\Lo et al. - 2017 - A Dynamic Deep Neural Network Design for Efficient.pdf:application/pdf}
}

@article{li_deep_2018,
	title = {Deep Learning for Smart Industry: Efficient Manufacture Inspection System With Fog Computing},
	volume = {14},
	issn = {1941-0050},
	doi = {10.1109/TII.2018.2842821},
	shorttitle = {Deep Learning for Smart Industry},
	abstract = {With the rapid development of Internet of things devices and network infrastructure, there have been a lot of sensors adopted in the industrial productions, resulting in a large size of data. One of the most popular examples is the manufacture inspection, which is to detect the defects of the products. In order to implement a robust inspection system with higher accuracy, we propose a deep learning based classification model in this paper, which can find the possible defective products. As there may be many assembly lines in one factory, one huge problem in this scenario is how to process such big data in real time. Therefore, we design our system with the concept of fog computing. By offloading the computation burden from the central server to the fog nodes, the system obtains the ability to deal with extremely large data. There are two obvious advantages in our system. The first one is that we adapt the convolutional neural network model to the fog computing environment, which significantly improves its computing efficiency. The other one is that we work out an inspection model, which can simultaneously indicate the defect type and its degree. The experiments well prove that the proposed method is robust and efficient.},
	pages = {4665--4673},
	number = {10},
	journaltitle = {{IEEE} Transactions on Industrial Informatics},
	author = {Li, Liangzhi and Ota, Kaoru and Dong, Mianxiong},
	date = {2018-10},
	keywords = {assembly lines, big data, cloud computing, Computational modeling, computing efficiency, convolutional neural network model, Deep learning, deep learning based classification model, defect type, defective products, Edge computing, extremely large data, feedforward neural nets, fog computing, fog computing environment, fog nodes, industrial productions, Industries, Informatics, inspection, Inspection, inspection model, intelligent manufacturing systems, Internet of Things, Internet of Things devices, learning (artificial intelligence), Machine learning, manufacture inspection, manufacture inspection system, network infrastructure, Production, production engineering computing, robust inspection system, smart industry},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ajk\\Zotero\\storage\\YBTTAA4G\\8370640.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\GTLX9THW\\Li et al. - 2018 - Deep Learning for Smart Industry Efficient Manufa.pdf:application/pdf}
}

@online{teerapittayanon_finding_2018,
	title = {Finding the Thresholds for Fast Inference},
	url = {https://blog.chainintel.com/finding-the-thresholds-for-fast-inference-ff96797fda7},
	abstract = {Previously, we discussed augmenting deep neural network models for fast inference with branches in Fast Inference for Deep Learning Models…},
	titleaddon = {Medium},
	author = {Teerapittayanon, Surat},
	urldate = {2019-12-04},
	date = {2018-11-12},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\BB6DMTMR\\finding-the-thresholds-for-fast-inference-ff96797fda7.html:text/html}
}

@inproceedings{wang_see:_2019,
	location = {Miami Beach, {FL}, {USA}},
	title = {{SEE}: Scheduling Early Exit for Mobile {DNN} Inference during Service Outage},
	isbn = {978-1-4503-6904-6},
	url = {http://dl.acm.org/citation.cfm?doid=3345768.3355917},
	doi = {10.1145/3345768.3355917},
	shorttitle = {{SEE}},
	abstract = {In recent years, the rapid development of edge computing enables us to process a wide variety of intelligent applications at the edge, such as real-time video analytics. However, edge computing could suffer from service outage caused by the fluctuated wireless connection or congested computing resource. During the service outage, the only choice is to process the deep neural network ({DNN}) inference at the local mobile devices. The obstacle is that due to the limited resource, it may not be possible to complete inference tasks on time. Inspired by the recently developed early exit of {DNNs}, where we can exit {DNN} at earlier layers to shorten the inference delay by sacrificing an acceptable level of accuracy, we propose to adopt such mechanism to process inference tasks during the service outage. The challenge is how to obtain the optimal schedule with diverse early exit choices. To this end, we formulate an optimal scheduling problem with the objective to maximize a general overall utility. However, the problem is in the form of integer programming, which cannot be solved by a standard approach. We therefore prove the Ordered Scheduling structure, indicating that a frame arrived earlier must be scheduled earlier. Such structure greatly decreases the searching space for an optimal solution. Then, we propose the Scheduling Early Exit ({SEE}) algorithm based on dynamic programming, to solve the problem optimally with polynomial computational complexity. Finally, we conduct trace-driven simulations and compare {SEE} with two benchmarks. The result shows that {SEE} can outperform the benchmarks by 50.9\%.},
	eventtitle = {the 22nd International {ACM} Conference},
	pages = {279--288},
	booktitle = {Proceedings of the 22nd International {ACM} Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems  - {MSWIM} '19},
	publisher = {{ACM} Press},
	author = {Wang, Zizhao and Bao, Wei and Yuan, Dong and Ge, Liming and Tran, Nguyen H. and Zomaya, Albert Y.},
	urldate = {2019-12-07},
	date = {2019},
	langid = {english},
	file = {Wang et al. - 2019 - SEE Scheduling Early Exit for Mobile DNN Inferenc.pdf:C\:\\Users\\ajk\\Zotero\\storage\\8P46A3PR\\Wang et al. - 2019 - SEE Scheduling Early Exit for Mobile DNN Inferenc.pdf:application/pdf}
}

@article{berestizshevsky_sacrificing_2019,
	title = {Sacrificing Accuracy for Reduced Computation: Cascaded Inference Based on Softmax Confidence},
	volume = {11728},
	url = {http://arxiv.org/abs/1805.10982},
	doi = {10.1007/978-3-030-30484-3_26},
	shorttitle = {Sacrificing Accuracy for Reduced Computation},
	abstract = {We study the tradeoff between computational effort and accuracy in a cascade of deep neural networks. During inference, early termination in the cascade is controlled by confidence levels derived directly from the softmax outputs of intermediate classifiers. The advantage of early termination is that classification is performed using less computation, thus adjusting the computational effort to the complexity of the input. Moreover, dynamic modification of confidence thresholds allow one to trade accuracy for computational effort without requiring retraining. Basing of early termination on softmax classifier outputs is justified by experimentation that demonstrates an almost linear relation between confidence levels in intermediate classifiers and accuracy. Our experimentation with architectures based on {ResNet} obtained the following results. (i) A speedup of 1.5 that sacrifices 1.4\% accuracy with respect to the {CIFAR}-10 test set. (ii) A speedup of 1.19 that sacrifices 0.7\% accuracy with respect to the {CIFAR}-100 test set. (iii) A speedup of 2.16 that sacrifices 1.4\% accuracy with respect to the {SVHN} test set.},
	pages = {306--320},
	journaltitle = {{arXiv}:1805.10982 [cs, stat]},
	author = {Berestizshevsky, Konstantin and Even, Guy},
	urldate = {2019-12-07},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1805.10982},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\VAFVHW45\\Berestizshevsky and Even - 2019 - Sacrificing Accuracy for Reduced Computation Casc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\GITV4MLC\\1805.html:text/html}
}

@article{figurnov_spatially_2017,
	title = {Spatially Adaptive Computation Time for Residual Networks},
	url = {http://arxiv.org/abs/1612.02297},
	abstract = {This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging {ImageNet} classification and {COCO} object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.},
	journaltitle = {{arXiv}:1612.02297 [cs]},
	author = {Figurnov, Michael and Collins, Maxwell D. and Zhu, Yukun and Zhang, Li and Huang, Jonathan and Vetrov, Dmitry and Salakhutdinov, Ruslan},
	urldate = {2019-12-07},
	date = {2017-07-02},
	eprinttype = {arxiv},
	eprint = {1612.02297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\NSTC9UD7\\Figurnov et al. - 2017 - Spatially Adaptive Computation Time for Residual N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\K6HS9WGY\\1612.html:text/html}
}

@inproceedings{panda_conditional_2016,
	title = {Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition},
	isbn = {978-3-9815370-7-9},
	url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7459357},
	doi = {10.3850/9783981537079_0819},
	abstract = {Deep learning neural networks have emerged as one of the most powerful classification tools for vision related applications. However, the computational and energy requirements associated with such deep nets can be quite high, and hence their energy-efficient implementation is of great interest. Although traditionally the entire network is utilized for the recognition of all inputs, we observe that the classification difficulty varies widely across inputs in real-world datasets; only a small fraction of inputs require the full computational effort of a network, while a large majority can be classified correctly with very low effort. In this paper, we propose Conditional Deep Learning ({CDL}) where the convolutional layer features are used to identify the variability in the difficulty of input instances and conditionally activate the deeper layers of the network. We achieve this by cascading a linear network of output neurons for each convolutional layer and monitoring the output of the linear network to decide whether classification can be terminated at the current stage or not. The proposed methodology thus enables the network to dynamically adjust the computational effort depending upon the difficulty of the input data while maintaining competitive classification accuracy. We evaluate our approach on the {MNIST} dataset. Our experiments demonstrate that our proposed {CDL} yields 1.91x reduction in average number of operations per input, which translates to 1.84x improvement in energy. In addition, our results show an improvement in classification accuracy from 97.5\% to 98.9\% as compared to the original network.},
	eventtitle = {Proceedings of the 2016 Design, Automation \& Test in Europe Conference \& Exhibition ({DATE})},
	pages = {475--480},
	booktitle = {Proceedings of the 2016 Design, Automation \& Test in Europe Conference \& Exhibition ({DATE})},
	publisher = {Research Publishing Services},
	author = {Panda, Priyadarshini and Sengupta, Abhronil and Roy, Kaushik},
	urldate = {2019-12-07},
	date = {2016},
	langid = {english},
	file = {Panda et al. - 2016 - Conditional Deep Learning for Energy-Efficient and.pdf:C\:\\Users\\ajk\\Zotero\\storage\\CLBR8I4M\\Panda et al. - 2016 - Conditional Deep Learning for Energy-Efficient and.pdf:application/pdf}
}

@article{kaya_shallow-deep_nodate,
	title = {Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	abstract = {We characterize a prevalent weakness of deep neural networks ({DNNs})—overthinking—which occurs when a {DNN} can reach correct predictions before its ﬁnal layer. Overthinking is computationally wasteful, and it can also be destructive when, by the ﬁnal layer, a correct prediction changes into a misclassiﬁcation. Understanding overthinking requires studying how each prediction evolves during a {DNN}’s forward pass, which conventionally is opaque. For prediction transparency, we propose the Shallow-Deep Network ({SDN}), a generic modiﬁcation to off-the-shelf {DNNs} that introduces internal classiﬁers. We apply {SDN} to four modern architectures, trained on three image classiﬁcation tasks, to characterize the overthinking problem. We show that {SDNs} can mitigate the wasteful effect of overthinking with conﬁdence-based early exits, which reduce the average inference cost by more than 50\% and preserve the accuracy. We also ﬁnd that the destructive effect occurs for 50\% of misclassiﬁcations on natural inputs and that it can be induced, adversarially, with a recent backdooring attack. To mitigate this effect, we propose a new confusion metric to quantify the internal disagreements that will likely lead to misclassiﬁcations.},
	pages = {10},
	author = {Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
	langid = {english},
	file = {Kaya et al. - Shallow-Deep Networks Understanding and Mitigatin.pdf:C\:\\Users\\ajk\\Zotero\\storage\\MCY8AMV6\\Kaya et al. - Shallow-Deep Networks Understanding and Mitigatin.pdf:application/pdf}
}

@software{noauthor_socket_nodate,
	title = {socket — Low-level networking interface — Python 3.8.0 documentation},
	url = {https://docs.python.org/3/library/socket.html},
	urldate = {2019-12-09},
	file = {socket — Low-level networking interface — Python 3.8.0 documentation:C\:\\Users\\ajk\\Zotero\\storage\\363H29DJ\\socket.html:text/html}
}

@article{han_convergence_2019,
	title = {Convergence of Edge Computing and Deep Learning: A Comprehensive Survey},
	url = {http://arxiv.org/abs/1907.08349},
	shorttitle = {Convergence of Edge Computing and Deep Learning},
	abstract = {Ubiquitous sensors and smart devices from factories and communities guarantee massive amounts of data and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people's lives, from face recognition to ambitious smart factories and cities, artificial intelligence (especially deep learning) applications and services have experienced a thriving development process. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of "providing artificial intelligence for every person and every organization at everywhere". Thus, recently, a better solution is unleashing deep learning services from the cloud to the edge near to data sources. Therefore, edge intelligence, aiming to facilitate the deployment of deep learning services by edge computing, has received great attention. In addition, deep learning, as the main representative of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually benefited edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely deep learning training and inference in the customized edge computing framework; 3) existing challenges and future trends of more pervasive and fine-grained intelligence. We believe that this survey can help readers to garner information scattered across the communication, networking, and deep learning, understand the connections between enabling technologies, and promotes further discussions on the fusion of edge intelligence and intelligent edge.},
	journaltitle = {{arXiv}:1907.08349 [cs]},
	author = {Han, Yiwen and Wang, Xiaofei and Leung, Victor C. M. and Niyato, Dusit and Yan, Xueqiang and Chen, Xu},
	urldate = {2019-12-09},
	date = {2019-07-18},
	eprinttype = {arxiv},
	eprint = {1907.08349},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\6BL6EV79\\Han et al. - 2019 - Convergence of Edge Computing and Deep Learning A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\HXDDJHAW\\1907.html:text/html}
}

@article{zeng_boomerang:_2019,
	title = {Boomerang: On-Demand Cooperative Deep Neural Network Inference for Edge Intelligence on the Industrial Internet of Things},
	volume = {33},
	issn = {1558-156X},
	doi = {10.1109/MNET.001.1800506},
	shorttitle = {Boomerang},
	abstract = {With the revolution of smart industry, more and more Industrial Internet of Things ({IIoT}) devices as well as {AI} algorithms are deployed to achieve industrial intelligence. While applying computation-intensive deep learning on {IIoT} devices, however, it is challenging to meet the critical latency requirement for industrial manufacturing. Traditional wisdom resorts to the cloud-centric paradigm but still works either inefficiently or ineffectively due to the heavy transmission latency overhead. To address this challenge, we propose Boomerang, an on-demand cooperative {DNN} inference framework for edge intelligence under the {IIoT} environment. Boomerang exploits {DNN} right-sizing and {DNN} partition to execute {DNN} inference tasks with low latency as well as high accuracy. {DNN} right-sizing reshapes the amount of {DNN} computation via the early-exit mechanism so as to reduce the total runtime of {DNN} inference. {DNN} partition adaptively segments {DNN} computation between the {IoT} devices and the edge server in order to leverage hybrid computation resources to achieve {DNN} inference immediacy. Combining these two keys, Boomerang carefully selects the partition point and the exit point to maximize the performance while promising the efficiency requirement. To further reduce the manual overhead of model profiling at the install phase, we develop an advanced version of Boomerang with the {DRL} model, achieving end-to-end automatic {DNN} inference plan generation. The prototype implementation and evaluations demonstrate the effectiveness of Boomerang on both versions in achieving efficient edge intelligence for {IIoT}.},
	pages = {96--103},
	number = {5},
	journaltitle = {{IEEE} Network},
	author = {Zeng, Liekang and Li, En and Zhou, Zhi and Chen, Xu},
	date = {2019-09},
	keywords = {{AI} algorithms, Bandwidth, Cloud computing, cloud-centric paradigm, computation-intensive deep learning, deep neural network inference, {DNN} computation, {DNN} inference immediacy, {DNN} inference tasks, {DNN} partition, edge intelligence, edge server, end-to-end automatic {DNN} inference plan generation, {IIoT} devices, {IIoT} environment, industrial manufacturing, Internet of Things, {IoT} devices, learning (artificial intelligence), manufacturing systems, neural nets, Neural networks, production engineering computing, Runtime, Servers, smart industry, Task analysis},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ajk\\Zotero\\storage\\H6DMRPQV\\8863733.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\T2S8XHYI\\Zeng et al. - 2019 - Boomerang On-Demand Cooperative Deep Neural Netwo.pdf:application/pdf}
}

@inproceedings{hu_dynamic_2019,
	title = {Dynamic Adaptive {DNN} Surgery for Inference Acceleration on the Edge},
	doi = {10.1109/INFOCOM.2019.8737614},
	abstract = {Recent advances in deep neural networks ({DNNs}) have substantially improved the accuracy and speed of a variety of intelligent applications. Nevertheless, one obstacle is that {DNN} inference imposes heavy computation burden to end devices, but offloading inference tasks to the cloud causes transmission of a large volume of data. Motivated by the fact that the data size of some intermediate {DNN} layers is significantly smaller than that of raw input data, we design the {DNN} surgery, which allows partitioned {DNN} processed at both the edge and cloud while limiting the data transmission. The challenge is twofold: (1) Network dynamics substantially influence the performance of {DNN} partition, and (2) State-of-the-art {DNNs} are characterized by a directed acyclic graph ({DAG}) rather than a chain so that partition is greatly complicated. In order to solve the issues, we design a Dynamic Adaptive {DNN} Surgery ({DADS}) scheme, which optimally partitions the {DNN} under different network condition. Under the lightly loaded condition, {DNN} Surgery Light ({DSL}) is developed, which minimizes the overall delay to process one frame. The minimization problem is equivalent to a min-cut problem so that a globally optimal solution is derived. In the heavily loaded condition, {DNN} Surgery Heavy ({DSH}) is developed, with the objective to maximize throughput. However, the problem is {NP}-hard so that {DSH} resorts an approximation method to achieve an approximation ratio of 3. Real-world prototype based on self-driving car video dataset is implemented, showing that compared with executing entire the {DNN} on the edge and cloud, {DADS} can improve latency up to 6.45 and 8.08 times respectively, and improve throughput up to 8.31 and 14.01 times respectively.},
	eventtitle = {{IEEE} {INFOCOM} 2019 - {IEEE} Conference on Computer Communications},
	pages = {1423--1431},
	booktitle = {{IEEE} {INFOCOM} 2019 - {IEEE} Conference on Computer Communications},
	author = {Hu, Chuang and Bao, Wei and Wang, Dan and Liu, Fengming},
	date = {2019-04},
	note = {{ISSN}: 0743-166X},
	keywords = {Bandwidth, Cloud computing, computational complexity, data size, data transmission, deep neural networks, Delays, different network condition, directed graphs, {DNN} inference, {DNN} partition, {DNN} surgery heavy, {DNN} Surgery Light, dynamic adaptive {DNN} surgery scheme, graph theory, heavily loaded condition, heavy computation burden, inference acceleration, inference mechanisms, intermediate {DNN} layers, lightly loaded condition, minimisation, network dynamics, neural nets, Neural networks, {NP}, offloading inference tasks, raw input data, Streaming media, Surgery, Throughput},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ajk\\Zotero\\storage\\66BQ3HWQ\\8737614.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ajk\\Zotero\\storage\\23ENRV9F\\Hu et al. - 2019 - Dynamic Adaptive DNN Surgery for Inference Acceler.pdf:application/pdf}
}

@article{geifman_selectivenet:_2019,
	title = {{SelectiveNet}: A Deep Neural Network with an Integrated Reject Option},
	url = {http://arxiv.org/abs/1901.09192},
	shorttitle = {{SelectiveNet}},
	abstract = {We consider the problem of selective prediction (also known as reject option) in deep neural networks, and introduce {SelectiveNet}, a deep neural architecture with an integrated reject option. Existing rejection mechanisms are based mostly on a threshold over the prediction confidence of a pre-trained network. In contrast, {SelectiveNet} is trained to optimize both classification (or regression) and rejection simultaneously, end-to-end. The result is a deep neural network that is optimized over the covered domain. In our experiments, we show a consistently improved risk-coverage trade-off over several well-known classification and regression datasets, thus reaching new state-of-the-art results for deep selective classification.},
	journaltitle = {{arXiv}:1901.09192 [cs, stat]},
	author = {Geifman, Yonatan and El-Yaniv, Ran},
	urldate = {2019-12-09},
	date = {2019-06-26},
	eprinttype = {arxiv},
	eprint = {1901.09192},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\A3QIJM2E\\Geifman and El-Yaniv - 2019 - SelectiveNet A Deep Neural Network with an Integr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\3SGH59QK\\1901.html:text/html}
}

@article{carvalho_consensual_2013,
	title = {A Consensual Linear Opinion Pool},
	url = {http://arxiv.org/abs/1204.5399},
	abstract = {An important question when eliciting opinions from experts is how to aggregate the reported opinions. In this paper, we propose a pooling method to aggregate expert opinions. Intuitively, it works as if the experts were continuously updating their opinions in order to accommodate the expertise of others. Each updated opinion takes the form of a linear opinion pool, where the weight that an expert assigns to a peer's opinion is inversely related to the distance between their opinions. In other words, experts are assumed to prefer opinions that are close to their own opinions. We prove that such an updating process leads to consensus, {\textbackslash}textit\{i.e.\}, the experts all converge towards the same opinion. Further, we show that if rational experts are rewarded using the quadratic scoring rule, then the assumption that they prefer opinions that are close to their own opinions follows naturally. We empirically demonstrate the efficacy of the proposed method using real-world data.},
	journaltitle = {{arXiv}:1204.5399 [cs, math, stat]},
	author = {Carvalho, Arthur and Larson, Kate},
	urldate = {2019-12-09},
	date = {2013-04-09},
	eprinttype = {arxiv},
	eprint = {1204.5399},
	note = {version: 3},
	keywords = {Computer Science - Multiagent Systems, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\K8JNR7AZ\\Carvalho and Larson - 2013 - A Consensual Linear Opinion Pool.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\R26BFH35\\1204.html:text/html}
}

@article{pei_why_2017,
	title = {Why It Takes So Long to Connect to a {WiFi} Access Point},
	url = {http://arxiv.org/abs/1701.02528},
	abstract = {Today's {WiFi} networks deliver a large fraction of traffic. However, the performance and quality of {WiFi} networks are still far from satisfactory. Among many popular quality metrics (throughput, latency), the probability of successfully connecting to {WiFi} {APs} and the time cost of the {WiFi} connection set-up process are the two of the most critical metrics that affect {WiFi} users' experience. To understand the {WiFi} connection set-up process in real-world settings, we carry out measurement studies on \$5\$ million mobile users from \$4\$ representative cities associating with \$7\$ million {APs} in \$0.4\$ billion {WiFi} sessions, collected from a mobile "{WiFi} Manager" App that tops the Android/{iOS} App market. To the best of our knowledge, we are the first to do such large scale study on: how large the {WiFi} connection set-up time cost is, what factors affect the {WiFi} connection set-up process, and what can be done to reduce the {WiFi} connection set-up time cost. Based on the measurement analysis, we develop a machine learning based {AP} selection strategy that can significantly improve {WiFi} connection set-up performance, against the conventional strategy purely based on signal strength, by reducing the connection set-up failures from \$33{\textbackslash}\%\$ to \$3.6{\textbackslash}\%\$ and reducing \$80{\textbackslash}\%\$ time costs of the connection set-up processes by more than \$10\$ times.},
	journaltitle = {{arXiv}:1701.02528 [cs]},
	author = {Pei, Changhua and Wang, Zhi and Zhao, Youjian and Wang, Zihan and Meng, Yuan and Pei, Dan and Peng, Yuanquan and Tang, Wenliang and Qu, Xiaodong},
	urldate = {2019-12-10},
	date = {2017-05-08},
	eprinttype = {arxiv},
	eprint = {1701.02528},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\ajk\\Zotero\\storage\\C29UNMQE\\Pei et al. - 2017 - Why It Takes So Long to Connect to a WiFi Access P.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ajk\\Zotero\\storage\\C8TNKM9U\\1701.html:text/html}
}

@article{chen_deep_2019,
	title = {Deep Learning With Edge Computing: A Review},
	volume = {107},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/8763885/},
	doi = {10.1109/JPROC.2019.2921977},
	shorttitle = {Deep Learning With Edge Computing},
	abstract = {Deep learning is currently widely used in a variety of applications, including computer vision and natural language processing. End devices, such as smartphones and Internet-of-Things sensors, are generating data that need to be analyzed in real time using deep learning or used to train deep learning models. However, deep learning inference and training require substantial computation resources to run quickly. Edge computing, where a ﬁne mesh of compute nodes are placed close to end devices, is a viable way to meet the high computation and low-latency requirements of deep learning on edge devices and also provides additional beneﬁts in terms of privacy, bandwidth efﬁciency, and scalability. This paper aims to provide a comprehensive review of the current state of the art at the intersection of deep learning and edge computing. Speciﬁcally, it will provide an overview of applications where deep learning is used at the network edge, discuss various approaches for quickly executing deep learning inference across a combination of end devices, edge servers, and the cloud, and describe the methods for training deep learning models across multiple edge devices. It will also discuss open challenges in terms of systems performance, network technologies and management, benchmarks, and privacy. The reader will take away the following concepts from this paper: understanding scenarios where deep learning at the network edge can be useful, understanding common techniques for speeding up deep learning inference and performing distributed training on edge devices, and understanding recent trends and opportunities.},
	pages = {1655--1674},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Chen, Jiasi and Ran, Xukan},
	urldate = {2019-12-10},
	date = {2019-08},
	langid = {english},
	file = {Chen and Ran - 2019 - Deep Learning With Edge Computing A Review.pdf:C\:\\Users\\ajk\\Zotero\\storage\\TV256HRE\\Chen and Ran - 2019 - Deep Learning With Edge Computing A Review.pdf:application/pdf}
}