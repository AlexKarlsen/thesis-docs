\hypertarget{conclusion}{%
\chapter{Conclusion}\label{ch:conclusion}}
%\thispagestyle{fancy}


In conclusion, early exit \gls{dnn}s is shown to be a powerful tool to handle the accuracy-latency trade-off important for time-critical applications. The study reveals, that only a few samples actually require extremely deep models and can be exited with high confidence at a shallower depth. We show, that some models are more suitable for early exit, as they obtain a higher accuracy early exits. However, in our case the model achieving lowest early exit accuracy, obtains a higher end-accuracy. Additionally, it compliments the lower accuracy by faster inference on \gls{gpu}-enabled devices. The accuracy-latency trade-off can be controlled either by a score threshold to reduce the average latency, or by a delay constraint. 

Our inference scheme \gls{aee}, is able to improve the reliability under stringent deadlines using early exit \gls{dnn}s. The scheme can be used for both local inference and remote offloading. Offloading for remote inference show improvements over local execution for most models. The adaptability of the scheme handles latency uncertainties from computation and communication delays by best effort. It enables service for time-budgeted applications in time ranges not possible with the conventional \gls{dnn}. Our results reveal, that no single setting solves all problems. If deadlines are relaxed, and communication and computation latencies are low, then the conventional approach outperforms our scheme. This of course calls for model selection, depending on hardware platforms, connectivity and application deadlines. Our analysis of combining prediction from subsequent exits gave no significant improvements and did show using the deepest available exit is the better option.

