\hypertarget{conclusion}{%
\chapter{Conclusion}\label{ch:conclusion}}
%\thispagestyle{fancy}


In conclusion, early exit \gls{dnn}s is shown to be a powerful tool to handle the accuracy-latency trade-off important for time-critical applications. The study reveals, that only a few samples actually require extremely deep model and can be exited with high confidence a shallower depth. We show, that some model are more suited for early exit, as they obtain a higher accuracy for classifiers at exit. However, the model achieving lowest intermediate accuracy, obtains a higher end-accuracy. Additionally, it compliments the lower accuracy by faster inference on \gls{gpu}-enabled devices. The accuracy-latency trade-off can be controlled either by a score threshold to reduce the average latency or by a delay constraint. The models can be trained in same manner as conventional models by using a sum of cross-entropy losses from all intermediate classifiers at the exits.

Our inference scheme \gls{aee}, is able to improve the reliability under stringent deadlines using early exit \gls{dnn}s. The scheme can be used for both local inference and remote offloading. Offloading for remote inference show improvements over local execution for most models. The adaptability of the scheme handles latency uncertainties from computation and communication delays by best effort. In fact, it enables service for time-budgeted applications in time ranges not possible with the conventional \gls{dnn}. Our results reveal, that no single setting solves all problems. If deadlines are relaxed, communication and computation latencies are low, then the conventional approach outperforms our scheme. This of course calls for model selection, depending on hardware platforms, connectivity and application deadlines. Our analysis of combining prediction from subsequent exits gave no significant improvements and did show using the deepest available exit is the better option.

