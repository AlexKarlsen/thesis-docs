\hypertarget{Edge Intelligence}{%
	\chapter{Edge Intelligence}\label{ch:edgeintelligence}}
\thispagestyle{fancy}


Section \ref{sec:ei-background} explains the background for \acrlong{ei}. Section \ref{sec:ei-architecture} describes different edge-centric architectures for \gls{dnn} inference. Section \ref{sec:ei-fast-inference} present enabling technologies for fast inference.


\section{Background}\label{sec:ei-background}

\acrlong{ei} is artificial intelligence applications and services deployed at the edge of the network. The primary objective of \gls{ei} is to enable \gls{ai} for mobile and \gls{iot} application. Progress within \gls{ml} have been pushed by the achievements within \gls{cv} and \gls{nlp} using \gls{dnn}s. However deployment of \gls{dnn} in real applications have been limited to the cloud, as state-of-the-art \gls{dnn} have been too computationally expensive to run elsewhere. The improvement have mostly been impacted by efforts in training deeper models with the ability to learn increasingly complex features. The tendency is exemplified by the winner of ImageNet Challenge \cite{russakovsky_imagenet_2015}; within a time span of only four years, have the number of layers have grown from 8 to 152 layers. 

Proposals of increasingly deeper model have hindered deployment of intelligent services onto mobile and \gls{iot} devices, which have made \gls{ci} inevitable. Running algorithm in large-scale data centers comes with the side-effect of introducing unpredictable communication delays, caused by possibility of congestion in shared communication medium and shared computing resources in the data center. The additional delay from \gls{ci} have made real-time \gls{ai} impractical for mobile and \gls{iot} devices. 
More lightweight, albeit less accurate \gls{dnn} architectures have been proposed to enable \gls{di}, as end-devices have become more powerful in recent years. \gls{ei} is a compromise, which enables deployment of very deep and demanding state-of-the-art \gls{dnn}s on edge server to reduce inference latency, energy consumption and memory footprint of on-device inference, by offloading the heavy task from the end-device onto edge servers, without introducing the communication latency bottleneck by running extremely deep models in the cloud. 

Edge intelligence is a promising compromise to obtain state-of-the-art performance in real-time, necessary for applications such as \gls{ar}/\gls{vr}, \gls{av} and Personal Assistant with very stringent latency and reliability requirements. In edge computing, data processing is done in closer proximity to end-users. The shortened communication path leads to a reduction in communication latency. Edge computing seeks to distibute computing resources, in sharp contrast to the last decade of centralization of computing resources for cloud computing \cite{shi_edge_2016}. Edge computing is envisioned to reliably serve the ever growing number of billions upon billions of connected mobile and \gls{iot} devices. By 2020 Cisco Internet Business Solutions Group, 50 billion things is predicted to be connected to the internet. The amount of data generated by mobile and \gls{iot} device at the edge of the network is expected to overwhelm cloud data centers and exhaust available bandwidth. Hence edge computing is a possible and probably a necessary step in the development and democratizing of \gls{ai}.

Research have shown, that the conventional cloud-only approach in many cases is actually slower than a mobile-only execution in \cite{kang_neurosurgeon:_2017}, as mobile devices are getting more resourceful and nowadays it is not uncommon to have a smartphone equipped with a \gls{gpu}. To avoid draining the battery \gls{iot} devices, it may not be desired to run heavy \gls{ai} algorithms, which makes remote offloading a necessity.  In \cite{karlsen_prototyping_nodate} edge-based offloading are investigated and show significant latency improvement compared to both device- and cloud intelligence by offloading \gls{ai} tasks from \gls{cpu}-enabled end devices to a \gls{gpu}-enabled edge server. 

Furthermore a major concern of cloud computing is privacy. Data generated by end devices might be confidential, hence not allowed to be processed by a data center unless confidentiality can be guaranteed.  Edge computing can may address this concern, as no data is expected to leave the network, as all processing is done at the network edge. However, edge processing alone, does not ensure privacy, as data may be susceptible to interception, as it may be well understood by an adversary. In section \ref{sec:ei-architecture}, it is described how certain edge-centric architectures can promote privacy. 

(Maybe add the push and pull factors of \cite{zhou_edge_2019}) 
 
The survey \citetitle{zhou_edge_2019} by \citet{zhou_edge_2019} review the current state within the research field of \gls{ei}. The survey includes training and inference of \gls{dnn} on the edge and categorizes several performance metrics for \gls{ei} applications and services. This thesis mainly focuses on the inference for real-time application and services and will not address edge-specific training methods. 

In these next section edge-centric architectures for \gls{dnn} model inference.

\newpage
\section{Edge-centric Architectures} \label{sec:ei-architecture}

Figure \ref{fig:edge_arch} illustrates four architectures for model inference at the edge; \protect\subref{fig:device-based} device-based, \protect\subref{fig:edge-based} edge-based, \protect\subref{fig:edge-device-mode} collaborative edge and \protect\subref{fig:edge-cloud-mode} collaborative edge-cloud. 

\begin{figure}
	\begin{minipage}{0.65\linewidth}
		\textbf{\protect\subref{fig:device-based} \textsc{Device-based}}
		\color{caption-color} \newline
		The only thing, that separates this architecture from \gls{di} is the model is obtained from the edge. The end device acquires input data and performs model inference, which is basically \gls{di}. Since all computation is done on the end device, the performance is solely reliant on the computing resources of the end device.  
	\end{minipage}%
	\hfill
	\begin{minipage}{0.3\linewidth}
		\centering
		\captionsetup[subfigure]{justification=centering}
		\begin{figure}
			\centering
			\subfloat[Device-based\label{fig:device-based}]{\includegraphics[width=\linewidth]{figures/models/device}}
		\end{figure}
	\end{minipage}
	
	\begin{minipage}{0.3\linewidth}
		\centering
		\begin{figure}
			\centering
			\captionsetup[subfigure]{justification=centering}
			\subfloat[Edge-based\label{fig:edge-based}]{\includegraphics[width=\linewidth]{figures/models/edge}}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}{0.65\linewidth}
		\textbf{\protect\subref{fig:edge-based} \textsc{Edge-based}}
		\color{caption-color} \newline
		Data and model inference is offloaded to the edge. The end device still acquires input data and transfer it to the edge. The edge performs model inference and send back the prediction results to the end device. The performance now relies on edge server computing resources and the available network bandwidth between end device and edge server. If time is the main concern, remote offloading is only sensible, whenever time could be saved, compared to device-based model inference. Or if energy efficiency is the main concern, remote offloading is only sensible if energy could be saved by communicating the data to the edge compared to device-based model inference.
	\end{minipage}
\end{figure}

The first two modes are the conventional schemes, which resembles the current \gls{di} and \gls{ci} frameworks, where all processing are done by one peer. The next two schemes presents collaborative architectures, where processing is divided for partial execution between the peers.

\begin{figure}
	\begin{minipage}{0.65\linewidth}
		\textbf{\protect\subref{fig:edge-device-mode} \textsc{Collaborative Edge}}
		\color{caption-color} \newline
		The end device acquires input data and performs partially model inference. The intermediate data is transferred to an edge server which finalizes model inference. The performance relies on the computing resource of both the end device and the edge server and edge server workload, as well as available network bandwidth between end device and edge server. Collaborative egde promotes privacy, as intermediate data of model inference is offloaded. These features have no apparent meaning for humans, which will improve the confidentiality of data.   
	\end{minipage}%
	\hfill
	\begin{minipage}{0.3\linewidth}
		\centering
		\captionsetup[subfigure]{justification=centering}
		\begin{figure}
			\centering
			\subfloat[Collaborative Edge\label{fig:edge-device-mode}]{\includegraphics[width=\linewidth]{figures/models/edge_device}}
		\end{figure}
	\end{minipage}
	\begin{minipage}{0.5\linewidth}
		\centering
		\captionsetup[subfigure]{justification=centering}
		\begin{figure}
			\centering
			\subfloat[Collaborative Edge-Cloud\label{fig:edge-cloud-mode}]{\includegraphics[width=\linewidth]{figures/models/edge_cloud}}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\linewidth}
		\textbf{\protect\subref{fig:edge-cloud-mode} \textsc{Collaborative Edge-Cloud}}
		\color{caption-color} \newline
		Resembles edge-device mode, however the model inference task is now partitioned between edge server and cloud data centers. The model is now reliant on edge server and data center computing resources, but even more so on network connection between edge and cloud. The scheme does too promote privacy, however only between edge and cloud and not in the local network at the edge. The two scheme could easily by combined to split inference between device, edge and cloud.
	\end{minipage}
	\caption[Edge-centric Architectures]{Edge-centric Architectures: \protect\subref{fig:device-based} device only execution, \protect\subref{fig:edge-based} edge only execution,\protect\subref{fig:edge-device-mode} edge and device partial execution, \protect\subref{fig:edge-cloud-mode} edge and cloud partial execution}
	\label{fig:edge_arch}
\end{figure}


Moving model inference away from data centers with high-end specialized hardware offering tremendous amounts of computing power, to less powerful edge clusters or even just a standalone edge server, calls for improvements to model inference of extremely deep models, to adapt to the more diverse hardware landscape at the edge. In the next section, we present existing literature proposals of enabling technologies aiming at reducing the model inference time.

\section{Fast Inference} \label{sec:ei-fast-inference}

In this section enabling technologies for fast inference will be describes. The main focus is \gls{ei}, however, several technologies are universal to \gls{di} and \gls{ci} as well. Our main concern is model inference for real-time applications. Thus we have chosen to present the technologies, that operate on a model level e.g., model architectures, -compression, -partitioning-, -selection, -early exit and layer skipping. We have chosen to omit technologies that operates on a infrastructural level e.g., edge caching, input filtering and support for multi-tenancy \cite{zhou_edge_2019}.

\subsection{Model Architectures}

To improve inference latency on a model-basis, efforts have been made in designing \gls{dnn}s, that reduces the size of the model to more efficiently run on mobile device. Commonly for \gls{mobilenet}s \cite{howard_mobilenets:_2017,sandler_mobilenetv2:_2018}, \gls{shufflenet}s \cite{zhang_shufflenet:_2017, ma_shufflenet_2018} and \gls{squeezenet}s \cite{iandola_squeezenet:_2016}, they all propose novel \gls{dnn} architectures, that uses innovative ways to reduce reduce the number of parameters, and the amount of \gls{flop}s required to run the model, without sacrificing model depth and accuracy. They propose more effective ways of connecting layers or more efficient convolution operations. Other efforts have been made to reduce the number of parameters for existing \gls{dnn} architectures using model compression. 

\subsection{Model Compression}

Model compression is all about finding a more compact or compressed way to represent the model. Weight pruning is one of such methods. Weight pruning removes redundant weights, which have shown significant speed-ups with only a small loss in accuracy \cite{zhou_edge_2019}. The impact of compression is application dependent and can be applied to any pre-trained \gls{dnn} and in combination with other techniques \cite{cheng_survey_2017}.

Another compression technique is quantization. A more compact representation of a floating point is used to reduce the amount of bits needed for each weight of the model \cite{cheng_survey_2017}. The extreme case is binarization, where weights and activations are learned as a binary representations. In BinaryConnect \cite{courbariaux_binaryconnect:_2015} and \gls{bnn} \cite{courbariaux_binarized_2016} most arithmetic operations are replaced with bit-wise operations, thus greatly improving the power-efficiency and inference latency, however using binary weights in extremely deep network have also shown significant degradation of model accuracy \cite{cheng_survey_2017}.

In \cite{hinton_distilling_2015} \gls{kd} has been proposed. \gls{kd} is a framework to train \gls{dnn} in a student-teacher paradigm. The student network are penalized using the output of an ensemble of teacher networks. \gls{kd} can be viewed as a compression of an ensemble of teacher network into a student network \cite{cheng_survey_2017}. 
In \cite{romero_fitnets:_2014} \gls{fitnet} have been proposed as a method to train thinner and shallower student networks using a deeper teacher network. FitNet learns the student network to mimick the teacher network, however this approach require the softmax loss function, which limits its usage \cite{cheng_survey_2017}.  

Other approaches specifically related to edge or cloud computing aiming to reduce inference latency, is model partitioning between end device and edge server. 

\subsection{Model Partitioning}

Model partitioning is an approach to reduce inference latency on an architectural level. Instead of solely executing a \gls{dnn} in the cloud, edge or on device, the computing resources collaborates to boost inference time. Model partitioning is also referred to as collaborative edge or scheme.

Splitting a model is an inherent nature of sequential \gls{dnn}s, that the inference process at any layer can be stopped. The intermediate output after the last executed layer are transferred over the network, and the inference continues at the next layer on an edge server, as shown in figure \ref{fig:offlaoding}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/partitioning}
	\caption[Model partitioning]{Edge-Device model partitioning run part of the model on-device and offload the rest to edge processing. Network partitioning utilize the assumption, that at some later point in the \gls{dnn} a smaller representation of the data is found, illustrated by the gradually decreasing model layers, to reduce the communication bottleneck. }
	\label{fig:offlaoding}
\end{figure}

Neurosurgeon \cite{kang_neurosurgeon:_2017} is a lightweight partitioning scheduler, that uses knowledge of the individual layers of the \gls{dnn} to effectively reduce inference latency. Communication delay is the bottleneck in such an offloading application, hence a smaller representation of the input data is needed, however the layers producing a smaller output than the original input, typically lies deep within the network. Neurosurgeon construct regression models for per layer execution time and output data size of the \gls{dnn}. The regression model are used to decide the best partition of the \gls{dnn} based on available communication data rate. The work is based on cloud intelligence setup and shows, that the conventional cloud-only approach is actually insufficient due to different low data rate connections to cloud data center and the trend that more mobile device is being equipped with \gls{gpu}s. 
% Evidently moving computation to the edge reduces the communication latency. 

Efforts have been made to reduce communication overhead, when splitting \gls{dnn}s to run in a collaborative scheme. One of which is compression of intermediate features before offloading to cloud or edge \cite{choi_deep_2018}. The paper shows, that lossless compression has, as expected, no impact on accuracy. However, bit saving is also rather limited. Lossy compression, on the other hand, results in 70\% bit savings, but also have negative impact of model accuracy. To compensate a compression aware training have been proposed. The follow up paper \cite{choi_near-lossless_2018} propose a novel compression technique, especially designed for deep features i.e. the output of an intermediate \gls{dnn} layer and with bit saving significantly higher bit savings compared to conventional image compression algorithms such as JPEG. Alternatively, methods such as \gls{bottlenet} proposes novel \gls{dnn} module, that creates a low dimensional representation of the output features, able to be restored to original dimensionality. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/compressed}
	\caption[Feature compression]{Feature compression}
\end{figure}

\gls{bottlenet} \cite{eshratifar_bottlenet:_2019} is a novel neural network module. Client-side it consists of a reduction unit and a compressor unit and server-side of a decompressor unit and restoration unit. The reduction unit creates a smaller representation of intermediate features by applying spatial- and channel-wise convolution. The compressor uses lossy JPEG compression and sends the data to the server. The server decompresses the received data and the restoration unit restores the intermediate feature using channel- and spatial-wise deconvolution, to get the required input size for the next layer in the \gls{dnn}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/bottlenet}
	\caption[BottleNet Unit]{BottleNet Unit}
\end{figure}

\gls{bottlenet} is able to achieve 84$\times$ bit savings compared to cloud-only approach with less than 2\% degradation of accuracy caused by lossy compression with compression-aware training. Under good networking condition, the evaluation of \gls{bottlenet} shows, that the best split is after the first convolutional block, as a smaller representation of the input can be found and the server is a more powerful machine. Compared to cloud-only approach using WiFi a 8$\times$ speed up is found.

The collaborative scheme between end device and edge servers shows improvements for \gls{cpu}-enabled end devices, compared to a cloud-only approach, however as communication is introduced the overall latency will vary depending on the networking conditions. Model selection is an approach, that tries to limit the amount of offloading, by first running an on-device model. 

\subsection{Model Selection}

Model selection is an approach to reduce inference by selecting an appropriately accurate model, hence not using an unnecessarily deep model, if a shallower model could give a satisfactory prediction. In \cite{bolukbasi_adaptive_2017} an adaptive model selection framework is proposed. The framework stacks three \gls{dnn} with increasing depth and increasingly higher accuracy models; \gls{alexnet}, \gls{googlenet} and \gls{resnet}50. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/adaptive}
	\caption[Adaptive Neural Network]{Adaptive Neural Network using model selection}
\end{figure}

The input is inferred to \gls{alexnet}, if the confidence scores is satisfactory the prediction is accepted, if not the framework decides to use either \gls{googlenet} or \gls{resnet}50 depending on a confidence threshold, however if the sample is inferred to \gls{googlenet} and the confidence is still unsatisfactory the sample is inferred to \gls{resnet}50 for final prediction. The work shows improvement of the average prediction time with only a small reduction in accuracy depending on the confidence threshold. However, for hard samples, since multiple model are introduced, the inference time is increased, as well as the computational cost and memory consumption, thus such model selection approach seems overwhelming to introduce on a constraint end device. Yet it may be feasible to offload to the edge or use collaborative edge using a Big/Little setup.

To obtain faster inference Big/Little \gls{dnn} \cite{park_big/little_2015} implements a hybrid edge architecture of device-only and selective offloading for edge-only processing. It runs a shallower, albeit less accurate model on device, and a deep and more accurate model on the edge server, as illustrated by figure \ref{fig:big/little-dnn}. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/big_little_dnn}
	\caption[Big/Little \gls{dnn} architecture]{Big/Little \gls{dnn}, a hybrid edge architecture. An on-device model is used to selectively offload to a more complex model hosted on an edge server.}
	\label{fig:big/little-dnn}
\end{figure}

If the prediction confidence of the little model is unsatisfactory, a decision is made to offload to the big model on the edge server. If a lot of samples are able to be correctly classified locally, a speed-up is gained. However, the down-side of this approach is, if too many samples require the big model to satisfy a certain confidence threshold, a lot of work is wasted on the on-device prediction. Although Big/Little \gls{dnn} obtain good results on energy savings and inference latency, other methods such as early exiting, does not require a inference to re-run on bigger model.

\subsection{Model Early Exit}

%Since early exiting less computation is wasted, if offloading is necessary after an exit compared to a Big/Little model selection setup. As less computation is needed and a smaller representation of input data can be obtained, thus less data must be offloaded to the edge server.
Cascading \gls{dnn} \cite{leroux_resource-constrained_2015} and \gls{branchynet} \cite{teerapittayanon_branchynet:_2016} are both early exiting frameworks for fast inference using \gls{dnn}s. Research have shown, that only a few number of samples actually require extremely deep models to be correctly classified \cite{bibid}. The frameworks, illustrated in figure \ref{fig:branchynet}, adds a cascade of intermediate classifiers, or branch exits to the \gls{dnn}, that allow samples with a confidence higher than a selected threshold, at an early exit, to be classified and exit the inference process prematurely. Other samples may require a later exit to obtain a high enough confidence score, or perhaps the entire depth of the model. The selection of an early exit threshold resembles model selection, if a too high threshold is selected, only a few sample might be able to exit the model at an early exit. If too low a threshold a lot of samples will exit the model and be incorrectly classified, thus degrading the model accuracy. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/branchy}
	\caption[BranchyNet Architecture]{BranchyNet Architecture}
	\label{fig:branchynet}
\end{figure}

Cascading neural network \cite{leroux_resource-constrained_2015} have been followed up in \cite{leroux_cascading_2017} and \gls{branchynet} have been extended to \gls{ddnn} in \cite{teerapittayanon_distributed_2017}. Both papers adds network splitting to the early exiting model, to distribute it over computing hierarchy e.g. end devices, edge and cloud. Figure \ref{fig:early-exit-colab} illustrates, that adding network partitioning to an early exit \gls{dnn} for collaborative edge, is simply just running run the model up to one or more exits on end-device, and continue with one or more exits on the edge server. However, networking partitioning adds transmission delay to the inference time, that most be accounted for e.g. using one of the method for network partitioning.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/cascaded}
	\caption[Cascaded \gls{dnn} over a computing hierarchy]{Cascaded \gls{dnn} over a computing hierarchy}
	\label{fig:early-exit-colab}
\end{figure}

\gls{ddnn} \cite{teerapittayanon_distributed_2017} have a different perspective, instead of having a single stream \gls{dnn}. \gls{ddnn} uses a cluster of $d$ end devices, that all run the same shallow part of an early exit \gls{dnn}, to collaboratively solve a classification challenge. The output of the $ d $ upstream \gls{dnn}s are aggregated at an exit point, and if needed offloaded to edge, to run the deep part of the \gls{dnn}. The paper investigate different aggregation schemes and find, that concatenation of features and using the deeper part of the \gls{dnn} for sensor fusion, reduce inference time and even improve accuracy. However, sensor fusion using concatenation require re-implementation of the early exit model, to adapt to additional amount of features at the aggregation point. Also concatenation of features increases the data size to $ d $ times data size to be offloaded to the edge server. Nonetheless, the work shows benefits from distributed computing to provide fault tolerance and support for sensor fusion of geographically dispersed sensors to improve recognition accuracy. \todo{should i give this critique?}

Edgent \cite{li_edge_2018} is built is a framework built on top of a \gls{branchynet} and is a proposal, that tries to handle the communication delay, by selecting optimal partitioning point. Edgent is an optimization of the latency-accuracy trade-off for mission-critical application with a predefined deadline. It tries to optimize the selection of exit and partitioning point of a cascaded early exiting model in the online phase. The optimization is based on a latency requirement, a regression model of each layer type of the used \gls{branchynet} model and the observed available bandwidth between end device and edge server. Experiments in the paper show, that Edgent is able to meet more stringent deadlines, than running solely on device or edge and naively selecting partitioning points. However, one may question whether Edgent is actually an early exiting framework, as the \gls{dnn} optimizer decides an exit and partitioning point upfront to use as much of available time budget as possible without violating the latency requirement, hence model selection or in this case exit selection. In \gls{branchynet} and \gls{ddnn} some samples should be able to exit early, thus when the \gls{dnn} is cascaded over the network, some samples should be able to exit locally on end devices to reduce expensive cost of network communication in terms of both latency and energy usage. When using a powerful edge server more weight will be added to offloading and no savings are gained on easy samples and the accuracy of harder samples are degraded. Edgent as a exit selection framework is still a significant contribution for time-critical applications. \todo{this should have another go... I do not know if i should give this critique here or save it for my justification of my offloading scheme}

On a per model level \citeauthor{huang_multi-scale_2017} have in \cite{huang_multi-scale_2017} investigated different state-of-the-art architectures for early exiting. They have found, that  densely connected layers of \gls{densenet} \cite{huang_densely_2016} are more suitable for early exiting than the popular \gls{resnet} architecture build of residual blocks. Densely connected block uses a concatenation of features from all layers for final classification. The combination of  general feature and increasingly more specific and complex features are shown to be important for early exiting models. This finding have been used to come of with a novel \gls{dnn} archtiecture specifically designed for early exiting. \gls{msdnet} \cite{huang_multi-scale_2017} uses the densely connected layers along with multi-scale paths.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/msdnet}
	\caption[\gls{msdnet} Architecture]{\gls{msdnet} Architecture, Source: \citetitle{huang_multi-scale_2017} \cite{huang_multi-scale_2017}}
	\label{fig:msdnet}
\end{figure}

\gls{msdnet} addresses two main problems concerning early exiting. The first problem is the lack of coarse-level features in early classifiers. Traditional \gls{dnn}s uses stacking of layers to get coarse level features, which the early classifier lacks, thus giving unsatisfactory high error-rates. Multi-scale feature maps addresses this issues by preserving high-resolution information and allow constructing coarse-level features for all classifiers in the network.

The second problem is early classifiers interfere with later classifiers. The early classifiers might cause early features to be optimized for the short-term by collapsing information prematurely, thus harming the later and final classifiers. Their study reveals, that densely connected layers suffers much less from intermediate classifiers, as a layer is connected to all previous layer and is therefore able to recover collapsed information.

\subsection{Model Layer Skipping}

Other approaches to reduce inference time of \gls{dnn} involve mechanism for skipping certain layers. SkipNet \cite{wang_skipnet:_2017} is a framework for adding dynamic decision for skipping layers. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/skipnet}
	\caption[SkipNet]{SkipNet}
\end{figure}

The framework adds complexity to the model by introducing skipping gates between intermediate layers of the network. SkipNet is trained using a hybrid of supervised learning and reinforcement learning. The classification is learnt as a usual supervised problem using labeled data. The skipping policies are learnt using reinforcement learning by rewarding skipping decisions, that have little impact on classification accuracy. The work shows, that only a small fraction of inputs actually require these extremely deep models, thus SkipNet is able to reduce the computational cost by 30\% of \gls{resnet}101 on \gls{ilsvrc2012}. 

BlockDrop \cite{wu_blockdrop:_2017} is another approach to learn skipping policies. However, instead of adding intermediate skipping gates for dynamic local decisions, BlockDrop trains a global policy network, that selectively chooses which model depth to use. BlockDrop is in fact a learned model selection framework, where the selection of models is not based on a confidence output of a smaller model, but on a \gls{dnn} trained to predict the complexity of an input sample. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/blockdrop}
	\caption[BlockDrop]{BlockDrop}
\end{figure}

The policy network is similarly trained using reinforcement learning. The training shows, which classes are easy and which are hard. SkipNet and BlockDrop could both be used in an edge-device mode such as a Big/Little \cite{park_big/little_2015} setup. Particularly BlockDrop where the inexpensive policy network is run by an end device to selective choose among a smaller model on-device or a larger model on an edge server to avoid wasteful executions.

We study early exiting model in this thesis, due to its flexible nature to prematurely exit samples useful for time-critical application with stringent deadlines. In the next chapter early exiting is elaborated upon and used for experiments.



