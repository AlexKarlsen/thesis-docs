\section{Edge Intelligence}

State-of-the-art \gls{dnn} are getting deeper i.e. adding more layer to the models to improve model accuracy, however the added amount of model parameters require additional computations, hence inference time is degraded \cite{bibid}. Conventionally intelligent mobile applications have been run in the cloud, introducing highly unreliable latency in \gls{wan}. As end-devices have become more powerful, smaller albeit less accurate \gls{ai} architectures have been proposed, however with the emergence of \gls{iot} less powerful devices, not able to run highly demanding algorithms are constantly being connected to the internet. Devices which work in application, that advantageously could benefit from intelligence. In order to find the right balance between primarily highly accurate \gls{mcc} and highly responsive mobile computing, a new computing paradigm emerges namely \gls{mec}.  

The survey \citetitle{zhou_edge_2019} by \citet{zhou_edge_2019} review the current state within the research field of \gls{ei}. The survey includes training and inference of \gls{dnn} on the edge and categorizes several performance metrics for \gls{ei} applications and services. 

This thesis mainly focuses on the inference process and will not address edge-specific training methods. In these next sections architectures, performance metrics and enabling technologies for edge-centric inference will be described.  

\subsection{Architectures}

Inference architectures for edge-centric \gls{ei} application and services can be categories into four main models \cite{zhou_edge_2019}:

%\begin{figure}
%	\centering
%	\captionsetup[subfigure]{justification=centering}
%	\subfloat[Device-based\label{fig:device-based}]{\includegraphics[width=.2\linewidth]{figures/models/device}}
%	\subfloat[Edge-based\label{fig:edge-based}]{\includegraphics[width=.2\linewidth]{figures/models/edge}}
%	\subfloat[Edge-Device mode\label{fig:edge-device-mode}]{\includegraphics[width=.2\linewidth]{figures/models/edge_device}}
%	\subfloat[Edge-Cloud mode\label{fig:edge-cloud-mode}]{\includegraphics[width=.37\linewidth]{figures/models/edge_cloud}}
%	\caption[Edge-centric architectures]{Edge Architectures: \protect\subref{fig:device-based} device only execution, \protect\subref{fig:edge-based} edge only execution,\protect\subref{fig:edge-device-mode} edge and device partially execution, \protect\subref{fig:edge-cloud-mode} edge and cloud partially execution. }
%\end{figure}
%
%\begin{description}
%	\item[Device-based Mode] end device obtain model from edge server. The end-device then acquires input data and performs model inference. Since all computation is done on the end device, performance is solely reliant on the end device's computing resources.  
%	\item[Edge-based Mode] end device acquires input data. The input data is transferred to an edge server, which performs model inference and send the prediction results to the end device. The performance relies on edge server computing resources and network bandwidth.
%	\item[Edge-Device Mode] end device acquires input data and performs partially model inference. The intermediate data is transferred to an edge server which finalizes model inference. The performance relies on end device's and edge server computing resource, network bandwidth and edge server workload. 
%	\item[Edge-Cloud Mode] resemble edge-device mode, however the model inference task is now partitioned between edge server and cloud data centers. The model is now reliant on edge server and data center computing resources, but even more reliant on \gls{wan} transmission rate between edge and cloud. 
%\end{description}

\subsection{Performance Metrics}

The aim of edge intelligence is to accommodate certain performance metrics:

\begin{description}
	\item[Latency] is defined as the overall time $T$ of the inference process, including from data is generated at the device, data transmission, preprocessing, model inference and postprocessing. For \gls{ei} real-time application, such as \gls{ar}/\gls{vr}, where stringent deadlines requirements must be met e.g. 100ms \cite{bibid}. Latency is affected by several factors; computing resources, data rate, model architecture and execution.
	
	\item[Accuracy] is defined as the ratio of correctly predicted input samples from the total number of inputs. 
	\begin{align*}
		\alpha = \frac{n_c}{N}
	\end{align*}
	Accuracy requirements are dependent on the \gls{ei} application, for instance autonomous vehicles require extreme accuracy with extremely low latency. The essential trade-off is how accurate a model can we use while still satisfying latency demands.  
	
	\item[Energy efficiency] end devices are typically battery powered, thus applications must be optimized for energy efficiency. Energy efficiency for offloading application is finding the right trade-off between computation and communication. The hardware of end-devices play a crucial role, it may be more sensible to have a device-only execution, if it's a powerful \gls{gpu} enabled mobile device. If it's a \gls{cpu}-only device, then naively offloading entire model inference may introduce a communication overhead in poor communication environments, hence a collaborative execution model between end-device and edge may be optimal. 
	
	\item[Communication overhead] is introduced whenever inference is offloaded for remote computation. Computational offloading is only sensible whenever time could be saved compared to local execution.
	\begin{align*}
	T_{local\: computation} > T_{remote\: computation} + T_{communication}
	\end{align*}
	Or if energy could be saved.
	\begin{align*}
		E_{local\: computation} > E_{communication}
	\end{align*}
	The energy impact of communication is highly dependent on the wireless technology, as is the overall latency. Cloud-offloading is highly dependent on unreliable and expensive \gls{wan} connection to a cloud data center, as high variation in network traffic and server workload occurs. Edge-offloading mitigates the impact of communication overhead, by solely depending on a more reliable \gls{lan} network connection to edge servers.  
	 
	\item[Privacy] data generated by end devices might be confidential, hence not allowed to be processed by a data center unless confidentiality can be guaranteed. Privacy relies on how data is process within the \gls{ei} application. Offloading model input data, may be susceptible to interception, as it may be well understood by an adversary, however offloading intermediate data with no apparent meaning for humans may help improve the confidentiality of data.    
	
	\item[Memory footprint] reducing memory footprint of \gls{dnn} inference is important especially for mobile device. The mobile device do not have dedicate \gls{gpu} memory, hence \gls{dnn} application will have to compete with other applications running on both \gls{cpu} and \gls{gpu}. The resource hungry \gls{dnn} application will drain battery resources and potentially harm performance of coexisting applications. 
	
	\item[Confidence] Confidence and Score-Margin  
\end{description}

This work is mainly concerned the accuracy-latency trade-off for inference in \gls{ei} applications and services. Thus, it will primarily address technologies regarding these performance metrics, and refer to the survey for a broader perspective on all of the performance metrics for \gls{ei} applications and services. The centralized device-based and edge-based architectures are addressed by common \gls{dl} literature. The distributed inference models for \gls{ei} is a fairly recent and promising field of research for truly enabling \gls{ai} applications and services.

According to \cite{zhou_edge_2019} there are 8 main effort to shorten inference latency, these comprises; \textit{Model Compression, Model Partition, Model Early Exit, Edge Caching, Input Filtering, Model Selection, Support Multi-Tenancy} and \textit{Application Specific Optimization} (Knowlegde Transfer Learning (student-teacher training)


