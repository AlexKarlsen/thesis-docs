\section{Edge Intelligence}

Motivation??

\subsection{Architectures}

Inference architectures for edge-centric \gls{ei} application and services can be categories into four main models \cite{zhou_edge_2019}:

\begin{description}
	\item[Device-based Mode] end device obtain model from edge server. The end-device then acquires input data and performs model inference. Since all computation is done on the end device, performance is solely reliant on the end device's computing resources.  
	\item[Edge-based Mode] end device acquires input data. The input data is transferred to an edge server, which performs model inference and send the prediction results to the end device. The performance relies on edge server computing resources and network bandwidth.
	\item[Edge-Device Mode] end device acquires input data and performs partially model inference. The intermediate data is transferred to an edge server which finalizes model inference. The performance relies on end device's and edge server computing resource, network bandwidth and edge server workload. 
	\item[Edge-Cloud Mode] resemble edge-device mode, however the model inference task is now partitioned between edge server and cloud data centers. The model is now reliant on edge server and data center computing resources, but even more reliant on \gls{wan} transmission rate between edge and cloud. 
\end{description}

\subsection{Performance Metrics}

The aim of edge intelligence is to accommodate certain performance metrics:

\begin{description}
	\item[Latency] is defined as the overall time of the inference process, including from data is generated at the device, data transmission, preprocessing, model inference and postprocessing. For \gls{ei} real-time application, such as \gls{ar}/\gls{vr}, where stringent deadlines requirements must be met e.g. 100ms. Latency is affected by several factors; computing resources, data rate, model architecture and execution.
	\item[Accuracy] is defined as the ratio correctly predicted input samples from the total number of inputs. 
	\begin{align*}
		\alpha = \frac{n_c}{N}
	\end{align*}
	Accuracy requirements are dependent on the \gls{ei} application, for instance autonomous vehicles require extreme accuracy with extremely low latency. The essential trade-off is how accurate a model can we use while still satisfying latency demands.  
	\item[Energy efficiency] is important as end devices are typically battery powered. Offloading model inference to edge servers introduces communication overhead for the \gls{ei} service.
	\item[Communication overhead] is introduced for all modes, except device-based mode, whenever inference is offloaded for remote computation. The cost in terms of latency is dependent network connection to edge servers and even more reliant on unreliable and expensive \gls{wan} connection to a cloud data center. 
	\item[Privacy] data generated by end devices might be confidential, hence not allowed to be processed by a data center unless confidentiality can be guaranteed. Privacy relies on how data is process within the \gls{ei} application.  
	
	\item[Memory footprint] description
\end{description}

\subsection{Enabling technologies (related work?)}