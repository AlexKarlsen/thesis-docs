\hypertarget{discussion}{%
	\chapter{Discussion}\label{ch:discussion}}
%\thispagestyle{fancy}

In this chapter, we discuss our design choices of our early exit models and inference scheme \gls{aee}. We compare with other related works for both early exiting and inference schemes using early exiting.

In \gls{ddnn} \cite{teerapittayanon_distributed_2017}, a \gls{dnn} is used to fuse the output from early exits to combine the information. Running an additional \gls{dnn}, for sensor fusion at the edge or on-device will cause additional delay, and take time away from running \gls{aee}. In \gls{ddnn}, fused features come from several upstream devices, with exits at the same level. In our case, we have predictions from different levels in the models. It will require future research to investigate if improvement can be found fusing predictions from exits at different levels. Furthermore, it will require an investigation of fusing data with different dimensions using \gls{dnn}s. 

The impact of communication uncertainties is shown in figures \ref{fig:resnet-offloading-vs-local}, \ref{fig:densenet-offloading-vs-local}, and \ref{fig:msdnet-offloading-vs-local}. Compared to local execution, the steps in reliability have been smoothed out when time allows reaching a new exit, as the uncertainties of communication time, causes deadline violations, and result in lost predictions. Our evaluation of \gls{tcp} at increasing distance indicates that using \gls{tcp} leads to significant overhead in retransmission, resulting in additional delay. The communication latency could be reduced if the WiFi communication did not go through an access point. Instead, the connection could be established ad-hoc using WiFi Direct \cite{noauthor_wi-fi_nodate}. Or we could change the \gls{tcp} transport layer to \gls{udp}. \gls{tcp} was used for reliable transport to send a single jpeg image for processing, as jpeg is a lossless compression technique. Using \gls{udp} could cause losing packets, which would impact the reconstruction of images server-side to be faulty and result in incorrectly classification, thus reducing the reliability. \gls{tcp} is not designed for video streaming applications, as retransmission of unacknowledged packet increases the communication delay. As shown in figure \ref{fig:tcp-overhead}, \gls{tcp} introduces a large overhead of retransmission, when network conditions are poor. For video streaming, \gls{udp} is more widely used. \gls{udp} is a more unreliable protocol, which is applicable in scenarios where some packet loss can be tolerated. The \gls{dnn}s may not be susceptible to small packet losses. The \gls{dnn}s have been trained with noise injection where pixels are randomly lost, see figure \ref{fig:augmentation}. Future research could address using \gls{udp} to reduce communication latency as in \cite{liu_maximizing_2019}. We did not pursue reducing the communication time, as our focus is not to obtain the best possible communication latency in this experimental setup, but rather show that \gls{aee} can reliably enable service at lower delays. 

Our inference scheme \gls{aee} is applicable for collaborative inference on device and edge, as in \cite{leroux_cascading_2017,teerapittayanon_distributed_2017}. Collaborative inference can be used to reduce the likelihood of missed predictions when experiencing sporadic communication delays. The on-device inference is only dependent on computation time for the local exits and removes the uncertainties from communicating with the edge server. However, the computing resources of the local device is typically significantly lower than the server's, which increases the delay and thereby reduces the chance of reaching a prediction from a later exit on the server due to time constraints. We investigated partitioning after the first exit but encountered increased transmission delays, as the size of the intermediate features is significantly larger than the compressed images. We also experienced longer compute times locally for the first exit, which is more time consuming on the smaller device than on edge server for most models. The early layers of the \gls{dnn}s are typically also the most demanding, which leads even worse inference time locally. Thus, using collaborative inference degrades the reliability, when no feature compression or \gls{bottlenet} modules is used to reduce transmission time is used. Introducing feature compression would additionally require compression-aware retraining, as stated in \cite{choi_near-lossless_2018,choi_near-lossless_2018,eshratifar_bottlenet:_2019},  to avoid experiencing a high cost of accuracy. Further experimenting with this idea was considered out of scope for this project. Figure \ref{fig:resnet-offloading-vs-local} and \ref{fig:densenet-offloading-vs-local}, show the local inference time of the first exit of \gls{bresnet} and \gls{bdensenet}, is always outperformed by the edge servers. Therefore, offloading the entire task not to waste idle time a faster server.

An alternative solution to handle the lost prediction dilemma is parallel execution of a shallower local \gls{dnn} and a deep remote \gls{dnn}, i.e., parallel Big/Little setup. The end device offloads the compressed image to the edge server, and in the parallel process, a smaller and less accurate \gls{dnn} locally. The upside is that the application can always use the locally obtained prediction or choose to discard it, as more accurate prediction arrives from the edge. Further, if a proper combination function is found, it can use the information to choose the best prediction. If the on-device inference is not an option, then the offloading \gls{aee} is the better option.

We have not considered real-time processing a stream of video frames, but only single image classification. We compare our inference scheme with Edgent. In Edgent, an upfront selection of exit is made to handle the accuracy-latency trade-off. Edgent tries to utilize the available time for the frame without postponing the next one. The choice of exit is based on regression models of inference time and a current state of bandwidth.  We argue that the potential of early exiting is not fully utilized by upfront sub-model selection. The Upfront selection of exit is prone to losing predictions caused by unexpected communication delays. If a timeout happens when exit 3 is selected, and the deadline is passed in block 3, no prediction will be provided.  In such cases, our solutions will be more reliable, as the latency overhead of our additional exits classifier is negligible. Hence, when the deadline is violated, at least two predictions are available from exit 1 and exit 2. In other words, an early prediction is better than no predictions. Only in worst-case, where the first exit is unreachable, neither \gls{aee} nor Edgent will suffice. 

If we encounter time-outs in between two exits, the computation is wasted, which optimally could have been used to process the next frame, if the frame has been received. However, our solution fully utilizes the available time if a subsequent frame is not received. A future extension to our scheme is to let the server handle the deadline. If the server knows the elapsed time, the deadline and the measured bandwidth, a decision can be made to terminate the inference process or continue with the next received frame if the server becomes aware it cannot reach the next exit  

Edge and cloud offloading can potentially experience a service outage. If an access point is no longer accessible, reconnection delays to a new or the same access point, e.g., using WiFi, can be expected to take up to 1 s \cite{pei_why_2017}. If the local inference is not an option, it will inevitably lead to lost predictions. If local execution is possible, we should switch to the local inference of \gls{aee} when experiences service outages. 
