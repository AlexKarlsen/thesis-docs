\hypertarget{discussion}{%
\chapter{Discussion}\label{ch:discussion}}
%\thispagestyle{fancy}

In this chapter we discuss our design choices of our early exit models and inference scheme \gls{aee}. We compare with other related works to both early exiting and inference schemes using early exiting. We do not discuss individual results of the chapter, but refer to the respective chapters for these details. In section \ref{sec:ee-summary} we summarize and discuss the results of our experiment with the early exit models. In section \ref{sec:edge-summary} we summarize and discuss the results from experiments with our proposed inference scheme \gls{aee}.


\section*{Early Exit DNN}

The placement of early exits was placed after down-sampling layers or block, with the primary reason to reduce the offloading data size of intermediate features for collaborative edge. Even though we ended up not pursuing collaborative architectures, reasons why will be discussed later, we got satisfying early exit accuracy. However, in other works, such as \cite{huang_multi-scale_2017}, the placement of early exit is placed within the resolution and dense blocks for repectively \gls{bresnet} and \gls{bdensenet}. They also construct other versions of the \gls{bresnet} and \gls{bdensenet}, which have evenly distributed number of layer within each block. We did not want to add too much overhead from introducing early exits classifiers to the model, hence we limited ourselves to only 4. In fact \cite{berestizshevsky_sacrificing_2019} uses the same placement of classifiers, as in ours. For the same reason, we did add convolution layers, as in \cite{teerapittayanon_branchynet:_2016}, in our exit branches to limit the overhead and wasted computation. Our exit only consists of a pooling layer to reduce the feature size and a linear classifier, as in \cite{kaya_shallow-deep_nodate}. 

We did not construct new versions with the number of layers, that suited our needs, as we did not want to retrain such extremely deep neural network, and we wanted to apply the \gls{branchynet} framework to existing architectures to validate the framework.

The \gls{msdnet} \cite{huang_multi-scale_2017} show very interestingly very good early exit performance, and is able to improve the accuracy, the deeper we let the sample infer. The model requires far less computation, than the \gls{bresnet} and \gls{bdensenet}. It has surprisingly good inference time on the \gls{nuc}. However, it halts on the \gls{gpu}-enabled platforms.mThe \gls{bresnet} achieve decent runtimes utilizing a \gls{gpu}, whereas \gls{bdensenet} achieve decent runtime in either case.Research in parallelization and operation is important to come of with even better models, that works across platforms.
 
The max score confidence threshold is widely used in early exiting literature \cite{leroux_resource-constrained_2015, leroux_cascading_2017, kaya_shallow-deep_nodate, berestizshevsky_sacrificing_2019}. The score-margin has not been used directly for early exiting but used in model selection framework \cite{park_big/little_2015,tann_flexible_2018}. However, one could argue that model selection by stacking an ensemble of network, could be considered early exiting, if we assume the ensemble of model is a single model and the models of the ensemble is the exits. The upside of early exiting is of course, that no inference most be redone, as it is a single model, that uses the results from lower layers, if the sample could not be exited. Whereas a stacked ensemble must do new full inference of deeper model. 

\section*{\acrlong{aee}}

Compared to \gls{ddnn} \cite{teerapittayanon_distributed_2017}, our investigation of \gls{aee} is not used with model partitioning for collaborative edge based. Instead it is based on either local execution or remote offloading, to make full use of computing resource of the more powerful edge server. Still our solution is applicable for collaborative edge, where the end-device locally processes the algorithm up to an early exit and obtains a prediction, then offloads the rest of the execution in a cascaded manner for remote execution. A locally obtained prediction could possibly reduce the amount of missed predictions, if no reply is received from the edge within the time frame. However, we did try partitioning after the first exit, but encountered increased transmission delays and computation time. Further experimenting with this idea was dropped, as it would require implmenting feature compression or \gls{bottlenet} modules to reduce transmission time. Introducing feature compression would additionally require compression-aware retraining, as stated in \cite{choi_near-lossless_2018,choi_near-lossless_2018,eshratifar_bottlenet:_2019}  to avoid experiencing a high cost in accuracy.

It can be justified from looking at figure \ref{fig:resnet-offloading-vs-local} and \ref{fig:densenet-offloading-vs-local}, that show the local inference time of the first exit of \gls{bresnet} and \gls{bdensenet}, is always outperformed by the edge servers. Thus, our choice to not waste idle time on server, by offloading the entire task is the better one for very stringent deadlines. The early layers of the \gls{dnn} is typically also the most demanding, which will lead to worsened inference time exits, and communication of features from the earliest exits, is also heavier than both  features obtained at later exits and sending the compressed image upfront, at least, if no feature compression technique is used.

Instead an alternative solutions to handle the lost prediction dilemma, is a parallel execution of a shallower local \gls{dnn} and a deep remote \gls{dnn} i.e. a parallel Big/Little setup. The end device offloads the compressed image to edge server and in parallel process a smaller and less accurate \gls{dnn} locally. The upside is, that the application can always use the locally obtained prediction and choose to discard it, as more accurate prediction arrives from edge or if a good combination function is found, use the information to choose the best prediction. If offloading is not an possible, then the \gls{aee} offloading is the better option.

We have not considered real-time processing a stream of video frames, but only single image classfication. Comparing our inference scheme \gls{aee} with Edgent, where an upfront selection of exit is to handle the accuracy-latency trade-off. Edgent tries to utilize available time for the frame without postponing the next one. The selection of exit is based on regression models of inference time and a current state of bandwidth. The upfront exit selection is prone to loose predictions caused by unexpected communication delays. Our solutions have higher probability of having received a single prediction, when a timeout occurs. For instance, if the first exit is reachable for \gls{aee}, and if the prediction unexpectedly violates the deadline for a later exit, at least one prediction is available. Whereas for Edgent the same later selected exit, that turned out to not be reachable, then no prediction is received in time. In other words, an early prediction is better than no prediction and only in worst-case where the first exit is unreachable neither \gls{aee} nor Edgent will suffice. We argue, that the potential of early exiting is not fully utilizes by this submodel selection, as the latency overhead of our additional exits classifier is negligible. Additionally such upfront selection also takes some time away from offloading and inference.

Our approach does not address video streaming initially, if we encounter time-out in between to exits the computation is wasted, which optimally could have been used to process the next frame, if received. However, our solution fully utilize the available time, if a next frame is not we received. A future extension to our scheme is to let the server known the deadline and measured bandwidth, and have the server implement a decision based on elapsed time, if the server becomes aware, it cannot reach the next exit, it decides to terminate the inference process, and continue with the next received frame.

Edge/cloud offloading can potentially experience service outage. If an access point is no longer accessible, reconnection delays to a new or the same access point e.g. using WiFi can be expected to take at least 1s \cite{pei_why_2017}. Which will inevitably lead to lost predictions, if no local inference is feasible. If local is feasible we should switch to local execution, when experiences service outages, as in \gls{see} \cite{wang_see:_2019}. \gls{see} is a scheduling scheme to handle on device inference of early exit \gls{dnn}s, if experiencing service outages to offloading services.

\subsection*{Reduce communication time}

The communication latency could possibly be reduced, if the WiFi communication did not go through an access point. Instead the connection could be establish ad-hoc using WiFi Direct. Or we could change the \gls{tcp} transport layer to \gls{udp}.

\gls{tcp} was used for reliable transport to send a single jpeg image for processing, as jpeg is a lossless compression technique, then loosing packets could cause reconstructed images on server-side to be incorrectly classified and reduce the reliability. \gls{tcp} is not designed for video streaming applications, as retransmission of unacknowledged packet increase the communication delay. As shown in figure \ref{fig:tcp-overhead}, \gls{tcp} introduces a large overhead of retransmission, when networking conditions are poor. For video streaming \gls{udp} is more widely used. \gls{udp} is an unreliable protocol, which is applicable in scenarios where some packet loss can be tolerated. For future research streaming either jpeg compressed images over \gls{udp}, as in \cite{liu_maximizing_2019} would be interesting to reduce the communication latency. We did not pursue reducing the communication time, as our focus is not to obtain the best possible results on the development platform, but rather to showcase and tell the story offloading \gls{aee}.

\subsection*{Information Combination} 


Unfortunately, we did not find a combination function, despite they in \cite{kaya_shallow-deep_nodate} show marginal improvements using the highest scoring prediction from all exits. however, in contrast our goal was to improve the accuracy, when all predictions was not available. 
In \gls{ddnn} \cite{teerapittayanon_distributed_2017}, they use a \gls{dnn} to fuse the output from early exits to combine the information. In \gls{ddnn} fused features come from a number of upstream devices, with exits at the same level, where as the sensor fusion in our scenario requires intermediate features of different sizes to be fused, which we do not know is possible or able to improve, hence will require som future research. Additionally it will requires the edge or device to runs this \gls{dnn}, which will cause additional delay  for an operation with possibly limited improvements.\todo{maybe unnecessary}