\hypertarget{discussion}{%
\chapter{Discussion}\label{ch:discussion}}
\thispagestyle{fancy}


\paragraph{Early Exit}
The placement of early exits was placed after down-sampling layers or block, with the primarily reason to reduce the offloading data size of intermediate features for collaborative edge. Even though we ended up not pursuing collaborative architectures, reasons why will be discussed later, we got satisfying early exit accuracy. However, in other work \cite{huang_multi-scale_2017}, the placement of early exit is placed within the blocks and several more exits are added. We did not want to add too much overhead from introducing ealry exits to the model, hence we limited ourselves to only 4 on the model we implemented. In \cite{berestizshevsky_sacrificing_2019} the same placement of classifiers, as in ours, are used.

\paragraph{Comparison of AEE and related work}

We did not come up with a good combination function by hand. In \gls{ddnn} \cite{teerapittayanon_distributed_2017}, they use a \gls{dnn} to fuse the output from  early exits to combine the information. Although, the early exit came from a number of upstream devices at the same exit level, the sensor fusion may still be applicable for our scenario with. \todo{maybe unnecessary}

Compared to \gls{ddnn} \cite{teerapittayanon_distributed_2017}, our investigation of \gls{aee} is not used with model partitioning for collaborative edge based. Instead it is based on either local execution or remote offloading, to make full use of computing resource of the more powerful edge server. Still our solution is applicable for collaborative edge, where the end-device locally processes the algorithm up to an early exit and obtains a prediction, then offloads the rest of the execution in a cascaded manner for remote execution. A locally obtained prediction could possibly reduce the amount of missed predictions, if no reply is received from the edge within the time frame. However, we did try partitioning after the first exit, but encountered increased transmission delays and computation time. Further experimenting with this idea was dropped, as it would require implmenting feature compression or \gls{bottlenet} modules to reduce transmission time. Introducing feature compression would additionally require compression-aware retraining, as stated in \cite{choi_near-lossless_2018,choi_near-lossless_2018,eshratifar_bottlenet:_2019}  to avoid experiencing a high cost in accuracy.

It can be justified from looking at figure \ref{fig:resnet-offloading-vs-local} and \ref{fig:densenet-offloading-vs-local}, that show the local inference time of the first exit of \gls{bresnet} and \gls{bdensenet}, is always outperformed by the edge servers. Thus, our choice to not waste idle time on server, by offloading the entire task is the better one for very stringent deadlines. The early layers of the \gls{dnn} is typically also the most demanding, which will lead to worsened inference time exits, and communication of features from the earliest exits, is also heavier than both  features obtained at later exits and sending the compressed image upfront, at least, if no feature compression technique is used.

Instead an alternative solutions to handle the lost prediction dilemma, is a parallel execution of a shallower local \gls{dnn} and a deep remote \gls{dnn} i.e. a parallel Big/Little setup. The end device offloads the compressed image to edge server and in parallel process a smaller and less accurate \gls{dnn} locally. The upside is, that the application can always use the locally obtained prediction and choose to discard it, as more accurate prediction arrives from edge or if a good combination function is found, use the information to choose the best prediction. If offloading is not an possible, then the \gls{aee} offloading is the better option.

We have not considered real-time processing a stream of video frames, but only single image classfication. Comparing our inference scheme \gls{aee} with Edgent, where an upfront selection of exit is to handle the accuracy-latency trade-off. Edgent tries to utilize available time for the frame without postponing the next one. The selection of exit is based on regression models of inference time and a current state of bandwidth. The upfront exit selection is prone to loose predictions caused by unexpected communication delays. Our solutions have higher probability of having received a single prediction, when a timeout occurs. For instance, if the first exit is reachable for \gls{aee}, and if the prediction unexpectedly violates the deadline for a later exit, at least one prediction is available. Whereas for Edgent the same later selected exit, that turned out to not be reachable, then no prediction is received in time. In other words, an early prediction is better than no prediction and only in worst-case where the first exit is unreachable neither \gls{aee} nor Edgent will suffice. We argue, that the potential of early exiting is not fully utilizes by this submodel selection, as the latency overhead of our additional exits classifier is negligible. Additionally such upfront selection also takes some time away from offloading and inference.

Our approach does not address video streaming initially, if we encounter time-out in between to exits the computation is wasted, which optimally could have been used to process the next frame, if received. However, our solution fully utilize the available time, if a next frame is not we received. A future extension to our scheme is to let the server known the deadline and measured bandwidth, and have the server implement a decision based on elapsed time, if the server becomes aware, it cannot reach the next exit, it decides to terminate the inference process, and continue with the next received frame.

Edge/cloud offloading can potentially experience service outage. If an access point is no longer accessible, reconnection delays to a new or the same access point e.g. using WiFi can be expected to take at least 1s \cite{pei_why_2017}. Which will inevitably lead to lost predictions, if no local inference is feasible. If local is feasible we should switch to local execution, when experiences service outages, as in \gls{see} \cite{wang_see:_2019}. \gls{see} is a scheduling scheme to handle on device inference of early exit \gls{dnn}s, if experiencing service outages to offloading services.

\paragraph{Reduce communication time}

The communication latency could possibly be reduced, if the WiFi communcation was not handles by an intermediate access point. Instead the connection could be establish ad-hoc using WiFi Direct. Or we could change the \gls{tcp} transport layer to \gls{udp}.

\gls{tcp} was used for reliable transport to send a single jpeg image for processing, as jpeg is a lossless compression technique, then loosing packets could cause reconstructed images on server-side to be incorrectly classified and reduce the reliability. \gls{tcp} is not designed for video streaming applications, as retransmission of unacknowledged packet increase the communication delay. As shown in figure \ref{fig:tcp-overhead}, \gls{tcp} introduces a large overhead of retransmission, when networking conditions are poor. For video streaming \gls{udp} is more widely used. \gls{udp} is an unreliable protocol, which is applicable in scenarios where some packet loss can be tolerated. For future research streaming either jpeg compressed images over \gls{udp}, as in \cite{liu_maximizing_2019} would be interesting to reduce the communication latency. 