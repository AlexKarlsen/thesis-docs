\hypertarget{inroduction}{%
\chapter{Introduction}\label{ch:introduction}}
%\thispagestyle{fancy}

\gls{dnn} have in recent years outperformed traditional \gls{ml}, and achieved even super-human performance in \gls{cv} for image classification and object detection \cite{russakovsky_imagenet_2015}. Emerging applications, such as AR/VR, autonomous vehicles, mission critical IoT applications, could all benefit from from \gls{ai} \cite{pettey_immersive_2018}. Opposed to traditional \gls{ml} algorithms, \gls{dnn} require tremendous computing power, which have made \gls{dnn} infeasible for mobile and \gls{iot} devices. All these applications require extreme low latency of \gls{ai} decision feedback infeasible using \gls{ci} \cite{zhou_edge_2019}. 

\acrlong{ei} reduces communication latency by moving the processing of AI algorithms away from cloud data centers at the core of the network, to the network edge using servers and small-scale data centers deployed in closer proximity to the application \cite{shi_edge_2016}. In chapter \ref{ch:edgeintelligence} we elaborate upon \gls{ei}. We describe the background of \gls{ei} and \gls{ei} inference architectures. We present related work in reducing inference latency and cost of \gls{dnn}s deployed at the edge. These method involves novel design of \gls{dnn}s, that reduces the computational effort, model compression to reduce the size of the models, thus the amount of computation required. Our main focus is early exiting models, where some samples exits the inference process using less layer of the \gls{dnn}. We mention other approaches, that involves layer skipping were certain layers in the model are skipped. Additionally, we review methods to partition \gls{dnn} for collaborative on device, edge and cloud. These methods are all elaborated upon in section \ref{sec:ei-fast-inference}. 

We investigate early exit \gls{dnn}s on image classification in chapter \ref{ch:earlyexit}. We describe ealy exits models, including the training and inference framework. We define an analytical model used for evaluation of early exits \gls{dnn}s and implement early exit models using the \gls{branchynet} framework \cite{teerapittayanon_branchynet:_2016} based on state-of-the-art \gls{resnet} \cite{he_deep_2015} and \gls{densenet} \cite{huang_densely_2016} model. We use these models along with \gls{msdnet} \cite{huang_multi-scale_2017}, a model specifically designed for early exits. We contribute with a study of early exiting \gls{dnn}'s capability to trade accuracy for latency using different exit thresholds, and also early exiting for time-critical application with deadlines. The study of early exit \gls{dnn} have led to our novel proposal.

We propose an inference scheme for time-critical application, we call \gls{aee}. The scheme is applicable for on-device inference and edge offloading. It utilizes the properties of early exits model, to use increasingly confident predictions from deeper layers of the \gls{dnn}. If \gls{aee} is deployed on edge the predictions are continuously sent back to the device. We use this best effort approach to cope with latency uncertainties from both computation and not least communication. Instead of an upfront optimal exit decision, as proposed in \cite{li_edge_2018}. They do not account for these uncertainties. Due to the very little overhead from additional classifiers we argue, that our approach is almost always able to reach the same exit selected by \cite{li_edge_2018} and suffers less from sporadic unexpected delays, as we have predictions from earlier exit, and is thus able to improve the overall reliability. In chapter \ref{ch:edgeoffloading} we present our inference scheme. We implement \gls{aee} and setup experiments on a Intel NUC and a Jetson TX2. We present our results and show that our solution improves application reliability under stringent delay requirements compared to on-device inference.