\hypertarget{inroduction}{%
\chapter{Introduction}\label{ch:introduction}}
\thispagestyle{fancy}

\section{Background \& Motivation}

\gls{dnn} have in recent years outperformed traditional \gls{ml}, and achieved even super-human performance in \gls{cv} for image classification and object detection. Emerging applications, such as AR/VR, autonomous vehicles, mission critical IoT applications, could all benefit from from using \gls{dnn}. Opposed to traditional \gls{ml} algorithms, \gls{dnn} require tremendous computing power, which have made \gls{dnn} infeasible for mobile and \gls{iot} devices. All these applications require extreme low latency of \gls{ai} decision feedback infeasible using \gls{ci}.\acrlong{ei} reduces communication latency by moving the processing of AI algorithms away from cloud data centers at the core of the network, to the network edge using servers and small-scale data centers deployed in closer proximity to the application.

Research have for some time investigated methods to reduce inference cost and latency of \gls{dnn}s. These method involves novel design of \gls{dnn}, that reduces the computational effort, model compression to reduce the size of the models, thus the amount of computation required. Other methods involves early exiting were sample are exited the inference process using less layer of the \gls{dnn} or layer skipping were certain layers in the model are skipped. These methods are elaborated upon in section \ref{sec:ei-fast-inference}. We investigate early exit \gls{dnn}s on image classification. We contribute with a study of early exiting \gls{dnn}'s capability to trade accuracy for latency, that have led to our novel proposal for standalone inference on edge or device. 

We propose and implement a \gls{aee}, a standalone inference architecture utilizing the properties of early exits, to use increasingly confident predictions from deeper layers of the \gls{dnn}. If \gls{aee} is deployed on edge the predictions are continuously send back to the device. We setup experiments on a Intel NUC and a Jetson TX2 to evaluate our design, and show that our solution improves application reliability under stringent delay requirement compared to on-device inference. 

In the next section we present work related to inference architecture. 

\section{Related work}

Early exiting is proposed in \cite{leroux_resource-constrained_2015, teerapittayanon_branchynet:_2016}. Early exit model is used in Edgent \cite{li_edge_2018} to investigate classification of real-time video streaming. They propose a partial offloading scheme for device/edge co-inference, using an optimizer to maximize the service accuracy while complying with latency requirements. Edgent uses model partitioning to find an optimal splitting point of the \gls{dnn} and model right-sizing to select an early exit point as final exit.

Their solution may suffer from sporadic unexpected communication delays or service outages under the inference process. We account for this by not selecting an exit point upfront, but uses all predictions we received prior to the deadline. In worst case the first exit is never reachable, hence neither \gls{aee}, nor Edgent will receive a prediction within the deadline. If the first exit is reachable \gls{aee} will always receive at least on prediction, whereas if Edgent selected a later exit, that turned out to not be reachable, no prediction is received in time. \todo{is this too much discussion and critique? Should i use it in justification of our design instead? as I sort of already do...}

In \cite{wang_see:_2019} \gls{see} is proposed. As the name implies, \gls{see} is a scheduling of early exit model for on-device inference.  \gls{see} also uses real-time video streaming as use case, by scheduling frames able to meet the deadline and  ignoring frames that cannot, to maximize the utility. We deem our solution could benefit from the scheduling of video frames, if is was to be extended to real-time video streaming. 

However, as the overhead of our early exit model's additional exits is negligible, we argue that using our optimistic scheme for edge inference, that always tries to deliver the best possible prediction before the deadline, would outperform upfront exit selection. Our scheme could easily be used in combination with \gls{see}, by switching to \gls{see} for on-device inference, when poor networking connection or service outage is detected. \todo{same}

In the next section the structure of the rest of the thesis presented

\section{Thesis Structure}

In chapter \ref{ch:edgeintelligence} the \gls{ei} research field is presented. We describe the background of \gls{ei}, \gls{ei} inference architectures, and finally we present research addressing fast inference categorized by inference architectures. 

In chapter \ref{ch:earlyexit} the early exit model is elaborated upon. Which as mentioned, is the main focus for fast inference in this thesis. We define the metrics used for evaluation of our experiments based on early exits models and describe implementation of early exit models used in our experiments. We describe the experiment and present our results. 

In chapter \ref{ch:edgeoffloading} we present our standalone inference scheme and explain implementation details. We define additional related to evaluating our solution. We describe experiments of the scheme and present the results. 

In chapter \ref{ch:discussion} we discuss the design choices and findings of the thesis. 

Finally in chapter \ref{ch:conclusion} we conclude the thesis.

%\section{Our contribution}
%
%\todo{our proposal and all our experiments, which have brought some news to litterature}
%
%The objective of this thesis is, taking mobile AR applications as use case, to design and implement of offloading deep learning algorithms to maximize the inference reliability while meeting the service latency deadline. The thesis will design feasible offloading schemes, such as deep neural network partitioning, preprosssing (feature extractions), in objective detection and classification. The proposed schemes will be implemented using Raspberry Pi and Jetson TX2. The communication and computation latency, as well as the inference accuracy and reliability will be measured and analyzed.

