
\hypertarget{introduction}{%
	\chapter{Introduction}\label{ch:introduction}}
%\thispagestyle{fancy}

Deep Neural Networks have in recent years outperformed traditional \gls{ml}, and achieved even super-human performance in \gls{cv} for image classification and object detection \cite{russakovsky_imagenet_2015}. Emerging applications, such as AR/VR, autonomous vehicles, mission critical IoT applications, could all benefit from \gls{ai} \cite{pettey_immersive_2018}. Opposed to traditional \gls{ml} algorithms, a \gls{dnn} requires tremendous computing power, which have made \gls{dnn}s infeasible for mobile and \gls{iot} devices. All these applications require extreme low latency of \gls{ai} decision feedback, which makes \gls{ci} infeasible \cite{zhou_edge_2019}. 

\acrlong{ei} reduces communication latency by moving the processing of AI algorithms away from cloud data centers at the core of the network, to the network edge using servers and small-scale data centers deployed in closer proximity to the application \cite{shi_edge_2016}. In chapter \ref{ch:edgeintelligence} we elaborate upon \gls{ei}. We describe the background of \gls{ei} and \gls{ei} inference architectures, and we present related work in reducing inference latency and cost of \gls{dnn}s deployed at the edge. Our main focus is early exiting models, due to the flexibility of reducing the inference latency by early exits using fewer layers of the \gls{dnn}. Methods to reduce inference latency at the edge are elaborated upon in \ref{sec:ei-fast-inference}. 

We investigate early exit \gls{dnn}s on image classification in chapter \ref{ch:earlyexit}. We describe early exits models, including the training and inference framework. We define an analytical model used for evaluation of the early exit and the conventional \gls{dnn}s. We implement the early exit models using the \gls{branchynet} framework \cite{teerapittayanon_branchynet:_2016} based on state-of-the-art \gls{resnet} \cite{he_deep_2015}, \gls{densenet} \cite{huang_densely_2016} models, and \gls{msdnet} \cite{huang_multi-scale_2017}, a model specifically designed for early exits. We contribute with a study of early exiting \gls{dnn}'s capability to trade accuracy for latency using different exit thresholds, and also early exiting for time-critical applications with deadlines.

In chapter \ref{ch:edgeoffloading}, we propose an inference scheme for time-critical applications named \acrfull{aee}. The scheme utilizes the flexibility of early exit models to produce increasingly confident predictions from deeper layers of the \gls{dnn} before the deadline. The scheme is applicable for both on-device inference and edge offloading. If \gls{aee} is deployed on the edge, the predictions are sent back to the device. Continuously sending back predictions is the best effort approach to cope with latency uncertainties from both computation and communication. \gls{aee} suffers less from sporadic, unexpected delays. Upfront optimal exit decision, as proposed in \cite{li_edge_2018}, do not account for these uncertainties. Due to the limited delay overhead from additional classifiers we argue, that our approach is able to reach the same exit, as in \cite{li_edge_2018}. We show that \gls{aee} has the potential to enable services with stringent latency requirements, which cannot be realized by the conventional DNNs. We implement \gls{aee} and setup experiments on an Intel NUC and a Jetson TX2. We present our results and show that the solution improves service reliability under stringent delay requirements compared to on-device inference.

In chapter \ref{ch:conclusion} we conclude upon the thesis.
