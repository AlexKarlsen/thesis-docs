\hypertarget{introduction}{%
\chapter{Introduction}\label{ch:introduction}}
%\thispagestyle{fancy}

Deep Neural Networks have in recent years outperformed traditional \gls{ml}, and achieved even super-human performance in \gls{cv} for image classification and object detection \cite{russakovsky_imagenet_2015}. Emerging applications, such as AR/VR, autonomous vehicles, mission critical IoT applications, could all benefit from \gls{ai} \cite{pettey_immersive_2018}. Opposed to traditional \gls{ml} algorithms, a \gls{dnn} requires tremendous computing power, which have made \gls{dnn}s infeasible for mobile and \gls{iot} devices. All these applications require extreme low latency of \gls{ai} decision feedback infeasible using \gls{ci} \cite{zhou_edge_2019}. 

\acrlong{ei} reduces communication latency by moving the processing of AI algorithms away from cloud data centers at the core of the network, to the network edge using servers and small-scale data centers deployed in closer proximity to the application \cite{shi_edge_2016}. In chapter \ref{ch:edgeintelligence} we elaborate upon \gls{ei}. We describe the background of \gls{ei} and \gls{ei} inference architectures, and we present related work in reducing inference latency and cost of \gls{dnn}s deployed at the edge. Our main focus is early exiting models, due to the flexibility of reducing the inference latency by early exits using less layers of the \gls{dnn}. Methods to reduce inference latency at the edge are elaborated upon in \ref{sec:ei-fast-inference}. 

We investigate early exit \gls{dnn}s on image classification in chapter \ref{ch:earlyexit}. We describe early exits models, including the training and inference framework. We define an analytical model used for evaluation of the early exit and the conventional \gls{dnn}s. We implement the early exit models using the \gls{branchynet} framework \cite{teerapittayanon_branchynet:_2016} based on state-of-the-art \gls{resnet} \cite{he_deep_2015}, \gls{densenet} \cite{huang_densely_2016} models, and \gls{msdnet} \cite{huang_multi-scale_2017}, a model specifically designed for early exits. We contribute with a study of early exiting \gls{dnn}'s capability to trade accuracy for latency using different exit thresholds, and also early exiting for time-critical application with deadlines. The study of early exit \gls{dnn} has led to our novel proposal.

In chapter \ref{ch:edgeoffloading} we propose an inference scheme for time-critical application named \acrfull{aee}. The scheme utilize the flexibility of early exits model to produce increasingly confident predictions from deeper layers of the \gls{dnn} until time is up. The scheme is applicable for both on-device inference and edge offloading.  If \gls{aee} is deployed on the edge, the predictions are sent back to the device. Continuously sending bac prediction is a best effort approach to cope with latency uncertainties from both computation and not least communication.\gls{aee} suffers less from sporadic unexpected delays, than of an upfront optimal exit decision, as proposed in \cite{li_edge_2018}, that do not account for these uncertainties. Due to the small overhead from additional classifiers we argue, that our approach is able to reach the same exit selected by \cite{li_edge_2018}. We show that \gls{aee} has potential to enable the emerging services with stringent latency requirements, which cannot be realized simply by the conventional DNNs. We implement \gls{aee} and setup experiments on an Intel NUC and a Jetson TX2. We present our results and show that the solution improves application reliability under stringent delay requirements compared to on-device inference.

In chapter \ref{ch:conclusion} we conclude upon the thesis.