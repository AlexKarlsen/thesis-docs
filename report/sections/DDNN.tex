\section{Distributed Deep Neural Network}

\gls{ddnn} \cite{teerapittayanon_distributed_2017} also proposed by \citeauthor{teerapittayanon_distributed_2017}, extend upon early exiting models and model partitioning,  to  create a distributed computing hierarchy over cloud, edge and end devices. The idea is having the same shallow model running on multiple end-device, that collaboratively classifies the object. If the network of end devices cannot obtain a classification of satisfactory confidence, a larger model taking the output features from all end-devices as input, is run on a \gls{lan} connected edge server. 

\begin{figure}
	*Figure of DDNN-ResNet50*
	\caption[\gls{ddnn}-ResNet architecture]{ResNet50 extended to implement the \gls{ddnn} framework.}
	\label{ddnn-resnet}
\end{figure}

The distributed network is trained solving the same joint-optimization problem as for BranchyNet.

\begin{align*}
L(\hat{\mathbf{y}},\mathbf{y};\theta) = \sum_{n=1}^{N} w_m L(\hat{\mathbf{y}}_{exit_n},\mathbf{y};\theta)
\end{align*}

Where the loss function is the softmax cross-entropy objective.
The weighted sum loss is back-propagated to optimize the weights of the network. 

