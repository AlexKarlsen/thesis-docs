
    \hypertarget{abstract}{%
    \chapter*{Abstract}\label{sec:abstract}}
    \small{\textcolor{caption-color}{The thesis \textit{"\thetitle"}, investigates methods to reduce deep neural network (DNN) inference latency for intelligent applications at the edge. This is important for emerging applications, such as AR/VR, autonomous vehicles, mission critical IoT applications, and others. These application all require extreme low latency, that the conventional cloud intelligence approach suffers to meet due to a communication bottleneck. The conventional cloud-centric framework offloads sensor data, e.g., images from end devices, to the central cloud to perform model inference and send the results back to the end devices. DNNs have been state-of-the-art for such tasks for nearly the last decade and have been too computationally demanding to run elsewhere, than in large-scale data centers. However, as hardware for accelerated computing are becoming increasingly accessible, a new computing paradigm emerges at the edge of the network.
    \newline\noindent Edge intelligence reduces communication latency by moving the processing of AI algorithms from the core of the network to the network edge using servers and small-scale data centers in closer proximity. Edge computing has many benefits such as lower communication latency, but also higher reliability and resiliency, better security and privacy, scalability and context-awareness and others. Despite being a fairly recent area of research, efforts have already been made in designing DNN models especially suited for edge computing by reducing the inference time at the cost of reliability. One of these are early exiting models.
    \newline\noindent Early exiting models have been used to either reduce the mean inference time by allowing samples to prematurely exit the inference process if a confident prediction can be obtained. Or to meet a certain delay requirement by sub-model inference up to an earlier exit of the DNN. We have implemented and trained early exiting models based on state-of-the-art DNN architectures and made a comprehensive evaluation of the promising accuracy-latency trade-off implied by early exiting. We have studied confidence threshold strategies for early exiting to reduce mean inference latency and improving reliability under delay constraints. Based on our findings, we propose a novel flexible offloading scheme able to comply with latency requirements. The proposed scheme is implemented as a prototype using Raspberry Pi as IoT end-device and NVIDIA Jetson TX2 as edge server and tested in a lab setup. The scheme takes advantage of inherent properties of early exiting models. The end device offload image data to an edge server which processes the DNN. Early predictions are not exited, but sent back to end device as soon as obtained. The end-device might receive multiple prediction within the allowed time-budget, and uses information from all predictions to make a final decision. Our work reveal the possibility to improve the reliability using all information we can gather within time-frame and still being able to meet service latency requirement. }}