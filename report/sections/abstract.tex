
\hypertarget{abstract}{%
	\chapter*{Abstract}\label{sec:abstract}}
\begin{justify}
	\begin{small}
		{\textcolor{caption-color}{The thesis "\thetitle", investigates methods to reduce inference latency of deep neural networks (DNNs) for intelligent applications on the edge of the network. This is important for emerging applications such as AR/VR, autonomous vehicles, mission-critical IoT applications, and others. These applications all require extremely low latency, which the conventional cloud intelligence approach cannot meet due to communication bottlenecks on the public internet. \newline
				The conventional, cloud-centric framework offloads sensor data, e.g., images from end devices to the central cloud. The cloud performs model inference and sends the results back to the end-devices. DNNs have been state-of-the-art for such tasks for the last decade but only when run in large-scale data centers. However, as hardware for accelerated computing is becoming increasingly accessible, a new computing paradigm has emerged on the edge of the internet.
				\newline Edge intelligence reduces communication latency by moving the processing of AI algorithms from the core of the network to the network edge using decentral servers and small scale data centers. Edge computing has many benefits such as lower communication latency, higher reliability and resilience, better security and privacy, scalability and context-awareness, and others. Despite being a relatively new area of research, efforts have already been made in designing DNN models specially suited for edge computing by reducing the inference time. One of these is the early exit model. 
				\newline Early exiting models have been used to either reduce the mean inference time by allowing samples to exit the inference process at an early stage if a confident prediction can be obtained. We have implemented and trained early exiting models based on state-of-the-art DNN architectures and made a comprehensive evaluation of the promising accuracy-latency trade-off implied by early exiting. We have studied confidence threshold strategies for early exiting to reduce mean inference latency and improve reliability under delay constraints.
				\newline
				Based on our findings, we propose a novel, flexible inference scheme, Adaptive Early Exit (AEE), which can comply with more stringent latency requirements. The proposed scheme is implemented as a prototype using an Intel NUC as IoT end-device and NVIDIA Jetson TX2 and a GPU Workstation as edge servers and tested in a lab setup. The scheme takes advantage of the inherent properties of early exiting models â€” the end-device offloads image data to an edge server, which processes the DNN. While the DNN keeps running, the predictions via early exits will be sent back to the end device as soon as they are ready. The end-device might receive multiple predictions within the allowed time frame and can use information from all predictions to make a final decision. Our work reveals the new possibility to improve the reliability under stringent latency requirements, which cannot be fulfilled by conventional DNNs. 
		}}
	\end{small}
\end{justify}