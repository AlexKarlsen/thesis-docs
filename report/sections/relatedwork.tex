\section{Related work}

Read cascade neural net for inspiration to this introduction

Over the last couple of years an increasing interest in reducing the inference time of intelligent applications to extend to real-time applications. To improve inference latency on a model-basis, efforts have been made in designing \gls{dnn}s, that efficiently runs on mobile device. Commonly for \gls{mobilenet} \cite{howard_mobilenets:_2017}, \gls{mobilenetv2} \cite{sandler_mobilenetv2:_2018}, \gls{shufflenet} \cite{zhang_shufflenet:_2017} and \gls{shufflenetv2} \cite{ma_shufflenet_2018} are all aiming to efficiently reducing the number of parameters, without compromising model accuracy by designing novel \gls{dnn} architectures.   

Other efforts to reduce inference latency involve model compression. 

\subsection{Model Compression}

Weight pruning probably the most widespread model compression technique improves model inference latency by removal of redundant weights. Another compression technique is quantization, which represents weight of a \gls{dnn} using a more compact format than 32-bit floating point. One of these efforts are \gls{bnn} \cite{courbariaux_binarized_2016}, which uses binarized weights and activations. In a \gls{bnn} most arithmetic operations are replaced with bit-wise operations, thus greatly improving the power-efficiency and inference latency. The impact of compression is application dependent and can be applied to any \gls{dnn}. Other approaches specifically related to edge computing aiming to reduce inference latency, is model partitioning between end device and edge server. 

\subsection{Model Partitioning}

Neurosurgeon \cite{kang_neurosurgeon:_2017} is a lightweight partitioning scheduler, that uses knowledge of the individual layers of the \gls{dnn} to effectively reduce inference latency. Splitting a model is an inherent nature of sequential \gls{dnn}s, that at any layer can be stopped. The intermediate output are transferred over the network and continued at the next layer on an edge server, as shown in figure \ref{fig:offlaoding}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/partitioning}
	\caption[Model partitioning]{Edge-Device model partitioning run part of the model on-device and offload the rest to edge processing. Network partitioning utilize the assumption, that at some later point in the \gls{dnn} a smaller representation of the data is found, illustrated by the gradually decreasing model layers, to reduce the communication bottleneck. }
	\label{fig:offlaoding}
\end{figure}

Communication latency is the bottleneck in an offloading application, hence a smaller representation of the input data is needed, however the layers producing a smaller output than the original input typically lies deep within the network. Neurosurgeon construct regression models for layer execution time and  output data size of the layers of a \gls{dnn}, to decide the best partition of the \gls{dnn} based on networking condition. The work is based on \gls{mcc} and shows, that the conventional cloud-only approach is insufficient due to different networking technologies and mobile device is becoming \gls{gpu} enabled. 
% Evidently moving computation to the edge reduces the communication latency. 

Another effort to reduce communication overhead of network splitting is adding feature compression of intermediate features before offloading to cloud \cite{choi_deep_2018}. The paper shows, that lossless compression have no impact on accuracy, however bit saving is rather limited. Lossy compression, on the other hand, results in 70\% bit savings, however also affects accuracy and require compression-aware training to compensate. The follow up paper \cite{choi_near-lossless_2018} propose compression techniques for deep features and achieve significant better bit saving, than conventional image compression algorithms. 

\gls{bottlenet} \cite{eshratifar_bottlenet:_2019} is a novel neural network module. Client-side it consists of a reduction unit and a compressor unit and server-side of a decompressor unit and restoration unit. The reduction unit creates a smaller representation of intermediate result by applying spatial- and channel-wise convolution. The compressor uses lossy \gls{jpeg} compression, that are decompressed server-side. The restoration unit apply deconvolution to restore the intermediate feature back to the required dimensionality for the next layer in the network. \gls{bottlenet} is able to achieve 84$\times$ bit savings compared to cloud-only approach with less than 2\% degradation of accuracy caused by lossy compression with compression-aware training. Under good networking condition, the evaluation of \gls{bottlenet} shows, that the best split is after the first convolutional block, as a smaller representation of the input can be found already here. Compared to cloud-only approach using WiFi a 8$\times$ speed up is found.

The collaborative scheme between end device and edge servers shows improvements for \gls{cpu}-enabled end devices, compared to a cloud-only approach, however as communication is introduced the overall latency will vary depending on the networking conditions. Model selection is an approach, that tries to limit the amount of offloading, by first running an on-device model. 

\subsection{Model Selection}

Model selection is an approach to reduce inference by selecting an appropriately accurate model, hence not using an unnecessarily deep model, if a shallower model should be satisfiable. In \cite{bolukbasi_adaptive_2017} a model selection framework is proposed. The framework stacks three increasingly deeper and more accurate models on top of each other; \gls{alexnet}, \gls{googlenet} and \gls{resnet}50. The input is inferred to \gls{alexnet}, if the confidence scores is satisfactory the prediction is accepted, if not the framework decides to use either \gls{googlenet} or \gls{resnet}50 depending on a confidence threshold, however if the sample is inferred to \gls{googlenet} and the confidence is still unsatisfactory the sample is inferred to \gls{resnet}50 for the final prediction. The work shows improvement of the average prediction time with only a small reduction in accuracy depending on the confidence threshold. However, for hard samples, since multiple model are introduced, the inference time is increased, as well as the computational cost and memory consumption, thus such model selection approach seems overwhelming to introduce on an end-device yet it may be feasible in an edge-device mode using a Big/Little setup.

To obtain faster inference Big/Little \gls{dnn} \cite{park_big/little_2015} is simple approach using model selection. It implements a hybrid edge architecture of device-only and selective offloading for edge-only processing. It runs a smaller yet less accurate model on device and a larger more accurate model on the edge server, as illustrated by figure \ref{fig:big/little-dnn}. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/models/big_little_dnn}
	\caption[Big/Little \gls{dnn} architecture]{Big/Little \gls{dnn}, a hybrid edge architecture. An on-device model is used to selectively offload to a more complex model hosted on an edge server.}
	\label{fig:big/little-dnn}
\end{figure}

If the prediction confidence of the little model is unsatisfactory, a decision is made to offload to the big model on the edge server. If a lot of samples are able to be correctly classified locally a speed-up is gained. However, the down-side of this approach is, if too many samples require the big model to satisfy a certain confidence threshold, a lot of work is wasted on the on-device prediction. Nonetheless the paper \cite{park_big/little_2015} contribute with another method to define a threshold, called \textsc{Score margin}. They have found, that the difference between the highest scoring and second highest scoring prediction, have a close relation to the actual true label. Although Big/Little \gls{dnn} obtain good results on energy savings and inference latency, more sophisticated frameworks such as early exiting, that also enabled model partitioning have been proposed.

\subsection{Model Early Exit}

Model early exiting is a way to handle the latency-accuracy trade-off, by reducing the average inference time without compromising model accuracy. Typically samples can be accurately classified using less \gls{dnn} layers which can improve the inference time, if a sample cannot be classified with proper confidence more layers can be used to obtain a more confident prediction. Since early exiting less computation is wasted, if offloading is necessary after an exit compared to a Big/Little model selection setup. As less computation is needed and a smaller representation of input data can be obtained, thus less data must be offloaded to the edge server.

Cascading neural networks \cite{leroux_resource-constrained_2015} by \citeauthor{leroux_resource-constrained_2015} proposes an early exiting framework by adding a cascade of intermediate classifiers, that allow some sample to exit the model early, hence improving inference time. \gls{branchynet} \cite{teerapittayanon_branchynet:_2016} proposed by \citeauthor{teerapittayanon_branchynet:_2016} is an early exiting framework for existing state-of-the-art model, that allow for fast inference. The paper proposes a novel joint optimization of the all output branches which shows added regularization to the model weights. The framework shows promising results of reduced inference time. The result are based on three well-known \gls{dnn} architectures; \gls{lenet} \cite{lecun_lecun-98.pdf_1998}, \gls{alexnet} \cite{krizhevsky_imagenet_2017} and \gls{resnet} \cite{he_deep_2015}, modified to implement the \gls{branchynet} framework, and to accommodate the MNIST \cite{lecun_mnist_2010} and Cifar-10 \cite{krizhevsky_cifar-10_nodate} datasets.

Cascading neural network \cite{leroux_resource-constrained_2015} have been followed up in \cite{leroux_cascading_2017} and \gls{branchynet} have been extended to \gls{ddnn} in \cite{teerapittayanon_distributed_2017}. Both papers are distributing the early exiting model over a distributed computing hierarchy over cloud, edge and end devices.  

The distributed cascading neural network and \gls{ddnn} is basically alike, however the papers have different perspectives. \gls{ddnn} focuses on a cluster of $k$ stationary end devices collaboratively solving a classification challenge. Using a shallow fixed early exit model on end devices and offloading obtained features to servers if necessary. The feature size offloaded to servers are $k$ times the amount of features at the exit point of the model, thus the \gls{ddnn} framework require complete retraining of the model, as the amount of features after the exit point is not consistent with the existing \gls{dnn} architecture or it will require new novel architectures optimized for the \gls{ddnn} framework. Nonetheless, the work shows benefits from distributed computing to provide fault tolerance when end devices are failing. and I should mention other stuff...

Cascading neural network \cite{leroux_cascading_2017} on the other hand is more focused on reducing inference time of a single \gls{dnn} with early exiting, which is more suitable for mobile \gls{p2p} applications, however still cascaded/distributed over a computing hierarchy. Training the cascaded neural network is no different than training any other early exiting model, however in \cite{leroux_cascading_2017}, they freeze the network weights and only train the softmax classifiers, whereas in \cite{teerapittayanon_branchynet:_2016} show, that training the model's weights benefit from the joint optimization in the form of added regularization and obtaining early features more suited for early exiting. In the cascading neural network \cite{leroux_cascading_2017}, early exiting and network partitioning point can be chosen depending available networking bandwidth, yet the exit and partition point is assigned statically and evaluated at two different settings, where the point is placed at two different depths. 

Another proposal Edgent \cite{li_edge_2018} by \citeauthor{li_edge_2018} is a framework built on top of a \gls{branchynet} model. Edgent is an optimization of the latency-accuracy trade-off for mission-critical application with a predefined deadline. It tries to optimize the selection of exit and partitioning point of a cascaded early exiting model in the online phase. The optimization is based on a latency requirement, a regression model of each layer type of the used \gls{branchynet} model and the observed available bandwidth between end device and edge server. Experiments in the paper show, that Edgent is able to meet more stringent deadlines, however since work is based on \gls{cifar10} and branchy \gls{alexnet}, one may question the results in a real-life scenario, where input samples greatly differs in complexity. However, the offloading algorithm is based on deciding a exit point upfront depending on the latency, when using a powerful edge server more weight will be added to actually offloading, when the notion of hard vs. easy samples is not taken into account when deciding upon exit and partition point, hence no samples are exited locally thus saving computations, time and energy.

\citeauthor{huang_multi-scale_2017} have in \cite{huang_multi-scale_2017} investigated different state-of-the-art architectures for early exiting. They have found, that  densely connected layers of \gls{densenet} \cite{huang_densely_2016} are more suitable for early exiting than the popular \gls{resnet} architecture build of residual blocks. Densely connected block uses a concatenation of features from all layers for final classification. The combination of  general feature and increasingly more specific and complex features are shown to be important for early exiting models. This finding have been used to come of with a novel \gls{dnn} archtiecture specifically designed for early exiting. \gls{msdnet} \cite{huang_multi-scale_2017} uses the densely connected layers along with multi-scale paths. The unique design of multi-scale densely connected blocks shows further improvement on early exiting.

\section{temp. skipping}

Other approaches to reduce inference time of \gls{dnn} involve mechanism for skipping certain layers. SkipNet \cite{wang_skipnet:_2017} is a framework for adding dynamic decision for skipping layers. The framework adds complexity to the model by introducing skipping gates. The skipping policies learn by the training process is no longer purely supervised, but require reinforcement learning to train the skipping decisions. The work shows, that only a small fraction of inputs actually require extremely deep models, thus being able to reduce computational cost by 30\% of \gls{resnet}101 on \gls{ilsvrc2012}. BlockDrop \cite{wu_blockdrop:_2017} is another approach to learn skipping policies, however instead of adding intermediate skipping gates for dynamic local decisions, BlockDrop trains a global policy network, that selectively chooses which model depth to use. The skipping network is similarly trained using reinforcement learning. The training shows, which classes are easy and which are hard.  Both SkipNet and BlockDrop does not support Early Exiting, both BlocDrop could be used in an edge-device mode, as the policy network run by an end device would selective choose among a smaller model on device or a larger model on an edge server in a BIG/LITTLE \cite{park_big/little_2015} setup, however none of SkipNet and BlockDrop support network cascading and early exiting. 

\section{Our contribution}

We look at combining early exiting with model partitioning. 